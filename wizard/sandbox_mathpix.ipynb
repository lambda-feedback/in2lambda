{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import requests\n",
    "\n",
    "# Load environment variables from .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2023\n",
    "path = \"./pastpapers/\"\n",
    "paper_file_path = f\"{year}_paper.mmd\"\n",
    "answers_file_path = f\"{year}_answers.mmd\"\n",
    "set_path = f\"{path}mathpixJSON/set_fluid_mechanics_{year}/\"\n",
    "\n",
    "# Load the Markdown files\n",
    "with open(path+paper_file_path, \"r\") as f:\n",
    "    paper_text = f.read()\n",
    "    #  ignore anything after Appendices\n",
    "    paper_text = paper_text.split(\"Appendices\")[0]\n",
    "with open(path+answers_file_path, \"r\") as f:\n",
    "    answers_text = f.read()\n",
    "    # ignore anything after Notes on the paper\n",
    "    answers_text = answers_text.split(\"Notes on the paper\")[0]\n",
    "\n",
    "# Print out a summary.\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {paper_file_path}: {len(paper_text)} characters\")\n",
    "print(f\"  {answers_file_path}: {len(answers_text)} characters\")\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {paper_file_path}: {paper_text}\")\n",
    "print(f\"  {answers_file_path}: {answers_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the figures from the paper and answers.\n",
    "figures = {}\n",
    "def extract_figures_from_text(text, ans=False):\n",
    "    \"\"\"\n",
    "    Extracts figures from the text using regex.\n",
    "    Finds figure references and their descriptions.\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    # Regex to match figure references and their descriptions\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    print(f\"Matches found: {matches}\")\n",
    "    \n",
    "    for match in matches:\n",
    "        url = match\n",
    "        url = url.strip()\n",
    "        figure_caption_pattern = rf'\\({re.escape(url)}\\)\\s*-?\\s*Figure\\s+(Q\\d+)\\s*-\\s*(.+?)\\n'\n",
    "        caption_match = re.search(figure_caption_pattern, text)\n",
    "\n",
    "        if caption_match:\n",
    "            title, description = caption_match.groups()\n",
    "            print(\"Caption match found\")\n",
    "        else:\n",
    "            title, description = \"\", \"\"\n",
    "\n",
    "        if url.startswith(\"http\"):\n",
    "            # Download the image and save it to a file\n",
    "            image = Image.open(requests.get(url, stream=True).raw)\n",
    "            # Create a figure name based on the URL\n",
    "            fig_name = os.path.basename(url)\n",
    "            figures[fig_name] = {\n",
    "                \"image\": image,\n",
    "                \"title\": title.strip(),\n",
    "                \"label\": description.strip(),\n",
    "                \"url\": url,\n",
    "                \"answerFile\": ans\n",
    "            }\n",
    "    return figures\n",
    "\n",
    "figures.update(extract_figures_from_text(paper_text))\n",
    "figures.update(extract_figures_from_text(answers_text, ans=True))\n",
    "\n",
    "Path(f\"{set_path}\").mkdir(exist_ok=True)\n",
    "Path(f\"{set_path}media/\").mkdir(exist_ok=True)\n",
    "for idx, (fig_name, fig_info) in enumerate(figures.items()):\n",
    "    print(f\"FIGURE Title='{fig_info['title']}', Label='{fig_info['label']}', URL='{fig_info['url']}'\")\n",
    "    image_name = f\"figure_{fig_info['title']}{\"_ans\" if fig_info[\"answerFile\"] else \"\"}.png\"\n",
    "    if image_name in os.listdir(f\"{set_path}media/\"):\n",
    "        image_name = f\"figure_{fig_info['title']}_{idx}{\"_ans\" if fig_info[\"answerFile\"] else \"\"}.png\"\n",
    "    fig_info[\"image\"].save(f\"{set_path}media/{image_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM via LangChain.\n",
    "temperature = 0\n",
    "llm = ChatOpenAI(\n",
    "            model=os.environ['OPENAI_MODEL'],\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "#             model=os.environ['GOOGLE_AI_MODEL'],\n",
    "#             temperature=temperature,\n",
    "#             google_api_key=os.environ['GOOGLE_AI_API_KEY'],\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Questions and Parts\n",
    "- get questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the tutorial output.\n",
    "class Exercise(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the exercise (only the text, no numbering)\")\n",
    "    content: str = Field(..., description=\"Content of the exercise (no exercise title, no subquestions)\")\n",
    "    subquestions: list[str] = Field(..., description=\"List of subquestions within the exercise (only the text, no numbering)\")\n",
    "    \n",
    "class Tutorial(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the tutorial\")\n",
    "    year: str = Field(..., description=\"Year of the tutorial\")\n",
    "    exercises: list[Exercise] = Field(..., description=\"List of tutorial questions\")\n",
    "\n",
    "def extract_tutorial_questions(doc_page_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual exercises from a tutorial sheet.\n",
    "\n",
    "    This function takes the content of a tutorial sheet (doc.page_content), constructs a prompt\n",
    "    instructing the LLM to infer the tutorial title and to split the text into separate questions.\n",
    "    The output must be a valid JSON string with the following structure:\n",
    "    \n",
    "    {\n",
    "        \"name\": \"<title of tutorial>\",\n",
    "        \"year\": \"<year of tutorial>\",\n",
    "        \"exercise\": [\n",
    "            { title: \"exercise text 1\", content: \"content text exercise 1\", subquestions: [\"subquestion text 1\", \"subquestion text 2\", ...] },\n",
    "            { title: \"exercise text 2\", content: \"content text exercise 2\", subquestions: [\"subquestion text 1\", \"subquestion text 2\", ...] },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    The tutorial sheet (IMPORTED_TUTORIAL) may contain reference solutions; do not alter\n",
    "    the original text of the exercises. The function returns a dictionary parsed from the JSON output.\n",
    "    \n",
    "    Args:\n",
    "        doc_page_content (str): The content of the tutorial sheet.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    # Initialize the output parser with the Tutorial schema.\n",
    "    parser = PydanticOutputParser(pydantic_object=Tutorial)\n",
    "\n",
    "    # Construct the prompt, appending the parser's format instructions.\n",
    "    prompt = f\"\"\"\n",
    "        IMPORTED_TUTORIAL\n",
    "        ```markdown\n",
    "        {doc_page_content}\n",
    "        ```\n",
    "\n",
    "        IMPORTED_TUTORIAL is a tutorial sheet with several exercises. It may or may\n",
    "        not include reference solutions. Please infer the title of the tutorial from\n",
    "        the content, and extract each individual question as a separate string. Do\n",
    "        not modify the text of the exercises. Only use $...$ for math expressions.\n",
    "\n",
    "        If the exercise mentions figures, then find all the captions of figures (no links). \n",
    "        Keep the captions as \"Figure Q1 - ...\".\n",
    "\n",
    "        If the exercise mentions tables, then include the table in the content.\n",
    "\n",
    "        Return a valid JSON string with the following structure:\n",
    "        {parser.get_format_instructions()}\n",
    "        \"\"\"\n",
    "\n",
    "    # Call the LLM\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Debug: print the raw LLM response\n",
    "    print(\"Raw LLM Response:\")\n",
    "    print(response)\n",
    "\n",
    "    try:\n",
    "        # Parse the response using the output parser.\n",
    "        parsed_output = parser.parse(response.content)\n",
    "        # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "        return parsed_output.model_dump()\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing LLM response as JSON:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_tutorial = extract_tutorial_questions(paper_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title\n",
    "title = imported_tutorial[\"name\"] + \" \" + imported_tutorial[\"year\"]\n",
    "\n",
    "# Print the title\n",
    "print(f\"Title: {title}\\n\")\n",
    "\n",
    "# Extract questions\n",
    "questions = imported_tutorial[\"exercises\"]\n",
    "\n",
    "# Loop over and print each question\n",
    "for idx, question in enumerate(questions, start=1):\n",
    "    print(f\"**Question {idx}**:\\n{question.get(\"title\")}\\n\")\n",
    "    print(f\"Content: {question.get(\"content\")}\\n\")\n",
    "    print(\"Subquestions:\")\n",
    "    for subquestion in question.get(\"subquestions\", []):\n",
    "        print(f\"- {subquestion}\")\n",
    "    print(\"-\" * 40)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Answers\n",
    "- get question_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the tutorial output.\n",
    "class ExerciseAnswers(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the exercise (only the text, no numbering)\")\n",
    "    workedSolutions: list[str] = Field(..., description=\"List of worked solution to subquestions within the exercise (no numbering or counting)\")\n",
    "    \n",
    "class TutorialAnswers(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the tutorial\")\n",
    "    year: str = Field(..., description=\"Year of the tutorial\")\n",
    "    exercises: list[ExerciseAnswers] = Field(..., description=\"List of tutorial questions\")\n",
    "\n",
    "def extract_tutorial_answers(doc_page_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual exercises from a tutorial sheet.\n",
    "\n",
    "    This function takes the content of a tutorial sheet (doc.page_content), constructs a prompt\n",
    "    instructing the LLM to infer the tutorial title and to split the text into separate questions.\n",
    "    The output must be a valid JSON string with the following structure:\n",
    "    \n",
    "    {\n",
    "        \"name\": \"<title of tutorial>\",\n",
    "        \"year\": \"<year of tutorial>\",\n",
    "        \"exercise\": [\n",
    "            { title: \"exercise text 1\", workedSolutions: [\"workedSolution text 1\", \"workedSolution text 2\", ...] },\n",
    "            { title: \"exercise text 2\", workedSolutions: [\"workedSolution text 1\", \"workedSolution text 2\", ...] },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    The tutorial sheet (IMPORTED_TUTORIAL) may contain reference solutions; do not alter\n",
    "    the original text of the exercises. The function returns a dictionary parsed from the JSON output.\n",
    "    \n",
    "    Args:\n",
    "        doc_page_content (str): The content of the tutorial sheet.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    # Initialize the output parser with the TutorialAnswers schema.\n",
    "    parser = PydanticOutputParser(pydantic_object=TutorialAnswers)\n",
    "\n",
    "    # Construct the prompt, appending the parser's format instructions.\n",
    "    prompt = f\"\"\"\n",
    "        IMPORTED_TUTORIAL\n",
    "        ```markdown\n",
    "        {doc_page_content}\n",
    "        ```\n",
    "\n",
    "        IMPORTED_TUTORIAL is a tutorial sheet with several exercises. It may or may\n",
    "        not include reference solutions. Please infer the title of the tutorial from\n",
    "        the content, and extract each individual question as a separate string. Do\n",
    "        not modify the text of the exercises. Only use $...$ for math expressions.\n",
    "\n",
    "        If the exercise mentions figures, then find all the captions of figures (no links). \n",
    "        Keep the captions as \"Figure Q1 - ...\".\n",
    "\n",
    "        If the exercise mentions tables, then include the table in the content.\n",
    "\n",
    "        Return a valid JSON string with the following structure:\n",
    "        {parser.get_format_instructions()}\n",
    "        \"\"\"\n",
    "\n",
    "    # Call the LLM\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Debug: print the raw LLM response\n",
    "    print(\"Raw LLM Response:\")\n",
    "    print(response)\n",
    "\n",
    "    try:\n",
    "        # Parse the response using the output parser.\n",
    "        parsed_output = parser.parse(response.content)\n",
    "        # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "        return parsed_output.model_dump()\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing LLM response as JSON:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_tutorial_answers = extract_tutorial_answers(answers_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = imported_tutorial_answers[\"name\"] + \" \" + imported_tutorial_answers[\"year\"]\n",
    "\n",
    "print(f\"Title: {title}\\n\")\n",
    "\n",
    "question_answers = imported_tutorial_answers[\"exercises\"]\n",
    "\n",
    "# Loop over and print each question\n",
    "for idx, question in enumerate(question_answers, start=1):\n",
    "    print(f\"**Question {idx}**:\\n{question.get(\"title\")}\\n\")\n",
    "    print(\"Subquestions:\")\n",
    "    for subquestion in question.get(\"workedSolutions\", []):\n",
    "        print(f\"- {subquestion}\")\n",
    "    print(\"-\" * 40)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Form JSON Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the nested Pydantic models based on the JSON schema.\n",
    "class WorkedSolution(BaseModel):\n",
    "    content: str = Field(..., description=\"Worked solution content\")\n",
    "    title: str = Field(..., description=\"Worked solution title\")\n",
    "    children: list = []\n",
    "\n",
    "class Part(BaseModel):\n",
    "    answerContent: str = Field(..., description=\"Part answer text\")\n",
    "    content: str = Field(..., description=\"Part content text\")\n",
    "    orderNumber: int = Field(..., description=\"The order number of this part\")\n",
    "    responseAreas: list = Field(..., description=\"List of response areas\")\n",
    "    tutorial: list = Field(..., description=\"List of tutorial items\")\n",
    "    workedSolution: WorkedSolution = Field(..., description=\"Worked solution details\")\n",
    "\n",
    "class QuestionJson(BaseModel):\n",
    "    orderNumber: int = Field(..., description=\"The order number of the question\")\n",
    "    displayFinalAnswer: bool = Field(..., description=\"Flag to display the final answer\")\n",
    "    displayStructuredTutorial: bool = Field(..., description=\"Flag to display the structured tutorial\")\n",
    "    displayWorkedSolution: bool = Field(..., description=\"Flag to display the worked solution\")\n",
    "    masterContent: str = Field(..., description=\"Top level question content\")\n",
    "    parts: list[Part] = Field(..., description=\"List of question parts\")\n",
    "    publish: bool = Field(..., description=\"Publish flag\")\n",
    "    title: str = Field(..., description=\"Question title\")\n",
    "\n",
    "def create_question_json(question: str, answers: str) -> dict:\n",
    "    # Initialize the output parser using the defined Pydantic model.\n",
    "    parser = PydanticOutputParser(pydantic_object=QuestionJson)\n",
    "\n",
    "    # Minimum JSON template to guide the model. (Used as context.)\n",
    "    minimum_json_template = r'''{\n",
    "      \"orderNumber\": 0,\n",
    "      \"displayFinalAnswer\": true,\n",
    "      \"displayStructuredTutorial\": true,\n",
    "      \"displayWorkedSolution\": true,\n",
    "      \"displayChatbot\": false,\n",
    "      \"masterContent\": \"Top level question here\",\n",
    "      \"parts\": [\n",
    "        {\n",
    "          \"answerContent\": \"\",\n",
    "          \"content\": \"Part text here\",\n",
    "          \"orderNumber\": 0,\n",
    "          \"responseAreas\": [],\n",
    "          \"tutorial\": [],\n",
    "          \"workedSolution\": {\n",
    "            \"content\": \"Part worked solution here\",\n",
    "            \"title\": \"\",\n",
    "            \"children\": []\n",
    "          }\n",
    "        }\n",
    "      ],\n",
    "      \"publish\": false,\n",
    "      \"title\": \"Question title here\"\n",
    "    }'''\n",
    "\n",
    "    # Construct the prompt, appending the parser's format instructions.\n",
    "    question_prompt = f'''\n",
    "      JSON_TEMPLATE\n",
    "      ```json\n",
    "      {minimum_json_template}\n",
    "      ```\n",
    "\n",
    "      IMPORTED_QUESTION\n",
    "      ```markdown\n",
    "      {question}\n",
    "      ```\n",
    "\n",
    "      IMPORTED_ANSWERS\n",
    "      ```markdown\n",
    "      {answers}\n",
    "      ```\n",
    "\n",
    "      Preserve the markdown math formatting to use $...$ for math expressions. Do not modify the original text of the question.\n",
    "\n",
    "      From the worked solution content of each part of IMPORTED_ANSWERS, infer the \n",
    "      final answer and put it in the answerContent field of the part. The worked solution\n",
    "      should be the full worked solution for the part, including all steps. The worked\n",
    "      solution should be in the workedSolution.content field. \n",
    "\n",
    "      Carefully map IMPORTED_QUESTION and IMPORTED_ANSWERS into the JSON_TEMPLATE and return valid JSON.\n",
    "\n",
    "      {parser.get_format_instructions()}\n",
    "      '''\n",
    "\n",
    "    # Invoke the language model.\n",
    "    response = llm.invoke(question_prompt)\n",
    "\n",
    "    try:\n",
    "        # Parse the response using the output parser.\n",
    "        parsed_output = parser.parse(response.content)\n",
    "        return parsed_output.model_dump()  # Return as a dictionary.\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON from LLM response:\", e)\n",
    "        print(\"LLM response:\", response.content)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_figure_references_to_questions(figures: dict, question_json: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Replaces figure reference text in the question_json content with markdown links,\n",
    "    specifically for references matching the pattern 'Figure Q1 - label'.\n",
    "\n",
    "    Args:\n",
    "        figures (dict): Dictionary of figures with their metadata (title, label, URL).\n",
    "        question_json (dict): JSON object containing question data.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated question_json with figure references replaced.\n",
    "    \"\"\"\n",
    "    for fig_name, fig_info in figures.items():\n",
    "        figure_title = fig_info[\"title\"]\n",
    "        figure_label = fig_info[\"label\"]\n",
    "        figure_url = fig_info[\"url\"]\n",
    "        print(f\"Figure: {fig_info}\")\n",
    "        if fig_info[\"answerFile\"] == True:\n",
    "            # Skip figures that are part of the answers\n",
    "            print(f\"Skipping figure {fig_name} as it is part of the answers.\")\n",
    "            continue\n",
    "        print(f\"Adding figure {figure_title} - {figure_label} to question JSON.\")\n",
    "\n",
    "        # Construct the markdown format for the figure\n",
    "        markdown_figure = f\"![Figure {figure_title} - {figure_label}]({figure_url})\\n\"\n",
    "\n",
    "        # Define the specific pattern to match 'Figure Q1 - label'\n",
    "        pattern = rf\"Figure\\s+{re.escape(figure_title)}\\s*-\\s*{re.escape(figure_label)}\"\n",
    "\n",
    "        # Replace exact matches in masterContent\n",
    "        question_json[\"masterContent\"] = re.sub(\n",
    "            pattern, markdown_figure, question_json[\"masterContent\"]\n",
    "        )\n",
    "\n",
    "        # Replace exact matches in each part's content\n",
    "        for part in question_json.get(\"parts\", []):\n",
    "            part[\"content\"] = re.sub(\n",
    "                pattern, markdown_figure, part[\"content\"]\n",
    "            )\n",
    "\n",
    "    return question_json\n",
    "\n",
    "# NOTE: Should not depend on mathpix jpeg urls, and just use the local files\n",
    "# TODO: but this errors when uploading to LambdaFeedback\n",
    "def add_local_figures_to_questions(figures: dict, question_json: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Replaces figure reference text in the question_json content with local file paths,\n",
    "    specifically for references matching the pattern 'Figure Q1 - label'.\n",
    "\n",
    "    Args:\n",
    "        figures (dict): Dictionary of figures with their metadata (title, label, URL).\n",
    "        question_json (dict): JSON object containing question data.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated question_json with figure references replaced.\n",
    "    \"\"\"\n",
    "    for fig_name, fig_info in figures.items():\n",
    "        figure_title = fig_info[\"title\"]\n",
    "        figure_label = fig_info[\"label\"]\n",
    "        figure_url = fig_info[\"url\"]\n",
    "        print(f\"Figure: {fig_info}\")\n",
    "        if fig_info[\"answerFile\"] == True:\n",
    "            # Skip figures that are part of the answers\n",
    "            print(f\"Skipping figure {fig_name} as it is part of the answers.\")\n",
    "            continue\n",
    "        print(f\"Adding figure {figure_title} - {figure_label} to question JSON.\")\n",
    "        \n",
    "        # Construct the markdown format for the figure\n",
    "        local_figure_path = f\"media/figure_{figure_title}.png\"\n",
    "        markdown_figure = f\"![Figure {figure_title} - {figure_label}]({local_figure_path})\\n\"\n",
    "\n",
    "        # Define the specific pattern to match 'Figure Q1 - label'\n",
    "        pattern = rf\"Figure\\s+{re.escape(figure_title)}\\s*-\\s*{re.escape(figure_label)}\"\n",
    "\n",
    "        # Replace exact matches in masterContent\n",
    "        question_json[\"masterContent\"] = re.sub(\n",
    "            pattern, markdown_figure, question_json[\"masterContent\"]\n",
    "        )\n",
    "\n",
    "        # Replace exact matches in each part's content\n",
    "        for part in question_json.get(\"parts\", []):\n",
    "            part[\"content\"] = re.sub(\n",
    "                pattern, markdown_figure, part[\"content\"]\n",
    "            )\n",
    "\n",
    "    return question_json\n",
    "\n",
    "# NOTE: FIGURES IN ANSWERS ARE HANDLED, they need to be added manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = imported_tutorial[\"exercises\"]\n",
    "question_answers = imported_tutorial_answers[\"exercises\"]\n",
    "\n",
    "# Loop over all questions and question_answers and print each question\n",
    "for idx, question, question_ans in zip(range(1, len(questions)+1), questions, question_answers):\n",
    "    print(f\"**Question {idx}**:\\n{question}\\n\")\n",
    "    print(f\"**Question Answers {idx}**:\\n{question_ans}\\n\")\n",
    "\n",
    "    print(\"INFO: Mapping question in markdown into JSON\")\n",
    "    question_json = create_question_json(question, question_ans)\n",
    "    question_json[\"orderNumber\"] = idx-1\n",
    "    print(f\"INFO: JSON {idx}:\\n{question_json}\\n\")\n",
    "    \n",
    "    print(\"INFO: Get figures\")\n",
    "    # updated_question_json = add_figure_references_to_questions(figures, question_json)\n",
    "    # updated_question_json = add_local_figures_to_questions(figures, question_json)\n",
    "    updated_question_json = question_json\n",
    "\n",
    "    question_name = updated_question_json[\"title\"].replace(\" \", \"_\")\n",
    "    question_index = f\"{(idx-1):03}\" \n",
    "    filename = f\"{set_path}question_{question_index}_{question_name}.json\"\n",
    "    print(f\"INFO: writing {filename}\")\n",
    "    open(filename, \"w\").write(json.dumps(updated_question_json, indent=2))\n",
    "    \n",
    "    # break # breaking here as just doing quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tutorial_metadata(tutorial_title: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a metadata JSON object for a tutorial.\n",
    "\n",
    "    The metadata includes a normalized short name (generated by lowercasing the\n",
    "    title, replacing spaces with underscores, and removing unsafe characters),\n",
    "    as well as several fixed visibility settings and a release date.\n",
    "\n",
    "    Args:\n",
    "        tutorial_title (str): The full tutorial title.\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "        dict: A dictionary containing the metadata.\n",
    "    \"\"\"\n",
    "    # Generate a short name for the tutorial (could name fancier using LLM).\n",
    "    # - replace spaces with underscores,\n",
    "    # - remove any characters except letters, numbers, underscores, and hyphens.\n",
    "    normalized_name = re.sub(r'\\s+', '_', tutorial_title)  # replace spaces with underscores        \n",
    "    normalized_name = re.sub(r'[^a-zA-Z0-9_]', '', normalized_name)  # remove unsafe characters\n",
    "\n",
    "    # Build the metadata dictionary\n",
    "    metadata = {\n",
    "        \"name\": tutorial_title,\n",
    "        \"description\": \"\",  # Optional description of the tutorial\n",
    "        \"manuallyHidden\": True,  # Defaults to true\n",
    "        \"finalAnswerVisibility\": \"OPEN_WITH_WARNINGS\",\n",
    "        \"workedSolutionVisibility\": \"OPEN_WITH_WARNINGS\",\n",
    "        \"structuredTutorialVisibility\": \"OPEN\",\n",
    "        \"chatbotVisibility\": \"HIDE\"\n",
    "    }\n",
    "    tutorial_normalized_title = normalized_name\n",
    "    \n",
    "    return metadata, tutorial_normalized_title\n",
    "\n",
    "tutorial_title = imported_tutorial[\"name\"] + \" \" + imported_tutorial[\"year\"]\n",
    "metadata, tutorial_normalized_title = create_tutorial_metadata(tutorial_title)\n",
    "\n",
    "set_filename = f\"set_{tutorial_normalized_title}.json\"\n",
    "\n",
    "print(f\"Saving metadata to {set_path}{set_filename}...\")\n",
    "json.dump(metadata, open(f\"{set_path}{set_filename}\", \"w\"), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
