{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_math_to_markdown(text: str, llm) -> str:\n",
    "    \"\"\"\n",
    "    Uses the LLM to convert scientific text (including mathematics)\n",
    "    into markdown (e.g. wrapping inline math with $ or display math with $$).\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Convert the following scientific text with mathematics into markdown format. \"\n",
    "        \"Ensure that all mathematical expressions are properly formatted using inline ($...$) \"\n",
    "        \"markdown syntax as appropriate.\\n\\n\"\n",
    "        f\"{text}\"\n",
    "    )\n",
    "    markdown_text = llm.invoke(prompt)\n",
    "    return markdown_text.content\n",
    "\n",
    "\n",
    "def extract_caption_from_bbox(page, bbox, threshold: float = 50) -> str:\n",
    "    caption_candidates = []\n",
    "    # Iterate over each block.\n",
    "    for block in page.get_text(\"blocks\"):\n",
    "        # Unpack the first five elements and ignore the rest.\n",
    "        bx0, by0, bx1, by1, text, *_ = block\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        # If the block's top coordinate is just below the image bounding box.\n",
    "        if by0 >= bbox[3] and (by0 - bbox[3]) < threshold:\n",
    "            caption_candidates.append((by0, text.strip()))\n",
    "    if caption_candidates:\n",
    "        # Return the caption of the block closest to the image.\n",
    "        caption_candidates.sort(key=lambda t: t[0])\n",
    "        return caption_candidates[0][1]\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_caption(caption: str) -> (str, str):\n",
    "    \"\"\"\n",
    "    Given a caption string, try to parse out a title and label.\n",
    "    For example, if the caption includes tokens like “Title: …” or “Label: …”.\n",
    "    \"\"\"\n",
    "    title = \"\"\n",
    "    label = \"\"\n",
    "    m_title = re.search(r\"Title\\s*[:\\-]\\s*(.+?)(,|$)\", caption, re.IGNORECASE)\n",
    "    if m_title:\n",
    "        title = m_title.group(1).strip()\n",
    "    m_label = re.search(r\"Label\\s*[:\\-]\\s*(.+?)(,|$)\", caption, re.IGNORECASE)\n",
    "    if m_label:\n",
    "        label = m_label.group(1).strip()\n",
    "    return title, label\n",
    "\n",
    "\n",
    "def get_figure_name(caption: str, default_name: str) -> str:\n",
    "    \"\"\"\n",
    "    If the caption contains a reference like “Figure 2”, return a name based on that.\n",
    "    Otherwise return the provided default name.\n",
    "    \"\"\"\n",
    "    if caption:\n",
    "        m_fig = re.search(r\"Figure\\s*(\\d+)\", caption, re.IGNORECASE)\n",
    "        if m_fig:\n",
    "            return f\"Figure_{m_fig.group(1)}\"\n",
    "    return default_name\n",
    "\n",
    "\n",
    "def process_pdf(pdf_path: str, llm):\n",
    "    \"\"\"\n",
    "    Reads a PDF file, extracts its text (with mathematics converted to markdown)\n",
    "    and extracts images (with associated captions parsed for title and label).\n",
    "    \n",
    "    Returns a LangChain Document whose page_content is the markdown text and whose\n",
    "    metadata includes a dictionary 'figures' mapping figure names to a dict:\n",
    "      { \"image\": <PIL.Image>, \"title\": <str>, \"label\": <str> }.\n",
    "    \"\"\"\n",
    "    # 1. Extract the main text using a loader optimized for scientific content.\n",
    "    text_loader = UnstructuredPDFLoader(pdf_path)\n",
    "    docs = text_loader.load()\n",
    "    combined_text = \"\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # Ignire the first page if it contains a title.\n",
    "    if len(docs) > 1 and docs[0].page_content.strip() == docs[1].page_content.strip():\n",
    "        combined_text = \"\\n\".join(doc.page_content for doc in docs[1:])\n",
    "    # Remove any leading or trailing whitespace.\n",
    "    combined_text = combined_text.strip()\n",
    "\n",
    "    # Stop processing text after \"Appendices\"\n",
    "    appendices_index = combined_text.find(\"Appendices\")\n",
    "    if appendices_index != -1:\n",
    "        combined_text = combined_text[:appendices_index]\n",
    "    \n",
    "    # Remove the [number%]\n",
    "    combined_text = re.sub(r\"\\[\\d+%\\]\", \"\", combined_text)\n",
    "    \n",
    "    # 2. Use the LLM to convert mathematics in the text to proper markdown.\n",
    "    markdown_text = convert_math_to_markdown(combined_text, llm)\n",
    "    \n",
    "    # 3. Extract images and associated metadata using PyMuPDF.\n",
    "    pdf_doc = fitz.open(pdf_path)\n",
    "    figures = {}\n",
    "    \n",
    "    for page_num in range(len(pdf_doc)):\n",
    "        page = pdf_doc[page_num]\n",
    "        # Use the \"dict\" interface for richer info.\n",
    "        page_dict = page.get_text(\"dict\")\n",
    "        blocks = page_dict.get(\"blocks\", [])\n",
    "\n",
    "        # Extract embedded images\n",
    "        images = page.get_images(full=True)\n",
    "        for img_index, img in enumerate(images):\n",
    "            xref = img[0]\n",
    "            base_image = pdf_doc.extract_image(xref)\n",
    "            image_bytes = base_image[\"image\"]\n",
    "            img_obj = Image.open(BytesIO(image_bytes))\n",
    "            img_obj.save(f\"media/page_{page_num + 1}_img_{img_index}.png\")\n",
    "\n",
    "        for block in blocks:\n",
    "            if isinstance(block, dict) and block.get(\"type\") == 1:\n",
    "                xref = block.get(\"image\")\n",
    "                bbox = block.get(\"bbox\")\n",
    "                if not xref:\n",
    "                    continue\n",
    "\n",
    "                # Check the type of xref.\n",
    "                if isinstance(xref, int):\n",
    "                    # xref is valid; extract the image using PyMuPDF.\n",
    "                    base_image = pdf_doc.extract_image(xref)\n",
    "                    image_bytes = base_image[\"image\"]\n",
    "                elif isinstance(xref, bytes):\n",
    "                    # xref already contains the image bytes (inline image).\n",
    "                    image_bytes = xref\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                # Open the image with PIL.\n",
    "                img_obj = Image.open(BytesIO(image_bytes))\n",
    "                \n",
    "                # Try to extract a caption near the image.\n",
    "                caption = extract_caption_from_bbox(page, bbox)\n",
    "                title, label = (\"\", \"\")\n",
    "                if caption:\n",
    "                    title, label = parse_caption(caption)\n",
    "                default_fig_name = f\"page{page_num+1}_img_{xref if isinstance(xref, int) else 'inline'}\"\n",
    "                fig_name = get_figure_name(caption, default_fig_name) if caption else default_fig_name\n",
    "                figures[fig_name] = {\"image\": img_obj, \"title\": title, \"label\": label}\n",
    "    \n",
    "    pdf_doc.close()\n",
    "    \n",
    "    # 4. Create and return a LangChain Document.\n",
    "    metadata = {\"figures\": figures}\n",
    "    final_doc = Document(page_content=markdown_text, metadata=metadata)\n",
    "    return final_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file.\n",
    "load_dotenv()\n",
    "\n",
    "# Set up the Azure OpenAI LLM via LangChain.\n",
    "temperature = 0\n",
    "llm = ChatOpenAI(\n",
    "            model=os.environ['OPENAI_MODEL'],\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )\n",
    "# llm = ChatGoogleGenerativeAI(\n",
    "#             model=os.environ['GOOGLE_AI_MODEL'],\n",
    "#             temperature=temperature,\n",
    "#             google_api_key=os.environ['GOOGLE_AI_API_KEY'],\n",
    "#         )\n",
    "\n",
    "pdf_file_path = \"pastpapers/2023_paper.pdf\"\n",
    "doc = process_pdf(pdf_file_path, llm)\n",
    "\n",
    "# Print out a summary.\n",
    "print(\"Markdown text: \")\n",
    "print(doc.page_content)\n",
    "\n",
    "# Needs to be improved to infer name/label/caption based on context.\n",
    "figures = doc.metadata.get(\"figures\", {})\n",
    "print(\"\\nExtracted figures:\")\n",
    "\n",
    "Path(\"media\").mkdir(exist_ok=True)\n",
    "for idx, (fig_name, fig_info) in enumerate(figures.items()):\n",
    "    print(f\"  {fig_name}: Title='{fig_info['title']}', Label='{fig_info['label']}'\")\n",
    "    # Save each image as a PNG file with a sequential name: figure_0.png, figure_1.png, etc.\n",
    "    fig_info[\"image\"].save(f\"media/figure_{idx}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the tutorial output.\n",
    "class Exercise(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the exercise (only the text, no numbering)\")\n",
    "    content: str = Field(..., description=\"Content of the exercise (no exercise title, no subquestions)\")\n",
    "    subquestions: list[str] = Field(..., description=\"List of subquestions within the exercise (only the text, no numbering)\")\n",
    "    \n",
    "class Tutorial(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the tutorial\")\n",
    "    year: str = Field(..., description=\"Year of the tutorial\")\n",
    "    exercises: list[Exercise] = Field(..., description=\"List of tutorial questions\")\n",
    "\n",
    "def extract_tutorial_questions(doc_page_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual exercises from a tutorial sheet.\n",
    "\n",
    "    This function takes the content of a tutorial sheet (doc.page_content), constructs a prompt\n",
    "    instructing the LLM to infer the tutorial title and to split the text into separate questions.\n",
    "    The output must be a valid JSON string with the following structure:\n",
    "    \n",
    "    {\n",
    "        \"name\": \"<title of tutorial>\",\n",
    "        \"year\": \"<year of tutorial>\",\n",
    "        \"exercise\": [\n",
    "            { title: \"exercise text 1\", content: \"content text exercise 1\", subquestions: [\"subquestion text 1\", \"subquestion text 2\", ...] },\n",
    "            { title: \"exercise text 2\", content: \"content text exercise 2\", subquestions: [\"subquestion text 1\", \"subquestion text 2\", ...] },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    The tutorial sheet (IMPORTED_TUTORIAL) may contain reference solutions; do not alter\n",
    "    the original text of the exercises. The function returns a dictionary parsed from the JSON output.\n",
    "    \n",
    "    Args:\n",
    "        doc_page_content (str): The content of the tutorial sheet.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    # Initialize the output parser with the Tutorial schema.\n",
    "    parser = PydanticOutputParser(pydantic_object=Tutorial)\n",
    "\n",
    "    # Construct the prompt, appending the parser's format instructions.\n",
    "    prompt = f\"\"\"\n",
    "        IMPORTED_TUTORIAL\n",
    "        ```markdown\n",
    "        {doc_page_content}\n",
    "        ```\n",
    "\n",
    "        IMPORTED_TUTORIAL is a tutorial sheet with several exercises. It may or may\n",
    "        not include reference solutions. Please infer the title of the tutorial from\n",
    "        the content, and extract each individual question as a separate string. Do\n",
    "        not modify the text of the exercises. Only use $...$ for math expressions.\n",
    "\n",
    "        Return a valid JSON string with the following structure:\n",
    "        {parser.get_format_instructions()}\n",
    "        \"\"\"\n",
    "\n",
    "    # Call the LLM\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Debug: print the raw LLM response\n",
    "    print(\"Raw LLM Response:\")\n",
    "    print(response)\n",
    "\n",
    "    try:\n",
    "        # Parse the response using the output parser.\n",
    "        parsed_output = parser.parse(response.content)\n",
    "        # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "        return parsed_output.model_dump()\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing LLM response as JSON:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_tutorial = extract_tutorial_questions(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title\n",
    "title = imported_tutorial[\"name\"] + \" \" + imported_tutorial[\"year\"]\n",
    "\n",
    "# Print the title\n",
    "print(f\"Title: {title}\\n\")\n",
    "\n",
    "# Extract questions\n",
    "questions = imported_tutorial[\"exercises\"]\n",
    "\n",
    "# Loop over and print each question\n",
    "for idx, question in enumerate(questions, start=1):\n",
    "    print(f\"**Question {idx}**:\\n{question}\\n\")\n",
    "    # print(\"Subquestions:\")\n",
    "    # for subquestion in question.get(\"subquestions\", []):\n",
    "    #     print(f\"- {subquestion}\")\n",
    "    print(\"-\" * 40)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tutorial_metadata(tutorial_title: str) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a metadata JSON object for a tutorial.\n",
    "\n",
    "    The metadata includes a normalized short name (generated by lowercasing the\n",
    "    title, replacing spaces with underscores, and removing unsafe characters),\n",
    "    as well as several fixed visibility settings and a release date.\n",
    "\n",
    "    Args:\n",
    "        tutorial_title (str): The full tutorial title.\n",
    "\n",
    "    Returns:\n",
    "    \n",
    "        dict: A dictionary containing the metadata.\n",
    "    \"\"\"\n",
    "    # Generate a short name for the tutorial (could name fancier using LLM).\n",
    "    # - convert to lower-case,\n",
    "    # - replace spaces with underscores,\n",
    "    # - remove any characters except letters, numbers, underscores, and hyphens.\n",
    "    normalized_name = tutorial_title.lower()  # convert to lower-case\n",
    "    normalized_name = re.sub(r'\\s+', '_', normalized_name)         # replace spaces with underscores\n",
    "    normalized_name = re.sub(r'[^a-z0-9_-]', '', normalized_name)   # remove other characters\n",
    "\n",
    "    # Build the metadata dictionary\n",
    "    metadata = {\n",
    "        \"name\": tutorial_title,\n",
    "        \"description\": \"\",  # Optional description of the tutorial\n",
    "        \"releasedAt\": \"2024-09-30T11:00:00.000Z\",  # ISO 8601 release date\n",
    "        \"manuallyHidden\": True,  # Defaults to true\n",
    "        \"finalAnswerVisibility\": \"OPEN_WITH_WARNINGS\",\n",
    "        \"workedSolutionVisibility\": \"OPEN_WITH_WARNINGS\",\n",
    "        \"structuredTutorialVisibility\": \"OPEN\",\n",
    "        \"chatbotVisibility\": \"HIDE\"\n",
    "    }\n",
    "    tutorial_normalized_title = normalized_name\n",
    "    \n",
    "    return metadata, tutorial_normalized_title\n",
    "\n",
    "tutorial_title = imported_tutorial[\"name\"] + \" \" + imported_tutorial[\"year\"]\n",
    "metadata, tutorial_normalized_title = create_tutorial_metadata(tutorial_title)\n",
    "\n",
    "set_filename = f\"set_{tutorial_normalized_title}.json\"\n",
    "set_filepath = set_filename.split(\".\")[0]\n",
    "if f\"{set_filepath}\" not in os.listdir(\"pastpapers/json/\"):\n",
    "        os.mkdir(f\"pastpapers/json/{set_filepath}\")\n",
    "\n",
    "print(f\"Saving metadata to pastpapers/json/{set_filepath}/{set_filename}...\")\n",
    "json.dump(metadata, open(f\"pastpapers/json/{set_filepath}/{set_filename}\", \"w\"), indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the nested Pydantic models based on the JSON schema.\n",
    "class WorkedSolution(BaseModel):\n",
    "    content: str = Field(..., description=\"Worked solution content\")\n",
    "    id: str = Field(..., description=\"Identifier for the worked solution\")\n",
    "    title: str = Field(..., description=\"Worked solution title\")\n",
    "    children: list = []\n",
    "\n",
    "class Part(BaseModel):\n",
    "    answerContent: str = Field(..., description=\"Part answer text\")\n",
    "    content: str = Field(..., description=\"Part content text\")\n",
    "    orderNumber: int = Field(..., description=\"The order number of this part\")\n",
    "    responseAreas: list = Field(..., description=\"List of response areas\")\n",
    "    tutorial: list = Field(..., description=\"List of tutorial items\")\n",
    "    universalPartId: str = Field(..., description=\"Universal part identifier\")\n",
    "    workedSolution: WorkedSolution = Field(..., description=\"Worked solution details\")\n",
    "\n",
    "class QuestionJson(BaseModel):\n",
    "    orderNumber: int = Field(..., description=\"The order number of the question\")\n",
    "    displayFinalAnswer: bool = Field(..., description=\"Flag to display the final answer\")\n",
    "    displayStructuredTutorial: bool = Field(..., description=\"Flag to display the structured tutorial\")\n",
    "    displayWorkedSolution: bool = Field(..., description=\"Flag to display the worked solution\")\n",
    "    masterContent: str = Field(..., description=\"Top level question content\")\n",
    "    parts: list[Part] = Field(..., description=\"List of question parts\")\n",
    "    publish: bool = Field(..., description=\"Publish flag\")\n",
    "    title: str = Field(..., description=\"Question title\")\n",
    "\n",
    "def create_question_json(question: str) -> dict:\n",
    "    # Initialize the output parser using the defined Pydantic model.\n",
    "    parser = PydanticOutputParser(pydantic_object=QuestionJson)\n",
    "\n",
    "    # Minimum JSON template to guide the model. (Used as context.)\n",
    "    minimum_json_template = r'''{\n",
    "      \"orderNumber\": 0,\n",
    "      \"displayFinalAnswer\": true,\n",
    "      \"displayStructuredTutorial\": true,\n",
    "      \"displayWorkedSolution\": true,\n",
    "      \"masterContent\": \"Top level question here\",\n",
    "      \"parts\": [\n",
    "        {\n",
    "          \"answerContent\": \"\",\n",
    "          \"content\": \"Part text here\",\n",
    "          \"orderNumber\": 0,\n",
    "          \"responseAreas\": [],\n",
    "          \"tutorial\": [],\n",
    "          \"universalPartId\": \"N/A\",\n",
    "          \"workedSolution\": {\n",
    "            \"content\": \"Part worked solution here\",\n",
    "            \"id\": \"N/A\",\n",
    "            \"title\": \"\",\n",
    "            \"children\": []\n",
    "          }\n",
    "        }\n",
    "      ],\n",
    "      \"publish\": false,\n",
    "      \"title\": \"Question title here\"\n",
    "    }'''\n",
    "\n",
    "    # Construct the prompt, appending the parser's format instructions.\n",
    "    question_prompt = f'''\n",
    "      JSON_TEMPLATE\n",
    "      ```json\n",
    "      {minimum_json_template}\n",
    "      ```\n",
    "\n",
    "      IMPORTED_QUESTION\n",
    "      ```markdown\n",
    "      {question}\n",
    "      ```\n",
    "\n",
    "      If you see something like \"HII 5\\u201310 mins\\n\\n\", drop it from the text. Preserve the Katex\n",
    "      math formatting. Do not modify the original text of the question.\n",
    "\n",
    "      Carefully map IMPORTED_QUESTION into the JSON_TEMPLATE and return valid JSON.\n",
    "\n",
    "      {parser.get_format_instructions()}\n",
    "      '''\n",
    "\n",
    "    # Invoke the language model.\n",
    "    response = llm.invoke(question_prompt)\n",
    "\n",
    "    try:\n",
    "        # Parse the response using the output parser.\n",
    "        parsed_output = parser.parse(response.content)\n",
    "        return parsed_output.model_dump()  # Return as a dictionary.\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON from LLM response:\", e)\n",
    "        print(\"LLM response:\", response.content)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# Define a Pydantic model representing the expected output schema.\n",
    "class QuestionName(BaseModel):\n",
    "    question_name: str = Field(..., description=\"A short tag representing the question’s topic.\")\n",
    "\n",
    "def create_question_name(question: str) -> str:\n",
    "    # Initialize the output parser with the schema.\n",
    "    parser = PydanticOutputParser(pydantic_object=QuestionName)\n",
    "\n",
    "    # Build a prompt that includes the parser's format instructions.\n",
    "    question_name_prompt = f'''\n",
    "      IMPORTED_QUESTION\n",
    "      ```markdown\n",
    "      {question}\n",
    "      ```\n",
    "      \n",
    "      QUERY:\n",
    "      Based on the above markdown content, infer a suitable short name tag that represents the question’s topic.\n",
    "      Follow these rules:\n",
    "      \n",
    "        1. Look for the heading text (for example, if the text starts with \"Question 1:\" followed by \"Hydraulic scale\", then the main topic is \"Hydraulic scale\").\n",
    "        2. Normalize the name by replacing spaces with underscores and removing punctuation.\n",
    "        3. Return only a valid JSON object with a single property \"question_name\".\n",
    "      \n",
    "      {parser.get_format_instructions()}\n",
    "    '''\n",
    "    # Invoke the language model.\n",
    "    response = llm.invoke(question_name_prompt)\n",
    "\n",
    "    try:\n",
    "        # Parse the response using the output parser.\n",
    "        parsed_output = parser.parse(response.content)\n",
    "        return parsed_output.question_name\n",
    "    except Exception as e:\n",
    "        print(\"Error parsing JSON from LLM response:\", e)\n",
    "        print(\"LLM response:\", response.content)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over and print each question\n",
    "for idx, question in enumerate(questions, start=1):\n",
    "    print(f\"**Question {idx}**:\\n{question}\\n\")\n",
    "\n",
    "    print(\"INFO: Mapping question in markdown into JSON\")\n",
    "    question_json = create_question_json(question)\n",
    "    question_json[\"orderNumber\"] = idx-1\n",
    "    print(f\"INFO: JSON {idx}:\\n{question_json}\\n\")\n",
    "    \n",
    "    print(\"INFO: Get question name.\")\n",
    "    question_name = create_question_name(question)\n",
    "\n",
    "    if f\"{set_filepath}\" not in os.listdir(\"pastpapers/json/\"):\n",
    "        os.mkdir(f\"pastpapers/json/{set_filepath}\")\n",
    "    \n",
    "    question_index = f\"{(idx-1):03}\" \n",
    "    filename = f\"pastpapers/json/{set_filepath}/question_{question_index}_{question_name}.json\"\n",
    "    print(f\"INFO: writing {filename}\")\n",
    "    open(filename, \"w\").write(json.dumps(question_json, indent=2))\n",
    "    \n",
    "    # break # breaking here as just doing quick test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
