{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# process description  \n",
    "\n",
    "the program takes in a pdf  \n",
    "mathpix is used to scan the pdf and turning it into markdown  \n",
    "markdown then processed to get the images  \n",
    "\n",
    "llm is used to extract the questions and solutions in **ONE** go.  \n",
    "the final JSON is made using the in2lambda api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import pypandoc\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from in2lambda.api.module import Module\n",
    "from in2lambda.api.question import Question\n",
    "from in2lambda.api.part import Part\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load environment variables from .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# scanning/processing the initial pdf into markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATHPIX_API_KEY = os.getenv(\"MATHPIX_API_KEY\")\n",
    "MATHPIX_APP_ID = os.getenv(\"MATHPIX_APP_ID\")\n",
    "\n",
    "def pdf_to_markdown(source_path: str, result_path: str):\n",
    "    ''' \n",
    "    converts the pdf at `source_path` to a markdown file at `result_path` using Mathpix API.\n",
    "    '''\n",
    "    # Upload PDF to Mathpix and returns a Markdown file with the content.\n",
    "    with open(source_path, \"rb\") as file:\n",
    "        r = requests.post(\n",
    "            \"https://api.mathpix.com/v3/pdf\",   \n",
    "            headers={\n",
    "                \"app_id\": MATHPIX_APP_ID,\n",
    "                \"app_key\": MATHPIX_API_KEY,\n",
    "            },\n",
    "            files={\"file\": file},\n",
    "        )\n",
    "        pdf_id = r.json()[\"pdf_id\"]\n",
    "        print(\"PDF ID:\", pdf_id)\n",
    "        print(\"Response:\", r.json())\n",
    "\n",
    "        # url of where the location of the processed PDF will be\n",
    "        url = f\"https://api.mathpix.com/v3/pdf/{pdf_id}.md\"\n",
    "        headers = {\n",
    "            \"app_id\": MATHPIX_APP_ID,\n",
    "            \"app_key\": MATHPIX_API_KEY,\n",
    "        }\n",
    "\n",
    "        max_retries = 10\n",
    "        retry_delay = 5  # seconds\n",
    "        for attempt in range(max_retries):\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                # Save the result if the request is successful\n",
    "                with open(result_path, \"w\") as f:\n",
    "                    f.write(response.text)\n",
    "                print(\"Downloaded MD successfully.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}/{max_retries}: Processing not complete. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "        else:\n",
    "            print(\"Failed to retrieve processed PDF after multiple attempts:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# setting up the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the output folder and media folder.\n",
    "folder_path = \"conversion_content\"\n",
    "input_path = f\"{folder_path}/input\"\n",
    "output_path = f\"{folder_path}/mathpix_to_llm_to_in2lambda_to_JSON_out\"\n",
    "media_path = f\"{output_path}/media\"\n",
    "\n",
    "# Create output and media directories if they do not exist.\n",
    "Path(media_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# location of the source pdf file and the result markdown file.\n",
    "files = [f for f in os.listdir(input_path) if f != '.gitkeep']\n",
    "source_path = f\"{input_path}/{files[0]}\" # the first file in the input folder\n",
    "result_path = f\"{output_path}/example.md\"\n",
    "\n",
    "\n",
    "\n",
    "# Only activate mathpix if the markdown has not been created yet.\n",
    "# This avoids unnecessary reprocessing of the same PDF.\n",
    "if not Path(result_path).exists():\n",
    "    if Path(source_path).exists():\n",
    "        extension = Path(source_path).suffix.lower() # obtains the file extension\n",
    "        if extension == \".pdf\":\n",
    "            pdf_to_markdown(source_path, result_path)\n",
    "        else:\n",
    "            pypandoc.convert_file(source_path, 'md', outputfile=result_path)\n",
    "    else:\n",
    "        print(f\"Error: Source PDF file not found at {source_path}\")\n",
    "        exit(1)\n",
    "\n",
    "# Read the markdown content from the result file.\n",
    "try:\n",
    "    with open(result_path, \"r\") as f:\n",
    "        md_content = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Markdown file not found at {result_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Print out a summary.\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {len(md_content)} characters\")\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {md_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# downlaoding extracted images from Mathpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the figures from the paper and answers.\n",
    "def extract_figures_from_text(text): #, ans=False):\n",
    "    \"\"\"\n",
    "    Extracts figures from the text using regex.\n",
    "    Finds figure references and their descriptions.\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    # Regex to match figure references and their descriptions\n",
    "    # Matches ![alt text](url) format for images\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    print(f\"Matches found: {matches}\")\n",
    "    \n",
    "    for match in matches:\n",
    "        url = match\n",
    "        url = url.strip()\n",
    "        \n",
    "        if url.startswith(\"http\"):\n",
    "            # Download the image and save it to a file\n",
    "            image = Image.open(requests.get(url, stream=True).raw)\n",
    "            # Create a figure name based on the URL\n",
    "            fig_name = os.path.basename(url)\n",
    "            figures[fig_name] = {\n",
    "                \"image\": image,\n",
    "                \"url\": url,\n",
    "                \"local_path\": \"\",\n",
    "                # \"answerFile\": ans\n",
    "            }\n",
    "    return figures\n",
    "\n",
    "# a dictionary storing information on the figures\n",
    "figures = extract_figures_from_text(md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# saving the images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figures_to_path(figures):\n",
    "    for idx, (fig_name, fig_info) in enumerate(figures.items()):\n",
    "        print(f\"URL='{fig_info['url']}'\")\n",
    "\n",
    "        # Extract file extension and create a clean filename\n",
    "        # Mathpix leaves image urls like `image.png?width=800&height=600`\n",
    "        # We only want the base name without query parameters.\n",
    "        if \"?\" in fig_name:\n",
    "            end_location = fig_name.index(\"?\")\n",
    "            image_name = f\"{idx}_{fig_name[:end_location]}\"\n",
    "        else:\n",
    "            image_name = f\"{idx}_{fig_name}\"\n",
    "        \n",
    "        fig_info[\"local_path\"] = image_name\n",
    "        try:\n",
    "            # Saves the image to the media path\n",
    "            fig_info[\"image\"].save(f\"{media_path}/{fig_info['local_path']}\")\n",
    "            print(f\"Saved image: {fig_info['local_path']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving image {image_name}: {e}\")\n",
    "\n",
    "save_figures_to_path(figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# replacing url for images with local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_figures_in_markdown(md_content, figures) -> str:\n",
    "    #replace the image URLs in the markdown content with local paths\n",
    "    # add pictureTag for Lambda Feedback to recognise it as a picture\n",
    "    md_content = md_content.replace(\"![]\", \"![pictureTag]\")\n",
    "    for fig_name, fig_info in figures.items():\n",
    "        md_content = md_content.replace(fig_info[\"url\"], fig_info[\"local_path\"])\n",
    "        print(f\"Replaced {fig_info['url']} with {fig_info['local_path']} in markdown content.\")\n",
    "    # Save the modified markdown content to a file\n",
    "    try:\n",
    "        with open(f\"{output_path}/example.md\", \"w\") as f:\n",
    "            f.write(md_content)\n",
    "        print(\"Modified markdown saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving modified markdown: {e}\")\n",
    "    \n",
    "    return md_content\n",
    "\n",
    "md_content = replace_figures_in_markdown(md_content, figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Initialising llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM via LangChain.\n",
    "\n",
    "# Uses gpt-4.1-nano:\n",
    "#    - a faster model\n",
    "#    - less intelligent\n",
    "\n",
    "llm_nano = ChatOpenAI(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )\n",
    "\n",
    "# Uses gpt-4.1-mini:\n",
    "#    - more intelligent\n",
    "llm_mini = ChatOpenAI(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Spelling and structure check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_task_correct_mistakes = \"\"\"\n",
    "The input is a markdown file that is converted from a pdf using Mathpix API.\n",
    "The pdf contains questions and may contain the solutions too.\n",
    "As the original pdf may contain hand written text, the markdown file may contain mistakes in spelling, grammar and structure.\n",
    "\n",
    "Important things to remember:\n",
    "    1. Leave all Math commands and LaTeX formatting the same. As they are completely valid. Do not change the LaTeX formatting and expressions.\n",
    "    2. Only ever use LaTeX math delimiters for math expressions. I.e. use `$...$` for inline math, and `$$...$$` for display math.\n",
    "    3. Leave references to images and figures the same. I.e. do not change the image links or alt text.\n",
    "\n",
    "Your task is to:\n",
    "    1. Correct any spelling mistakes in the markdown file.\n",
    "    2. Correct any grammar mistakes in the markdown file.\n",
    "    3. Correct any layout mistakes in the markdown file, such that it follows the styles of the entire markdown file.\n",
    "    4. Do not change the content of the markdown file, only correct the mistakes.\n",
    "Output only a valid markdown file with the corrections applied, if any. Do not add any additional text or comments.\n",
    "\"\"\"\n",
    "\n",
    "def correct_mistakes_in_markdown(md_content: str) -> str:\n",
    "    correct_mistakes_prompt = f\"\"\"\n",
    "        {llm_task_correct_mistakes}\n",
    "\n",
    "        ```input\n",
    "        {md_content}\n",
    "        ```\n",
    "\n",
    "        Return the markdown now.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_nano.invoke(correct_mistakes_prompt)\n",
    "    print(\"Corrected markdown content:\")\n",
    "    print(response.content.strip())\n",
    "\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Extract Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define initial question model\n",
    "class QuestionModelLines(BaseModel):\n",
    "    # full question and full solution\n",
    "    question_content_start: int = Field(..., description=\"Line number the question starts on.\")\n",
    "    question_content_end: int = Field(..., description=\"Line number the question ends on.\")\n",
    "    solution_content_start: int = Field(..., description=\"Line number the solution starts on.\")\n",
    "    solution_content_end: int = Field(..., description=\"Line number the solution ends on.\")\n",
    "\n",
    "class AllQuestionsModelLines(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[QuestionModelLines] = Field(..., description=\"A list of questions.\")\n",
    "\n",
    "llm_task_seperate_questions = \"\"\"\n",
    "    Your task is to extract the line numbers for the start and end of each question and solution from the markdown file, then format it as a JSON object.\n",
    "    These line numbers will be used later to extract the content of the questions and solutions procedurally.\n",
    "    \n",
    "    1.  **Content Extraction:**\n",
    "        -   Your may choose a suitable name for the set of questions.\n",
    "        -   Identify the `year` of the questions, otherwise use \"0\".\n",
    "        -   Begin by Identifying the questions in the markdown file, and for each question:\n",
    "            -   Identify the start and end line numbers of the full question content, and place them in `question_content_start` and `question_content_end`.\n",
    "            -   Identify the start and end line numbers of the full relevant solution content, and place them in `solution_content_start` and `solution_content_end`.\n",
    "            -   Be careful to ensure that everything related to the question and solution is included, including any math delimiters and LaTeX formatting.\n",
    "            -   Do not forget to include any images or figures that are part of the question or solution.\n",
    "    \n",
    "    2.  **Output Format:**\n",
    "        -   You MUST output ONLY a single, raw, valid JSON string that matches the provided schema.\n",
    "        -   Do NOT include any explanations, comments, or markdown code blocks (like ```json).\n",
    "    \"\"\"\n",
    "\n",
    "def llm_extract_questions_lines(doc_page_content: list[str]) -> dict:\n",
    "    # Initialise the parser for the output.\n",
    "    parser = PydanticOutputParser(pydantic_object=AllQuestionsModelLines)\n",
    "\n",
    "    # Prompt for the LLM to extract questions.\n",
    "    seperate_questions_prompt = f\"\"\"\n",
    "        Your task is to extract a JSON with the following structure exactly, ready to be parsed by a pydantic model:\n",
    "        {parser.get_format_instructions()}\n",
    "\n",
    "        {llm_task_seperate_questions}\n",
    "\n",
    "        Input markdown:\n",
    "        ```\n",
    "        {doc_page_content}\n",
    "        ```\n",
    "        Return the JSON now.\n",
    "    \"\"\"\n",
    "\n",
    "    for attempt_idx in range(3):\n",
    "        try:\n",
    "            response = llm_mini.invoke(seperate_questions_prompt)\n",
    "            return parser.parse(response.content).model_dump()\n",
    "        except ValidationError as e:\n",
    "            print(f\"Validation error on attempt {attempt_idx + 1}: {e}\")\n",
    "            if attempt_idx == 2:\n",
    "                raise e\n",
    "            else:\n",
    "                print(\"Retrying...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts image URLs from the markdown text.\n",
    "    Returns a list of image URLs.\n",
    "    \"\"\"\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionModel(BaseModel):\n",
    "    # full question and full solution\n",
    "    question_content: str = Field(..., description=\"The content of the question.\")\n",
    "    solution_content: str = Field(..., description=\"The content of the solution.\")\n",
    "    images: list[str] = Field(..., description=\"A list of image URLs associated with the question.\")\n",
    "\n",
    "class AllQuestionsModel(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[QuestionModel] = Field(..., description=\"A list of questions.\")\n",
    "\n",
    "\n",
    "def extract_questions(allQuestionsModel: dict, doc_page_content: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts questions from the AllQuestions model and returns a list of Question objects.\n",
    "    \"\"\"\n",
    "    name = allQuestionsModel[\"name\"]\n",
    "    year = allQuestionsModel[\"year\"]\n",
    "    questions = []\n",
    "\n",
    "    for question in allQuestionsModel[\"questions\"]:\n",
    "        question_content = \"\\n\".join(doc_page_content[question[\"question_content_start\"]:question[\"question_content_end\"]+1])\n",
    "        solution_content = \"\\n\".join(doc_page_content[question[\"solution_content_start\"]:question[\"solution_content_end\"]+1])\n",
    "        #important, image will be wrong if two identical images are used, although this should not be possible.\n",
    "        images = list(set(extract_images(question_content) + extract_images(solution_content)))\n",
    "\n",
    "        questions.append(\n",
    "            QuestionModel(\n",
    "                question_content=question_content,\n",
    "                solution_content=solution_content,\n",
    "                images=images\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    allQuestions = AllQuestionsModel(\n",
    "        name=name,\n",
    "        year=year,\n",
    "        questions=questions\n",
    "    )\n",
    "    return allQuestions.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Back slash correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_latex_backslashes(text: str) -> str:\n",
    "    # This regex finds any single backslash `\\` that is not already part of an escaped backslash `\\\\`\n",
    "    # or a newline `\\n`, and replaces it with a double backslash `\\\\`.\n",
    "    return re.sub(r'\\\\\\\\|\\\\(?!n)', r'\\\\\\\\', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Extract question parts and solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the tutorial output.\n",
    "class Set_Question(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the question (only the text, no numbering)\")\n",
    "    content: str = Field(..., description=\"Content of the question (no exercise title, no subquestions)\")\n",
    "    parts: list[str] = Field(..., description=\"List of parts within the question (only the text, no numbering)\")\n",
    "    images: list[str] = Field(..., description=\"List of image URLs associated with the question (no alt text, only URLs)\")\n",
    "\n",
    "class Set_Solution_Part(BaseModel):\n",
    "    part_solution: str = Field(..., description=\"The worked solution for the part (no numbering or counting)\")\n",
    "\n",
    "class Set_Solution(BaseModel):\n",
    "    parts_solutions: list[str] = Field(..., description=\"List of worked solutions for the question (no numbering or counting)\")\n",
    "\n",
    "    def __init__(self, parts_solutions: list[Set_Solution_Part]):\n",
    "        \"\"\"\n",
    "        Initialize the Set_Solution with a list of solutions for each part.\n",
    "        \n",
    "        Args:\n",
    "            parts_solutions (list[Set_Solution_Part]): The worked solutions for the parts.\n",
    "        \"\"\"\n",
    "        super().__init__(parts_solutions=[part.part_solution for part in parts_solutions])\n",
    "\n",
    "class Set_Question_With_Solution(Set_Question):\n",
    "    parts_solutions: list[str] = Field(..., description=\"The worked solution for the parts.\")\n",
    "\n",
    "    def __init__(self, question: Set_Question, solution: Set_Solution):\n",
    "        \"\"\"\n",
    "        Initialize the Set_Question_With_Solution with a question and its solution.\n",
    "        \n",
    "        Args:\n",
    "            question (Set_Question): The question object.\n",
    "            solution (Set_Solution): The solution object.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            **question.model_dump(),\n",
    "            parts_solutions=solution.parts_solutions\n",
    "        )\n",
    "\n",
    "\n",
    "class Set(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[Set_Question_With_Solution] = Field(..., description=\"List of questions in the set\")\n",
    "\n",
    "llm_task_seperate_parts_question = r\"\"\"\n",
    "    Your task is to separate a question's content into a main stem and distinct parts, then format it as a JSON object.\n",
    "    Follow these rules precisely:\n",
    "\n",
    "    1.  **Content Splitting:**\n",
    "        -   From the input `question_content`, identify the main introductory text (the stem) and place it in the `content` field.\n",
    "        -   Identify all sub-questions (e.g., \"(a)\", \"(b)\", \"i.\", \"ii.\") and place their text into the `parts` list. Sub-questions may also be implied.\n",
    "        -   Questions with no sub-questions should have a single part in the `parts` list, which is the entire question text.\n",
    "        -   Ensure that images references are correctly placed with their respective parts.\n",
    "        -   Preserve all content perfectly, including text, LaTeX, and image tags like `![pictureTag](filename.jpg)`. Do not duplicate images.\n",
    "        -   Ensure no solution content is included in the `content` or `parts` fields.\n",
    "        -  You may choose what the title of the question should be.\n",
    "        -   The `images` list should be copied exactly from the input.\n",
    "\n",
    "    2.  **Output Format (Crucial):**\n",
    "        -   You MUST output ONLY a single, raw, valid JSON string.\n",
    "        -   Do NOT include any explanations, comments, or markdown code blocks (like ```json).\n",
    "\n",
    "    3.  **JSON Formatting Rules:**\n",
    "        -   **Backslash Escaping (Very Important):**\n",
    "            -   For **LaTeX commands**, a single backslash `\\` MUST be escaped as a double backslash `\\\\`. For example, `\\frac` must become `\\\\frac`.\n",
    "            -   For **newlines**, you MUST use a single `\\n`. Do NOT escape it as `\\\\n`.\n",
    "        -   **Content Integrity:** Preserve all text, LaTeX (`$...$`, `$$...$$`), and image tags (`![pictureTag](...)`) perfectly. Do not alter or summarize content.\n",
    "    \"\"\"\n",
    "\n",
    "llm_task_seperate_parts_solution = r\"\"\"\n",
    "    Your task is to extract the solution for a specific question part from the full solution provided.\n",
    "    Please follow these rules carefully:\n",
    "\n",
    "    1.  **Content Extraction:**\n",
    "        -   From the `full solution`, find the worked solution that corresponds to the given `target question part`.\n",
    "        -   Use the full question content and full question parts to help identify the correct parts of the solution to be extracted.\n",
    "        -   Place this exact text into the `part_solution` field.\n",
    "        -   Ensure that images references are correctly placed with their respective parts. Do not duplicate images.\n",
    "        -   Preserve all content perfectly, including text, LaTeX, and image tags like `![pictureTag](filename.jpg)`.\n",
    "        -   If no specific solution is found, use an empty string `\"\"`.\n",
    "\n",
    "    2.  **Output Format (Crucial):**\n",
    "        -   You MUST output ONLY a single, raw, valid JSON string.\n",
    "        -   Do NOT include any explanations, comments, or markdown code blocks (like ```json).\n",
    "\n",
    "    3.  **JSON Formatting Rules:**\n",
    "        -   **Backslash Escaping (Very Important):**\n",
    "            -   For **LaTeX commands**, a single backslash `\\` MUST be escaped as a double backslash `\\\\`. For example, `\\frac` must become `\\\\frac` in the JSON string. This is the most important rule.\n",
    "        -   **Newlines:** Use `\\n` for newlines within the JSON string values.\n",
    "        -   **Math Delimiters:** Ensure all math delimiters (`$...$` and `$$...$$`) are correctly balanced and preserved.\n",
    "    \"\"\"\n",
    "\n",
    "def process_single_question(question_data):\n",
    "    \"\"\"Process a single question and its parts in parallel\"\"\"\n",
    "    question_idx, question = question_data\n",
    "    \n",
    "    # Initialize the output parser with the Set_Question schema.\n",
    "    question_parser = PydanticOutputParser(pydantic_object=Set_Question)\n",
    "    \n",
    "    # Process the question part\n",
    "    for attempt_idx in range(3):\n",
    "        # Prompt for the LLM to extract The question parts.\n",
    "        # Use the full question content and the images to extract the parts.\n",
    "        seperate_parts_question_prompt = f\"\"\"\n",
    "            Your task is to extract a JSON with the following structure exactly, ready to be parsed by a pydantic model:\n",
    "            {question_parser.get_format_instructions()}\n",
    "\n",
    "            {llm_task_seperate_parts_question}\n",
    "\n",
    "            Input Dictionary:\n",
    "            ```JSON\n",
    "            {json.dumps(question)}\n",
    "            ```\n",
    "\n",
    "            Return the JSON now.\n",
    "            \"\"\"\n",
    "\n",
    "        response = llm_mini.invoke(seperate_parts_question_prompt)\n",
    "\n",
    "        try:\n",
    "            parsed_output_parts = question_parser.parse(response.content)\n",
    "            print(f\"LLM response successfully parsed question {question_idx + 1}.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLM response as JSON for question {question_idx + 1}:\")\n",
    "            print(f\"Retrying... Attempt No.{attempt_idx + 1}\")\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        print(\"Final LLM Response:\")\n",
    "        print(response.content)\n",
    "        raise Exception(f\"Failed to parse LLM response as JSON after multiple attempts for question {question_idx + 1}.\")\n",
    "\n",
    "    # Process solution parts in parallel\n",
    "    def process_solution_part(part_data):\n",
    "        part_idx, part = part_data\n",
    "        solution_parser = PydanticOutputParser(pydantic_object=Set_Solution_Part)\n",
    "        \n",
    "        for attempt_idx in range(3):\n",
    "            # Prompt for the LLM to extract The solution part.\n",
    "            # Use the full solution content and the part to extract the specific solution.\n",
    "            seperate_parts_solution_prompt = f\"\"\"\n",
    "                Your task is to extract a JSON with the following structure exactly, ready to be parsed by a pydantic model:\n",
    "                {solution_parser.get_format_instructions()}\n",
    "\n",
    "                {llm_task_seperate_parts_solution}\n",
    "\n",
    "                full solution:\n",
    "                {question[\"solution_content\"]}\n",
    "\n",
    "                full question content:\n",
    "                {parsed_output_parts.content}\n",
    "\n",
    "                full question parts:\n",
    "                {parsed_output_parts.parts}\n",
    "\n",
    "                target question part:\n",
    "                {part}\n",
    "                \"\"\"\n",
    "            \n",
    "            response = llm_mini.invoke(seperate_parts_solution_prompt)\n",
    "            \n",
    "            try:\n",
    "                parsed_output_solution_part = solution_parser.parse(response.content)\n",
    "                print(f\"LLM response successfully parsed solution for part {part_idx + 1} of question {question_idx + 1}.\")\n",
    "                return parsed_output_solution_part\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing LLM response as JSON for part {part_idx + 1} of question {question_idx + 1}:\")\n",
    "                print(f\"Retrying... Attempt No.{attempt_idx + 1}\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        else:\n",
    "            print(\"Final LLM Response:\")\n",
    "            print(response.content)\n",
    "            raise Exception(f\"Failed to parse LLM response as JSON after multiple attempts part {part_idx + 1} of question {question_idx + 1}:\")\n",
    "\n",
    "    # Process all parts in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        part_data_list = [(i, part) for i, part in enumerate(parsed_output_parts.parts)]\n",
    "        solutions_parts = list(executor.map(process_solution_part, part_data_list))\n",
    "\n",
    "    set_solution = Set_Solution(parts_solutions=solutions_parts)\n",
    "    return Set_Question_With_Solution(\n",
    "        question=parsed_output_parts,\n",
    "        solution=set_solution\n",
    "    )\n",
    "\n",
    "def extract_parts_question(questions_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "    Now processes questions in parallel while maintaining order.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process all questions in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        question_data_list = [(i, q) for i, q in enumerate(questions_dict[\"questions\"])]\n",
    "        questions_in_parts = list(executor.map(process_single_question, question_data_list))\n",
    "\n",
    "    return Set(\n",
    "        name=questions_dict[\"name\"],\n",
    "        year=questions_dict[\"year\"],\n",
    "        questions=questions_in_parts\n",
    "    ).model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# LLM evaluation of the content of JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models for validation\n",
    "class PartTextModel(BaseModel):\n",
    "    part_text: str = Field(..., description=\"The text of the part\")\n",
    "\n",
    "class PartSolutionModel(BaseModel):\n",
    "    part_solution: str = Field(..., description=\"The solution for the part\")\n",
    "\n",
    "class QuestionContentModel(BaseModel):\n",
    "    title: str = Field(..., description=\"The title of the question\")\n",
    "    content: str = Field(..., description=\"The main content of the question\")\n",
    "\n",
    "llm_task_text_check = r\"\"\"\n",
    "    Your task is to validate and correct the content of the provided JSON fields to ensure it is clean, well-formatted, and valid Texdown (Markdown with LaTeX).\n",
    "    You MUST return ONLY a single, raw, valid JSON string that strictly follows the original schema. Do NOT add any explanations, comments, or markdown code blocks.\n",
    "\n",
    "    Apply these correction rules to the content inside the JSON fields:\n",
    "    1.  **JSON Escaping:** All LaTeX backslashes (`\\`) MUST be escaped as double backslashes (`\\\\`). For example, `\\frac` must become `\\\\frac`. Do not escape backslashes for newlines (`\\n`).\n",
    "    2.  **Enforce Math Delimiters:** This is the most important rule. Any text containing LaTeX commands (e.g., `\\lim`, `\\frac`, `\\sin`, `\\alpha`, `^`, `_`) or mathematical structures that is NOT already enclosed in `$..$` or `$$..$$` MUST be wrapped.\n",
    "        -   Use `$$...$$` for standalone equations and `$...$` for inline math.\n",
    "        -   Be careful to not wrap text that is already correctly formatted with LaTeX math delimiters.\n",
    "    3.  **Display Math Formatting:** This rule is critical. Display math blocks MUST be formatted strictly as follows: a blank line, the opening `$$` on its own line, the LaTeX content, the closing `$$` on its own line, and a blank line.\n",
    "        -   **Incorrect:** `...text $$x=y$$ more text...`\n",
    "        -   **Incorrect:** `...text$$x=y$$\\nmore text...`\n",
    "        -   **Incorrect:** `...text\\n$$x=y$$more text...`\n",
    "        -   **Incorrect:** `...text\\n$$\\nx=y\\n\\n$$\\nmore text...`\n",
    "        -   **Correct:** `...text\\n\\n$$\\nx=y\\n$$\\n\\nmore text...`\n",
    "    4.  **LaTeX Environments:** Environments like `aligned`, `cases`, `matrix`, `gathered`, etc., must be entirely contained within a single display math block (`$$...$$`). Ensure that every `\\begin{...}` has a matching `\\end{...}`.\n",
    "    7.  **Markdown Lists:** Ensure that markdown lists (e.g., using `*`, `-`, or `1.`) are correctly formatted with proper indentation and spacing.\n",
    "    8.  **Spacing and Readability:** Ensure there is a single blank line between paragraphs, lists, and other block elements to improve readability. Remove any excessive blank lines.\n",
    "    9.  **Cleanup Redundancy:** Correct or remove any repetitive or nonsensical phrases that may be artifacts from OCR (e.g., \"is monotone. is monotone\" should be corrected to \"is monotone\").\n",
    "    10. **Content Integrity:** Do not change, paraphrase, or summarize any text, formulas, or image links. Only fix formatting, spacing, and structural errors according to these rules.\n",
    "    \"\"\"\n",
    "\n",
    "def validate_part_text(part_text_data):\n",
    "    \"\"\"Validate a single part text with retry logic\"\"\"\n",
    "    question_idx, part_idx, part_text = part_text_data\n",
    "    part_text_parser = PydanticOutputParser(pydantic_object=PartTextModel)\n",
    "    \n",
    "    for attempt_idx in range(3):\n",
    "        part_text_validation_data = {\n",
    "            \"part_text\": part_text\n",
    "        }\n",
    "        \n",
    "        validation_prompt = f\"\"\"\n",
    "            Your task is to extract a JSON with the following structure exactly, to be parsed by a pydantic model:\n",
    "            {part_text_parser.get_format_instructions()}\n",
    "\n",
    "            Your task is to validate and correct the content within the `part_text` field of the provided JSON input.\n",
    "            {llm_task_text_check}\n",
    "\n",
    "            Input Part Text:\n",
    "            ```json\n",
    "            {json.dumps(part_text_validation_data, indent=2)}\n",
    "            ```\n",
    "            return the JSON with the content fixed if needed.\n",
    "            \"\"\"\n",
    "\n",
    "        response = llm_mini.invoke(validation_prompt)\n",
    "\n",
    "        try:\n",
    "            parsed_output = part_text_parser.parse(response.content)\n",
    "            print(f\"LLM response successfully parsed part text validation for question {question_idx + 1}, part {part_idx + 1}\")\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(f\"Part text validation error for question {question_idx + 1}, part {part_idx + 1}: {ve}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing part text validation LLM response for question {question_idx + 1}, part {part_idx + 1}: {e}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "    \n",
    "    raise Exception(f\"Failed to parse part text validation LLM response after multiple attempts for question {question_idx + 1}, part {part_idx + 1}.\")\n",
    "\n",
    "def validate_part_solution(part_solution_data):\n",
    "    \"\"\"Validate a single part solution with retry logic\"\"\"\n",
    "    question_idx, part_idx, part_solution = part_solution_data\n",
    "    part_solution_parser = PydanticOutputParser(pydantic_object=PartSolutionModel)\n",
    "    \n",
    "    for attempt_idx in range(3):\n",
    "        part_solution_validation_data = {\n",
    "            \"part_solution\": part_solution\n",
    "        }\n",
    "\n",
    "        validation_prompt = f\"\"\"\n",
    "            Your task is to extract a JSON with the following structure exactly, to be parsed by a pydantic model:\n",
    "            {part_solution_parser.get_format_instructions()}\n",
    "\n",
    "            Your task is to validate and correct the content within the `part_solution` field of the provided JSON input.\n",
    "            {llm_task_text_check}\n",
    "\n",
    "            Input Part Solution:\n",
    "            ```json\n",
    "            {json.dumps(part_solution_validation_data, indent=2)}\n",
    "            ```\n",
    "            return the JSON with the content fixed if needed.\n",
    "            \"\"\"\n",
    "\n",
    "        response = llm_mini.invoke(validation_prompt)\n",
    "\n",
    "        try:\n",
    "            parsed_output = part_solution_parser.parse(response.content)\n",
    "            print(f\"LLM response successfully parsed part solution validation for question {question_idx + 1}, part {part_idx + 1}\")\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(f\"Part solution validation error for question {question_idx + 1}, part {part_idx + 1}: {ve}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing part solution validation LLM response for question {question_idx + 1}, part {part_idx + 1}: {e}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "    \n",
    "    raise Exception(f\"Failed to parse part solution validation LLM response after multiple attempts for question {question_idx + 1}, part {part_idx + 1}.\")\n",
    "\n",
    "def validate_question_content(question_data):\n",
    "    \"\"\"Validate question title and content with retry logic\"\"\"\n",
    "    question_idx, title, content = question_data\n",
    "    content_parser = PydanticOutputParser(pydantic_object=QuestionContentModel)\n",
    "    \n",
    "    for attempt_idx in range(3):\n",
    "        content_validation_data = {\n",
    "            \"title\": title,\n",
    "            \"content\": content\n",
    "        }\n",
    "        \n",
    "        validation_prompt = f\"\"\"\n",
    "            Your task is to extract a JSON with the following structure exactly, to be parsed by a pydantic model:\n",
    "            {content_parser.get_format_instructions()}\n",
    "\n",
    "            Your task is to validate and correct the content within the `title` and `content` fields of the provided JSON input.\n",
    "            {llm_task_text_check}\n",
    "\n",
    "            Input Question Content:\n",
    "            ```json\n",
    "            {json.dumps(content_validation_data, indent=2)}\n",
    "            ```\n",
    "            return the JSON with the content fixed if needed.\n",
    "            \"\"\"\n",
    "\n",
    "        response = llm_mini.invoke(validation_prompt)\n",
    "\n",
    "        try:\n",
    "            parsed_output = content_parser.parse(response.content)\n",
    "            print(f\"LLM response successfully parsed content validation for question {question_idx + 1}\")\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(f\"Content validation error for question {question_idx + 1}: {ve}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing content validation LLM response for question {question_idx + 1}: {e}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "    \n",
    "    raise Exception(f\"Failed to parse content validation LLM response after multiple attempts for question {question_idx + 1}.\")\n",
    "\n",
    "def process_single_question_validation(question_data):\n",
    "    \"\"\"Process validation for a single question's content, parts, and solutions in parallel\"\"\"\n",
    "    question_idx, question = question_data\n",
    "    \n",
    "    # Validate question content (title and content) separately\n",
    "    content_data = (question_idx, question.get(\"title\", \"\"), question.get(\"content\", \"\"))\n",
    "    validated_content = validate_question_content(content_data)\n",
    "    \n",
    "    # Prepare part text data for parallel processing\n",
    "    part_text_data_list = [\n",
    "        (question_idx, part_idx, part_text)\n",
    "        for part_idx, part_text in enumerate(question.get(\"parts\", []))\n",
    "    ]\n",
    "    \n",
    "    # Prepare part solution data for parallel processing\n",
    "    part_solution_data_list = [\n",
    "        (question_idx, part_idx, part_solution)\n",
    "        for part_idx, part_solution in enumerate(question.get(\"parts_solutions\", []))\n",
    "    ]\n",
    "    \n",
    "    # Process part texts and solutions in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit all validation tasks\n",
    "        part_text_futures = [executor.submit(validate_part_text, data) for data in part_text_data_list]\n",
    "        part_solution_futures = [executor.submit(validate_part_solution, data) for data in part_solution_data_list]\n",
    "        \n",
    "        # Collect results maintaining order\n",
    "        validated_part_texts = [future.result() for future in part_text_futures]\n",
    "        validated_part_solutions = [future.result() for future in part_solution_futures]\n",
    "\n",
    "    validated_parts = [p[\"part_text\"] for p in validated_part_texts]\n",
    "    validated_parts_solutions = [p[\"part_solution\"] for p in validated_part_solutions]\n",
    "    \n",
    "    return {\n",
    "        \"title\": validated_content[\"title\"],\n",
    "        \"content\": validated_content[\"content\"],\n",
    "        \"parts\": validated_parts,\n",
    "        \"parts_solutions\": validated_parts_solutions,\n",
    "        \"images\": question.get(\"images\", [])\n",
    "    }\n",
    "\n",
    "def content_texdown_check(validated_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Checks if the content of the JSON is in Texdown format by processing each question's content, parts, and solutions separately.\n",
    "    Now processes questions in parallel while maintaining order.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process all questions in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        question_data_list = [(i, q) for i, q in enumerate(validated_dict[\"questions\"])]\n",
    "        questions_in_parts = list(executor.map(process_single_question_validation, question_data_list))\n",
    "    \n",
    "    return {\n",
    "        \"name\": validated_dict[\"name\"],\n",
    "        \"year\": validated_dict[\"year\"],\n",
    "        \"questions\": questions_in_parts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_to_json(md_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "    \n",
    "    Args:\n",
    "        md_content (str): The content of a set.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    md_content_lines = md_content.splitlines()\n",
    "\n",
    "    # corrected_md_content = correct_mistakes_in_markdown(md_content)\n",
    "    # print(\"Markdown content corrected for spelling, grammar, and structure.\")\n",
    "\n",
    "    questions_dict_lines = llm_extract_questions_lines(md_content_lines)\n",
    "    print(\"Successfully extracted the lines for questions and solutions from the markdown lines. Now extracting the questions...\")\n",
    "\n",
    "    questions_dict = extract_questions(questions_dict_lines, md_content_lines)\n",
    "    print((json.dumps(questions_dict)))\n",
    "    print(\"successfully extracted the questions from the markdown. Now extracting the parts...\")\n",
    "\n",
    "    extracted_dict = extract_parts_question(questions_dict)\n",
    "    print(\"succesfully extracted the parts from the questions.\")\n",
    "    print(json.dumps(extracted_dict, indent=2))\n",
    "    print(\"Now validating the content...\")\n",
    "\n",
    "    return extracted_dict\n",
    "\n",
    "    content_validated_dict = content_texdown_check(extracted_dict)\n",
    "    print(\"successfully validated the content.\")\n",
    "    print(json.dumps(content_validated_dict, indent=2))\n",
    "    print(\"successfully converted markdown to JSON.\")\n",
    "\n",
    "    return content_validated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_json_question_set = md_to_json(md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Displaying questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title\n",
    "title = full_json_question_set[\"name\"] + \" \" + full_json_question_set[\"year\"]\n",
    "\n",
    "# Print the title\n",
    "print(f\"Title: {title}\\n\")\n",
    "\n",
    "# Extract questions\n",
    "questions = full_json_question_set[\"questions\"]\n",
    "\n",
    "# Loop over and print each question\n",
    "for question_idx, question in enumerate(questions, start=1):\n",
    "    print(f\"**Question {question_idx}**:\\n{question.get('title')}\\n\")\n",
    "    print(f\"Content: {question.get('content')}\\n\")\n",
    "    for part_idx, (part_question, part_answer) in enumerate(zip(question.get(\"parts\", []), question.get(\"parts_solutions\", [])), start=1):\n",
    "        print(f\"Question {question_idx}:\")\n",
    "        print(f\"- Subquestion {part_idx}: {part_question}\")\n",
    "        print(f\"- Worked Solution {part_idx}: {part_answer}\")\n",
    "        print(\"\\n\")\n",
    "    print(\"-\" * 40)  # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "# in2lambda to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = full_json_question_set[\"questions\"]\n",
    "\n",
    "in2lambda_questions = []\n",
    "\n",
    "# Loop over all questions and question_answers and use in2lambda API to create a JSON.\n",
    "for question_idx, question_dict in enumerate(questions, start=1):\n",
    "    parts = []\n",
    "    for part_question, part_solution in zip(question_dict.get(\"parts\", []), question_dict.get(\"parts_solutions\", [])):\n",
    "        part_obj = Part(\n",
    "            text=part_question,\n",
    "            worked_solution=part_solution\n",
    "        )\n",
    "        parts.append(part_obj)\n",
    "\n",
    "    # Handle image paths - ensure they exist\n",
    "    image_paths = []\n",
    "    for img in question_dict.get(\"images\", []):\n",
    "        if img.startswith(\"http\"):\n",
    "            # Skip URLs that weren't processed\n",
    "            continue\n",
    "        full_path = f\"{media_path}/{img}\"\n",
    "        if Path(full_path).exists():\n",
    "            image_paths.append(full_path)\n",
    "        else:\n",
    "            print(f\"Warning: Image file not found: {full_path}\")\n",
    "\n",
    "    question = Question(\n",
    "        title=question_dict.get(\"title\", f\"Question {question_idx}\"),\n",
    "        main_text=question_dict.get(\"content\", \"\"),\n",
    "        parts=parts,\n",
    "        images=image_paths\n",
    "    )\n",
    "    in2lambda_questions.append(question)\n",
    "\n",
    "try:\n",
    "    Module(in2lambda_questions).to_json(f\"{output_path}/out\")\n",
    "    print(\"JSON output successfully created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating JSON output: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
