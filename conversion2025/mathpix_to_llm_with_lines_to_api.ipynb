{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# process description  \n",
    "\n",
    "the program takes in a pdf  \n",
    "mathpix is used to scan the pdf and turning it into markdown  \n",
    "markdown then processed to get the images  \n",
    "\n",
    "llm is used to extract the questions and solutions in **ONE** go.  \n",
    "the final JSON is made using the in2lambda api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "import pypandoc\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from in2lambda.api.module import Module\n",
    "from in2lambda.api.question import Question\n",
    "from in2lambda.api.part import Part\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load environment variables from .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# scanning/processing the initial pdf into markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATHPIX_API_KEY = os.getenv(\"MATHPIX_API_KEY\")\n",
    "MATHPIX_APP_ID = os.getenv(\"MATHPIX_APP_ID\")\n",
    "\n",
    "def pdf_to_markdown(source_path: str, result_path: str):\n",
    "    ''' \n",
    "    converts the pdf at `source_path` to a markdown file at `result_path` using Mathpix API.\n",
    "    '''\n",
    "    # Upload PDF to Mathpix and returns a Markdown file with the content.\n",
    "    with open(source_path, \"rb\") as file:\n",
    "        r = requests.post(\n",
    "            \"https://api.mathpix.com/v3/pdf\",   \n",
    "            headers={\n",
    "                \"app_id\": MATHPIX_APP_ID,\n",
    "                \"app_key\": MATHPIX_API_KEY,\n",
    "            },\n",
    "            files={\"file\": file},\n",
    "        )\n",
    "        pdf_id = r.json()[\"pdf_id\"]\n",
    "        print(\"PDF ID:\", pdf_id)\n",
    "        print(\"Response:\", r.json())\n",
    "\n",
    "        # url of where the location of the processed PDF will be\n",
    "        url = f\"https://api.mathpix.com/v3/pdf/{pdf_id}.md\"\n",
    "        headers = {\n",
    "            \"app_id\": MATHPIX_APP_ID,\n",
    "            \"app_key\": MATHPIX_API_KEY,\n",
    "        }\n",
    "\n",
    "        max_retries = 10\n",
    "        retry_delay = 5  # seconds\n",
    "        for attempt in range(max_retries):\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                # Save the result if the request is successful\n",
    "                with open(result_path, \"w\") as f:\n",
    "                    f.write(response.text)\n",
    "                print(\"Downloaded MD successfully.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}/{max_retries}: Processing not complete. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "        else:\n",
    "            print(\"Failed to retrieve processed PDF after multiple attempts:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# setting up the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the output folder and media folder.\n",
    "folder_path = \"conversion_content\"\n",
    "input_path = f\"{folder_path}/input\"\n",
    "output_path = f\"{folder_path}/mathpix_to_llm_with_lines_to_api\"\n",
    "media_path = f\"{output_path}/media\"\n",
    "\n",
    "# Create output and media directories if they do not exist.\n",
    "Path(media_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# location of the source pdf file and the result markdown file.\n",
    "files = [f for f in os.listdir(input_path) if f != '.gitkeep']\n",
    "source_path = f\"{input_path}/{files[0]}\" # the first file in the input folder\n",
    "result_path = f\"{output_path}/example.md\"\n",
    "\n",
    "\n",
    "\n",
    "# Only activate mathpix if the markdown has not been created yet.\n",
    "# This avoids unnecessary reprocessing of the same PDF.\n",
    "if not Path(result_path).exists():\n",
    "    if Path(source_path).exists():\n",
    "        extension = Path(source_path).suffix.lower() # obtains the file extension\n",
    "        if extension == \".pdf\":\n",
    "            pdf_to_markdown(source_path, result_path)\n",
    "        else:\n",
    "            pypandoc.convert_file(source_path, 'md', outputfile=result_path)\n",
    "    else:\n",
    "        print(f\"Error: Source PDF file not found at {source_path}\")\n",
    "        exit(1)\n",
    "\n",
    "# Read the markdown content from the result file.\n",
    "try:\n",
    "    with open(result_path, \"r\") as f:\n",
    "        md_content = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Markdown file not found at {result_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Print out a summary.\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {len(md_content)} characters\")\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {md_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# downlaoding extracted images from Mathpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the figures from the paper and answers.\n",
    "def extract_figures_from_text(text): #, ans=False):\n",
    "    \"\"\"\n",
    "    Extracts figures from the text using regex.\n",
    "    Finds figure references and their descriptions.\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    # Regex to match figure references and their descriptions\n",
    "    # Matches ![alt text](url) format for images\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    print(f\"Matches found: {matches}\")\n",
    "    \n",
    "    for match in matches:\n",
    "        url = match\n",
    "        url = url.strip()\n",
    "        \n",
    "        if url.startswith(\"http\"):\n",
    "            # Download the image and save it to a file\n",
    "            image = Image.open(requests.get(url, stream=True).raw)\n",
    "            # Create a figure name based on the URL\n",
    "            fig_name = os.path.basename(url)\n",
    "            figures[fig_name] = {\n",
    "                \"image\": image,\n",
    "                \"url\": url,\n",
    "                \"local_path\": \"\",\n",
    "                # \"answerFile\": ans\n",
    "            }\n",
    "    return figures\n",
    "\n",
    "# a dictionary storing information on the figures\n",
    "figures = extract_figures_from_text(md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# saving the images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figures_to_path(figures):\n",
    "    for idx, (fig_name, fig_info) in enumerate(figures.items()):\n",
    "        print(f\"URL='{fig_info['url']}'\")\n",
    "\n",
    "        # Extract file extension and create a clean filename\n",
    "        # Mathpix leaves image urls like `image.png?width=800&height=600`\n",
    "        # We only want the base name without query parameters.\n",
    "        if \"?\" in fig_name:\n",
    "            end_location = fig_name.index(\"?\")\n",
    "            image_name = f\"{idx}_{fig_name[:end_location]}\"\n",
    "        else:\n",
    "            image_name = f\"{idx}_{fig_name}\"\n",
    "        \n",
    "        fig_info[\"local_path\"] = image_name\n",
    "        try:\n",
    "            # Saves the image to the media path\n",
    "            fig_info[\"image\"].save(f\"{media_path}/{fig_info['local_path']}\")\n",
    "            print(f\"Saved image: {fig_info['local_path']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving image {image_name}: {e}\")\n",
    "\n",
    "save_figures_to_path(figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# replacing url for images with local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_figures_in_markdown(md_content, figures) -> str:\n",
    "    #replace the image URLs in the markdown content with local paths\n",
    "    # add pictureTag for Lambda Feedback to recognise it as a picture\n",
    "    md_content = md_content.replace(\"![]\", \"![pictureTag]\")\n",
    "    for fig_name, fig_info in figures.items():\n",
    "        md_content = md_content.replace(fig_info[\"url\"], fig_info[\"local_path\"])\n",
    "        print(f\"Replaced {fig_info['url']} with {fig_info['local_path']} in markdown content.\")\n",
    "    # Save the modified markdown content to a file\n",
    "    try:\n",
    "        with open(f\"{output_path}/example.md\", \"w\") as f:\n",
    "            f.write(md_content)\n",
    "        print(\"Modified markdown saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving modified markdown: {e}\")\n",
    "    \n",
    "    return md_content\n",
    "\n",
    "md_content = replace_figures_in_markdown(md_content, figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Initialising llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM via LangChain.\n",
    "\n",
    "# Uses gpt-4.1-nano:\n",
    "#    - a faster model\n",
    "#    - less intelligent\n",
    "\n",
    "llm_nano = ChatOpenAI(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )\n",
    "\n",
    "# Uses gpt-4.1-mini:\n",
    "#    - more intelligent\n",
    "llm_mini = ChatOpenAI(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Spelling and structure check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_task_correct_mistakes = \"\"\"\n",
    "The input is a markdown file that is converted from a pdf using Mathpix API.\n",
    "The pdf contains questions and may contain the solutions too.\n",
    "As the original pdf may contain hand written text, the markdown file may contain mistakes in spelling, grammar and structure.\n",
    "\n",
    "Important things to remember:\n",
    "    1. Leave all Math commands and LaTeX formatting the same. As they are completely valid. Do not change the LaTeX formatting and expressions.\n",
    "    2. Only ever use LaTeX math delimiters for math expressions. I.e. use `$...$` for inline math, and `$$...$$` for display math.\n",
    "    3. Leave references to images and figures the same. I.e. do not change the image links or alt text.\n",
    "\n",
    "Your task is to:\n",
    "    1. Correct any spelling mistakes in the markdown file.\n",
    "    2. Correct any grammar mistakes in the markdown file.\n",
    "    3. Correct any layout mistakes in the markdown file, such that it follows the styles of the entire markdown file.\n",
    "    4. Do not change the content of the markdown file, only correct the mistakes.\n",
    "Output only a valid markdown file with the corrections applied, if any. Do not add any additional text or comments.\n",
    "\"\"\"\n",
    "\n",
    "def correct_mistakes_in_markdown(md_content: str) -> str:\n",
    "    correct_mistakes_prompt = f\"\"\"\n",
    "        {llm_task_correct_mistakes}\n",
    "\n",
    "        ```input\n",
    "        {md_content}\n",
    "        ```\n",
    "\n",
    "        Return the markdown now.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_nano.invoke(correct_mistakes_prompt)\n",
    "    print(\"Corrected markdown content:\")\n",
    "    print(response.content.strip())\n",
    "\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Extract Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define initial question model\n",
    "class QuestionModelLines(BaseModel):\n",
    "    # full question and full solution\n",
    "    question_content_start: int = Field(..., description=\"Line number the question starts on.\")\n",
    "    question_content_end: int = Field(..., description=\"Line number the question ends on.\")\n",
    "    solution_content_start: int = Field(..., description=\"Line number the solution starts on.\")\n",
    "    solution_content_end: int = Field(..., description=\"Line number the solution ends on.\")\n",
    "\n",
    "class AllQuestionsModelLines(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[QuestionModelLines] = Field(..., description=\"A list of questions.\")\n",
    "\n",
    "llm_task_seperate_questions = \"\"\"\n",
    "    Your task is to extract the line numbers for the start and end of each question and solution from the markdown file, then format it as a JSON object.\n",
    "    These line numbers will be used later to extract the content of the questions and solutions procedurally.\n",
    "    \n",
    "    1.  **Content Extraction:**\n",
    "        -   Your may choose a suitable name for the set of questions.\n",
    "        -   Identify the `year` of the questions, otherwise use \"0\".\n",
    "        -   Begin by Identifying the questions in the markdown file, and for each question:\n",
    "            -   Identify the start and end line numbers of the full question content, and place them in `question_content_start` and `question_content_end`.\n",
    "            -   Identify the start and end line numbers of the full relevant solution content, and place them in `solution_content_start` and `solution_content_end`.\n",
    "            -   Be careful to ensure that everything related to the question and solution is included, including any math delimiters and LaTeX formatting.\n",
    "            -   Do not forget to include any images or figures that are part of the question or solution.\n",
    "    \n",
    "    2.  **Output Format:**\n",
    "        -   You MUST output ONLY a single, raw, valid JSON string that matches the provided schema.\n",
    "        -   Do NOT include any explanations, comments, or markdown code blocks (like ```json).\n",
    "    \"\"\"\n",
    "\n",
    "def llm_extract_questions_lines(doc_page_content: list[str]) -> dict:\n",
    "    # Initialise the parser for the output.\n",
    "    parser = PydanticOutputParser(pydantic_object=AllQuestionsModelLines)\n",
    "\n",
    "    # Prompt for the LLM to extract questions.\n",
    "    seperate_questions_prompt = f\"\"\"\n",
    "        Your task is to extract a JSON with the following structure exactly, ready to be parsed by a pydantic model:\n",
    "        {parser.get_format_instructions()}\n",
    "\n",
    "        {llm_task_seperate_questions}\n",
    "\n",
    "        Input markdown:\n",
    "        ```\n",
    "        {list(enumerate(doc_page_content))}\n",
    "        ```\n",
    "        Return the JSON now.\n",
    "    \"\"\"\n",
    "\n",
    "    for attempt_idx in range(3):\n",
    "        try:\n",
    "            response = llm_mini.invoke(seperate_questions_prompt)\n",
    "            return parser.parse(response.content).model_dump()\n",
    "        except ValidationError as e:\n",
    "            print(f\"Validation error on attempt {attempt_idx + 1}: {e}\")\n",
    "            if attempt_idx == 2:\n",
    "                raise e\n",
    "            else:\n",
    "                print(\"Retrying...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_images(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts image URLs from the markdown text.\n",
    "    Returns a list of image URLs.\n",
    "    \"\"\"\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionModel(BaseModel):\n",
    "    # full question and full solution\n",
    "    question_content: str = Field(..., description=\"The content of the question.\")\n",
    "    solution_content: str = Field(..., description=\"The content of the solution.\")\n",
    "    images: list[str] = Field(..., description=\"A list of image URLs associated with the question.\")\n",
    "\n",
    "class AllQuestionsModel(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[QuestionModel] = Field(..., description=\"A list of questions.\")\n",
    "\n",
    "\n",
    "def extract_questions(allQuestionsModel: dict, doc_page_content: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts questions from the AllQuestions model and returns a list of Question objects.\n",
    "    \"\"\"\n",
    "    name = allQuestionsModel[\"name\"]\n",
    "    year = allQuestionsModel[\"year\"]\n",
    "    questions = []\n",
    "\n",
    "    for question in allQuestionsModel[\"questions\"]:\n",
    "        question_content = \"\\n\".join(doc_page_content[question[\"question_content_start\"]:question[\"question_content_end\"]+1])\n",
    "        solution_content = \"\\n\".join(doc_page_content[question[\"solution_content_start\"]:question[\"solution_content_end\"]+1])\n",
    "        #important, image will be wrong if two identical images are used, although this should not be possible.\n",
    "        images = list(set(extract_images(question_content) + extract_images(solution_content)))\n",
    "\n",
    "        questions.append(\n",
    "            QuestionModel(\n",
    "                question_content=question_content,\n",
    "                solution_content=solution_content,\n",
    "                images=images\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    allQuestions = AllQuestionsModel(\n",
    "        name=name,\n",
    "        year=year,\n",
    "        questions=questions\n",
    "    )\n",
    "    return allQuestions.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Extract question parts and solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the tutorial output.\n",
    "class Set_Question_Part_Lines(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a part of a question with its start and end lines.\n",
    "    \"\"\"\n",
    "    part_start: int = Field(..., description=\"The start line number of the part.\")\n",
    "    part_end: int = Field(..., description=\"The end line number of the part.\")\n",
    "    \n",
    "class Set_Question_Lines(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the question (only the text, no numbering)\")\n",
    "    content_start: int = Field(..., description=\"start of the content of the question (no exercise title, no subquestions)\")\n",
    "    content_end: int = Field(..., description=\"end of the content of the question (no exercise title, no subquestions)\")\n",
    "    parts: list[Set_Question_Part_Lines] = Field(..., description=\"List of parts within the question (only the text, no numbering)\")\n",
    "\n",
    "class Set_Question(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the question (only the text, no numbering)\")\n",
    "    content: str = Field(..., description=\"Content of the question (no exercise title, no subquestions)\")\n",
    "    parts: list[str] = Field(..., description=\"List of parts within the question (only the text, no numbering)\")\n",
    "    images: list[str] = Field(..., description=\"List of image URLs associated with the question (no alt text, only URLs)\")\n",
    "\n",
    "\n",
    "class Set_Solution_Part_Lines(BaseModel):\n",
    "    part_solution_start: int = Field(..., description=\"The start of the worked solution for the part (no numbering or counting)\")\n",
    "    part_solution_end: int = Field(..., description=\"The end of the worked solution for the part (no numbering or counting)\")\n",
    "\n",
    "class Set_Solution(BaseModel):\n",
    "    parts_solutions: list[str] = Field(..., description=\"List of worked solutions for the question (no numbering or counting)\")\n",
    "\n",
    "\n",
    "class Set_Question_With_Solution(Set_Question):\n",
    "    parts_solutions: list[str] = Field(..., description=\"The worked solution for the parts.\")\n",
    "\n",
    "    def __init__(self, question: Set_Question, solution: Set_Solution):\n",
    "        \"\"\"\n",
    "        Initialize the Set_Question_With_Solution with a question and its solution.\n",
    "        \n",
    "        Args:\n",
    "            question (Set_Question): The question object.\n",
    "            solution (Set_Solution): The solution object.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            **question.model_dump(),\n",
    "            parts_solutions=solution.parts_solutions,\n",
    "        )\n",
    "\n",
    "\n",
    "class Set_Lines(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[Set_Question_With_Solution] = Field(..., description=\"List of questions in the set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_set_question_lines_to_set_question(set_question_lines: Set_Question_Lines, question_content: list[str], images: list[str] = []) -> Set_Question:\n",
    "    \"\"\"\n",
    "    Convert Set_Question_Lines to Set_Question.\n",
    "    \"\"\"\n",
    "    return Set_Question(\n",
    "        title=set_question_lines.title,\n",
    "        content=\"\\n\".join(question_content[set_question_lines.content_start:set_question_lines.content_end + 1]),\n",
    "        parts=[\"\\n\".join(question_content[part.part_start:part.part_end + 1]) for part in set_question_lines.parts],\n",
    "        images=images\n",
    "    )\n",
    "\n",
    "def convert_set_solution_lines_to_set_solution(set_solution_lines: list[Set_Solution_Part_Lines], solution_content: list[str]) -> Set_Solution:\n",
    "    \"\"\"\n",
    "    Convert Set_Solution_Part_Lines to Set_Solution.\n",
    "    \"\"\"\n",
    "    return Set_Solution(\n",
    "        parts_solutions=[\n",
    "            \"\\n\".join(solution_content[part.part_solution_start:part.part_solution_end + 1])\n",
    "            for part in set_solution_lines\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_task_seperate_parts_question = r\"\"\"\n",
    "    1. **Content Extraction:**\n",
    "        -   You may choose the `title` for the question.\n",
    "        -   From the input `Full Question Content`, identify the start line and end line for the main introductory text (the stem), place them in `content_start` and `content_end`. \n",
    "        -   From the input `Full Question Content`, identify and separate all the `parts`(sub-questions), they could be explicit (e.g. using, \"(a)\", \"(b)\", \"i.\", \"ii.\"... etc.), but may also be implied. For each identified sub-question:\n",
    "            -   Place the start line going into `part_start` and the end line going into `part_end`.\n",
    "            -   If the question has no sub-questions, leave `part_start` as 0 and `part_end` as -1.\n",
    "            -   You may use the `Full Solution Content` to help with identifying the parts.\n",
    "        -   Be careful to ensure that everything related to the question stem/parts is included, including any math delimiters and LaTeX formatting.\n",
    "        -   Do not forget to include any images or figures that are part of the question stem, parts or solution.\n",
    "        -   Ensure no solution content is included in the `content` or `parts` fields.\n",
    "    \n",
    "    2.  **Output Format:**\n",
    "        -   You MUST output ONLY a single, raw, valid JSON string that matches the provided schema.\n",
    "        -   Do NOT include any explanations, comments, or markdown code blocks (like ```json).\n",
    "    \"\"\"\n",
    "\n",
    "llm_task_seperate_parts_solution = r\"\"\"\n",
    "    1. **Content Extraction:**\n",
    "        -   From the input `full solution content`, identify the specific solution part that corresponds to the `target question part`, and place the start line and end line into `part_solution_start` and `part_solution_end`.\n",
    "        -   If the `target question part` is empty, identify the specific solution part that corresponds to the `full question stem`.\n",
    "        -   Use the `full question stem` and `full question parts` to help identify the specific solution part.\n",
    "        -   Ensure that the `target question part` is used to extract the specific solution part.\n",
    "        -   Be careful to ensure that everything related to the solution part is included, including any math delimiters and LaTeX formatting.\n",
    "        -   Do not forget to include any images or figures that are part of the solution.\n",
    "\n",
    "    2.  **Output Format:**\n",
    "        -   You MUST output ONLY a single, raw, valid JSON string that matches the provided schema.\n",
    "        -   Do NOT include any explanations, comments, or markdown code blocks (like ```json).\n",
    "    \"\"\"\n",
    "\n",
    "def process_single_question(question_data):\n",
    "    \"\"\"Process a single question and its parts in parallel\"\"\"\n",
    "    question_idx, question = question_data\n",
    "    \n",
    "    # Initialize the output parser with the Set_Question schema.\n",
    "    question_parser = PydanticOutputParser(pydantic_object=Set_Question_Lines)\n",
    "\n",
    "    question_input: list[str] = question[\"question_content\"].splitlines()\n",
    "    solution_input: str = question[\"solution_content\"]\n",
    "    all_images = question[\"images\"]\n",
    "\n",
    "    # Prompt for the LLM to extract The question parts.\n",
    "    # Use the full question content and the images to extract the parts.\n",
    "    seperate_parts_question_prompt = f\"\"\"\n",
    "        Your task is to extract a JSON with the following structure exactly, ready to be parsed by a pydantic model:\n",
    "        {question_parser.get_format_instructions()}\n",
    "\n",
    "        {llm_task_seperate_parts_question}\n",
    "\n",
    "        Full Solution Content:\n",
    "        {solution_input}\n",
    "\n",
    "        Full Question Content:\n",
    "        {list(enumerate(question_input))}\n",
    "\n",
    "        Return the JSON now.\n",
    "        \"\"\"\n",
    "    \n",
    "    # Process the question part\n",
    "    for attempt_idx in range(3):\n",
    "\n",
    "        response = llm_mini.invoke(seperate_parts_question_prompt)\n",
    "\n",
    "        try:\n",
    "            parsed_output_parts = question_parser.parse(response.content)\n",
    "            print(f\"LLM response successfully parsed question {question_idx + 1}.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLM response as JSON for question {question_idx + 1}:\")\n",
    "            print(f\"Retrying... Attempt No.{attempt_idx + 1}\")\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        print(\"Final LLM Response:\")\n",
    "        print(response.content)\n",
    "        raise Exception(f\"Failed to parse LLM response as JSON after multiple attempts for question {question_idx + 1}.\")\n",
    "\n",
    "    # Convert from Set_Question_Lines to Set_Question\n",
    "    parsed_output_parts = convert_set_question_lines_to_set_question(parsed_output_parts, question_input, all_images)\n",
    "\n",
    "    # Process solution parts in parallel\n",
    "    def process_solution_part(part_data) -> Set_Solution_Part_Lines:\n",
    "        part_idx, part = part_data\n",
    "        solution_parser = PydanticOutputParser(pydantic_object=Set_Solution_Part_Lines)\n",
    "\n",
    "        target_solution_input: list[str] = solution_input.splitlines()\n",
    "\n",
    "        # Prompt for the LLM to extract The solution part.\n",
    "        # Use the full solution content and the part to extract the specific solution.\n",
    "        seperate_parts_solution_prompt = f\"\"\"\n",
    "            Your task is to extract a JSON with the following structure exactly, ready to be parsed by a pydantic model:\n",
    "            {solution_parser.get_format_instructions()}\n",
    "\n",
    "            {llm_task_seperate_parts_solution}\n",
    "\n",
    "            full question stem:\n",
    "            {parsed_output_parts.content}\n",
    "\n",
    "            full question parts:\n",
    "            {parsed_output_parts.parts}\n",
    "            \n",
    "            full solution content:\n",
    "            {list(enumerate(target_solution_input))}\n",
    "\n",
    "            target question part:\n",
    "            {part}\n",
    "            \"\"\"\n",
    "            \n",
    "        for attempt_idx in range(3):\n",
    "            \n",
    "            response = llm_mini.invoke(seperate_parts_solution_prompt)\n",
    "            \n",
    "            try:\n",
    "                parsed_output_solution_part = solution_parser.parse(response.content)\n",
    "                print(f\"LLM response successfully parsed solution for part {part_idx + 1} of question {question_idx + 1}.\")\n",
    "                return parsed_output_solution_part\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing LLM response as JSON for part {part_idx + 1} of question {question_idx + 1}:\")\n",
    "                print(f\"Retrying... Attempt No.{attempt_idx + 1}\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        else:\n",
    "            print(\"Final LLM Response:\")\n",
    "            print(response.content)\n",
    "            raise Exception(f\"Failed to parse LLM response as JSON after multiple attempts part {part_idx + 1} of question {question_idx + 1}:\")\n",
    "\n",
    "    # Process all parts in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        part_data_list = [(i, part) for i, part in enumerate(parsed_output_parts.parts)]\n",
    "        solutions_parts = list(executor.map(process_solution_part, part_data_list))\n",
    "\n",
    "    solutions_parts = convert_set_solution_lines_to_set_solution(\n",
    "        solutions_parts, \n",
    "        solution_input.splitlines()\n",
    "    )\n",
    "\n",
    "    # set_solution = Set_Solution(parts_solutions=solutions_parts)\n",
    "    return Set_Question_With_Solution(\n",
    "        question=parsed_output_parts,\n",
    "        solution=solutions_parts\n",
    "    )\n",
    "\n",
    "def extract_parts_question(questions_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "    Now processes questions in parallel while maintaining order.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process all questions in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        question_data_list = [(i, q) for i, q in enumerate(questions_dict[\"questions\"])]\n",
    "        questions_in_parts = list(executor.map(process_single_question, question_data_list))\n",
    "\n",
    "    return Set_Lines(\n",
    "        name=questions_dict[\"name\"],\n",
    "        year=questions_dict[\"year\"],\n",
    "        questions=questions_in_parts\n",
    "    ).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_to_json(md_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "    \n",
    "    Args:\n",
    "        md_content (str): The content of a set.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "\n",
    "    md_content_lines = md_content.splitlines()\n",
    "\n",
    "    # corrected_md_content = correct_mistakes_in_markdown(md_content)\n",
    "    # print(\"Markdown content corrected for spelling, grammar, and structure.\")\n",
    "\n",
    "    questions_dict_lines = llm_extract_questions_lines(md_content_lines)\n",
    "    print(\"Successfully extracted the lines for questions and solutions from the markdown lines. Now extracting the questions...\")\n",
    "\n",
    "    questions_dict = extract_questions(questions_dict_lines, md_content_lines)\n",
    "    print((json.dumps(questions_dict)))\n",
    "    print(\"successfully extracted the questions from the markdown. Now extracting the parts...\")\n",
    "\n",
    "    extracted_dict = extract_parts_question(questions_dict)\n",
    "    print(\"succesfully extracted the parts from the questions.\")\n",
    "    print(json.dumps(extracted_dict, indent=2))\n",
    "    print(\"Now validating the content...\")\n",
    "\n",
    "    return extracted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_json_question_set = md_to_json(md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Displaying questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title\n",
    "title = full_json_question_set[\"name\"] + \" \" + full_json_question_set[\"year\"]\n",
    "\n",
    "# Print the title\n",
    "print(f\"Title: {title}\\n\")\n",
    "\n",
    "# Extract questions\n",
    "questions = full_json_question_set[\"questions\"]\n",
    "\n",
    "# Loop over and print each question\n",
    "for question_idx, question in enumerate(questions, start=1):\n",
    "    print(f\"**Question {question_idx}**:\\n{question.get('title')}\\n\")\n",
    "    print(f\"Content: {question.get('content')}\\n\")\n",
    "    for part_idx, (part_question, part_answer) in enumerate(zip(question.get(\"parts\", []), question.get(\"parts_solutions\", [])), start=1):\n",
    "        print(f\"Question {question_idx}:\")\n",
    "        print(f\"- Subquestion {part_idx}: {part_question}\")\n",
    "        print(f\"- Worked Solution {part_idx}: {part_answer}\")\n",
    "        print(\"\\n\")\n",
    "    print(\"-\" * 40)  # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# in2lambda to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = full_json_question_set[\"questions\"]\n",
    "\n",
    "in2lambda_questions = []\n",
    "\n",
    "# Loop over all questions and question_answers and use in2lambda API to create a JSON.\n",
    "for question_idx, question_dict in enumerate(questions, start=1):\n",
    "    parts = []\n",
    "\n",
    "    for part_question, part_solution in zip(question_dict.get(\"parts\", []), question_dict.get(\"parts_solutions\", [])):\n",
    "        part_obj = Part(\n",
    "            text=part_question,\n",
    "            worked_solution=part_solution\n",
    "        )\n",
    "        parts.append(part_obj)\n",
    "\n",
    "    # Handle image paths - ensure they exist\n",
    "    image_paths = []\n",
    "    for img in question_dict.get(\"images\", []):\n",
    "        if img.startswith(\"http\"):\n",
    "            # Skip URLs that weren't processed\n",
    "            continue\n",
    "        full_path = f\"{media_path}/{img}\"\n",
    "        if Path(full_path).exists():\n",
    "            image_paths.append(full_path)\n",
    "        else:\n",
    "            print(f\"Warning: Image file not found: {full_path}\")\n",
    "\n",
    "    question = Question(\n",
    "        title=question_dict.get(\"title\", f\"Question {question_idx}\"),\n",
    "        main_text=question_dict.get(\"content\", \"\"),\n",
    "        parts=parts,\n",
    "        images=image_paths\n",
    "    )\n",
    "    in2lambda_questions.append(question)\n",
    "\n",
    "try:\n",
    "    Module(in2lambda_questions).to_json(f\"{output_path}/out\")\n",
    "    print(\"JSON output successfully created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating JSON output: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
