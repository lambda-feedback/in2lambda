{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# process description  \n",
    "\n",
    "the program takes in a pdf  \n",
    "mathpix is used to scan the pdf and turning it into markdown  \n",
    "markdown then processed to get the images  \n",
    "\n",
    "llm is used to extract the questions and solutions in **ONE** go.  \n",
    "the final JSON is made using the in2lambda api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import concurrent.futures\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from in2lambda.api.module import Module\n",
    "from in2lambda.api.question import Question\n",
    "from in2lambda.api.part import Part\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load environment variables from .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# scanning/processing the initial pdf into markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATHPIX_API_KEY = os.getenv(\"MATHPIX_API_KEY\")\n",
    "MATHPIX_APP_ID = os.getenv(\"MATHPIX_APP_ID\")\n",
    "\n",
    "def pdf_to_markdown(source_path: str, result_path: str):\n",
    "    ''' \n",
    "    converts the pdf at `source_path` to a markdown file at `result_path` using Mathpix API.\n",
    "    '''\n",
    "    # Upload PDF to Mathpix and returns a Markdown file with the content.\n",
    "    with open(source_path, \"rb\") as file:\n",
    "        r = requests.post(\n",
    "            \"https://api.mathpix.com/v3/pdf\",   \n",
    "            headers={\n",
    "                \"app_id\": MATHPIX_APP_ID,\n",
    "                \"app_key\": MATHPIX_API_KEY,\n",
    "            },\n",
    "            files={\"file\": file},\n",
    "        )\n",
    "        pdf_id = r.json()[\"pdf_id\"]\n",
    "        print(\"PDF ID:\", pdf_id)\n",
    "        print(\"Response:\", r.json())\n",
    "\n",
    "        url = f\"https://api.mathpix.com/v3/pdf/{pdf_id}.md\"\n",
    "        headers = {\n",
    "            \"app_id\": MATHPIX_APP_ID,\n",
    "            \"app_key\": MATHPIX_API_KEY,\n",
    "        }\n",
    "\n",
    "        max_retries = 10\n",
    "        retry_delay = 5  # seconds\n",
    "        for attempt in range(max_retries):\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                # Save the result if the request is successful\n",
    "                with open(result_path, \"w\") as f:\n",
    "                    f.write(response.text)\n",
    "                print(\"Downloaded MD successfully.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}/{max_retries}: Processing not complete. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "        else:\n",
    "            print(\"Failed to retrieve processed PDF after multiple attempts:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# setting up the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"conversion_content\"\n",
    "output_path = f\"{folder_path}/mathpix_to_llm_to_in2lambda_to_JSON_out\"\n",
    "media_path = f\"{output_path}/media\"\n",
    "\n",
    "Path(media_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "source_path = f\"{folder_path}/example.pdf\"\n",
    "result_path = f\"{output_path}/example.md\"\n",
    "\n",
    "# Only activate mathpix if the markdown has not been created yet.\n",
    "# This avoids unnecessary reprocessing of the same PDF.\n",
    "if not Path(result_path).exists():\n",
    "    if Path(source_path).exists():\n",
    "        pdf_to_markdown(source_path, result_path)\n",
    "    else:\n",
    "        print(f\"Error: Source PDF file not found at {source_path}\")\n",
    "        exit(1)\n",
    "\n",
    "try:\n",
    "    with open(result_path, \"r\") as f:\n",
    "        md_content = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Markdown file not found at {result_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Print out a summary.\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {len(md_content)} characters\")\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {md_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# downlaoding extracted images from Mathpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the figures from the paper and answers.\n",
    "def extract_figures_from_text(text): #, ans=False):\n",
    "    \"\"\"\n",
    "    Extracts figures from the text using regex.\n",
    "    Finds figure references and their descriptions.\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    # Regex to match figure references and their descriptions\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    print(f\"Matches found: {matches}\")\n",
    "    \n",
    "    for match in matches:\n",
    "        url = match\n",
    "        url = url.strip()\n",
    "        figure_caption_pattern = rf'\\({re.escape(url)}\\)\\s*-?\\s*Figure\\s+(Q\\d+)\\s*-\\s*(.+?)\\n'\n",
    "        caption_match = re.search(figure_caption_pattern, text)\n",
    "\n",
    "        if caption_match:\n",
    "            title, description = caption_match.groups()\n",
    "            print(\"Caption match found\")\n",
    "        else:\n",
    "            title, description = \"\", \"\"\n",
    "\n",
    "        if url.startswith(\"http\"):\n",
    "            # Download the image and save it to a file\n",
    "            image = Image.open(requests.get(url, stream=True).raw)\n",
    "            # Create a figure name based on the URL\n",
    "            fig_name = os.path.basename(url)\n",
    "            figures[fig_name] = {\n",
    "                \"image\": image,\n",
    "                \"title\": title.strip(),\n",
    "                \"label\": description.strip(),\n",
    "                \"url\": url,\n",
    "                \"local_path\": \"\",\n",
    "                # \"answerFile\": ans\n",
    "            }\n",
    "    return figures\n",
    "\n",
    "# a dictionary storing information on the figures\n",
    "figures = extract_figures_from_text(md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# saving the images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figures_to_path(figures):\n",
    "    for idx, (fig_name, fig_info) in enumerate(figures.items()):\n",
    "        print(f\"FIGURE Title='{fig_info['title']}', Label='{fig_info['label']}', URL='{fig_info['url']}'\")\n",
    "        # Extract file extension and create a clean filename\n",
    "        if \"?\" in fig_name:\n",
    "            end_location = fig_name.index(\"?\")\n",
    "            image_name = f\"{idx}_{fig_name[:end_location]}\"\n",
    "        else:\n",
    "            image_name = f\"{idx}_{fig_name}\"\n",
    "        \n",
    "        fig_info[\"local_path\"] = image_name\n",
    "        try:\n",
    "            fig_info[\"image\"].save(f\"{media_path}/{fig_info['local_path']}\")\n",
    "            print(f\"Saved image: {fig_info['local_path']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving image {image_name}: {e}\")\n",
    "\n",
    "save_figures_to_path(figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# replacing url for images with local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_figures_in_markdown(md_content, figures):\n",
    "    #replace the image URLs in the markdown content with local paths\n",
    "    # add pictureTag for Lambda Feedback to recognise it as a picture\n",
    "    md_content = md_content.replace(\"![]\", \"![pictureTag]\")\n",
    "    for fig_name, fig_info in figures.items():\n",
    "        md_content = md_content.replace(fig_info[\"url\"], fig_info[\"local_path\"])\n",
    "        print(f\"Replaced {fig_info['url']} with {fig_info['local_path']} in markdown content.\")\n",
    "    # Save the modified markdown content to a file\n",
    "    try:\n",
    "        with open(f\"{output_path}/example.md\", \"w\") as f:\n",
    "            f.write(md_content)\n",
    "        print(\"Modified markdown saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving modified markdown: {e}\")\n",
    "\n",
    "replace_figures_in_markdown(md_content, figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Initialising llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM via LangChain.\n",
    "\n",
    "# Uses gpt-4.1-nano:\n",
    "#    - a faster model\n",
    "#    - less intelligent\n",
    "\n",
    "llm_nano = ChatOpenAI(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )\n",
    "\n",
    "# Uses gpt-4o-mini:\n",
    "#    - more intelligent\n",
    "llm_mini = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Spelling and structure check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_task_correct_mistakes = \"\"\"\n",
    "The input is a markdown file that is converted from a pdf using Mathpix API.\n",
    "The pdf contains questions and may contain the solutions too.\n",
    "As the original pdf may contain hand written text, the markdown file may contain mistakes in spelling, grammar and structure.\n",
    "\n",
    "Important things to remember:\n",
    "    1. Leave all Math commands and LaTeX formatting the same. As they are completely valid. Do not change the LaTeX formatting and expressions.\n",
    "    2. Only ever use LaTeX math delimiters for math expressions. I.e. use `$...$` for inline math, and `$$...$$` for display math.\n",
    "    3. Leave references to images and figures the same. I.e. do not change the image links or alt text.\n",
    "\n",
    "Your task is to:\n",
    "    1. Correct any spelling mistakes in the markdown file.\n",
    "    2. Correct any grammar mistakes in the markdown file.\n",
    "    3. Correct any layout mistakes in the markdown file, such that it follows the styles of the entire markdown file.\n",
    "    4. Do not change the content of the markdown file, only correct the mistakes.\n",
    "Output only a valid markdown file with the corrections applied, if any. Do not add any additional text or comments.\n",
    "\"\"\"\n",
    "\n",
    "def correct_mistakes_in_markdown(md_content: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "        {llm_task_correct_mistakes}\n",
    "\n",
    "        ```input\n",
    "        {md_content}\n",
    "        ```\n",
    "\n",
    "        Return the markdown now.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_nano.invoke(prompt)\n",
    "    print(\"Corrected markdown content:\")\n",
    "    print(response.content.strip())\n",
    "\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Extract Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define initial question model\n",
    "class QuestionModel(BaseModel):\n",
    "    # full question and full solution\n",
    "    question_content: str = Field(..., description=\"The content of the question.\")\n",
    "    solution_content: str = Field(..., description=\"The content of the solution.\")\n",
    "    images: list[str] = Field(..., description=\"A list of image URLs associated with the question.\")\n",
    "\n",
    "class AllQuestionsModel(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[QuestionModel] = Field(..., description=\"A list of questions.\")\n",
    "\n",
    "llm_task_seperate_questions = r\"\"\"\n",
    "    Your task is to extract all individual questions and their worked solutions from the provided markdown content.\n",
    "\n",
    "    1.  **Content Extraction:**\n",
    "        -   Identify a suitable `name` for the set of questions.\n",
    "        -   Identify the `year` if mentioned; otherwise, use \"0\".\n",
    "        -   For each question, carefully extract the full question text into `question_content` and the corresponding full solution/answer text into `solution_content`. They may not be in the same section.\n",
    "        -   If no solution is found, leave `solution_content` as an empty string `\"\"`.\n",
    "        -   Preserve all image tags like `![pictureTag](filename.jpg)`, making sure they are placed with their respective \"question_content\" and \"solution_content\".\n",
    "        -   For Each Question extract all image references (e.g., `filename.jpg`) found within the `question_content` and `solution_content` and place them in the `images` list.\n",
    "\n",
    "    2.  **Output Format (Crucial):**\n",
    "        -   You MUST output ONLY a single, raw, valid JSON string that matches the provided schema.\n",
    "        -   Do NOT include any explanations, comments, or markdown code blocks (like ```json).\n",
    "\n",
    "    3.  **JSON Formatting Rules:**\n",
    "        -   **Escape Backslashes:** All LaTeX backslashes (`\\`) MUST be escaped as double backslashes (`\\\\`). For example, `\\cup` must become `\\\\cup`. This is the most important rule.\n",
    "        -   **Newlines:** Use `\\n` for newlines within the JSON string values.\n",
    "        -   **Content Integrity:** Preserve all text, LaTeX (`$...$`, `$$...$$`), and image tags perfectly. Do not alter or summarize content.\n",
    "        -   **Strict Schema:** Ensure the final JSON has no trailing commas and includes all fields, even if they are empty.\n",
    "    \"\"\"\n",
    "\n",
    "def extract_questions(doc_page_content: str) -> dict:\n",
    "    # Initialise the parser for the output.\n",
    "    parser = PydanticOutputParser(pydantic_object=AllQuestionsModel)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Your task is to extract a JSON with the following structure exactly, ready to be parsed by a pydantic model:\n",
    "        {parser.get_format_instructions()}\n",
    "\n",
    "        {llm_task_seperate_questions}\n",
    "\n",
    "        Input markdown:\n",
    "        ```\n",
    "        {doc_page_content}\n",
    "        ```\n",
    "        Return the JSON now.\n",
    "    \"\"\"\n",
    "\n",
    "    # tries to call the LLM multiple times to ensure robustness.\n",
    "    for attempt_idx in range(3):\n",
    "        \n",
    "        # Call the LLM\n",
    "        response = llm_mini.invoke(prompt)\n",
    "\n",
    "        # Debug: print the raw LLM response\n",
    "        # print(\"Raw LLM Response:\")\n",
    "        # print(response)\n",
    "\n",
    "        try:\n",
    "            # Parse the response using the output parser.\n",
    "            parsed_output = parser.parse(response.content.strip())\n",
    "            print(\"LLM response successfully parsed as JSON with questions:\")\n",
    "            print(response.content)\n",
    "            # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "            return parsed_output.model_dump()\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing LLM response as JSON:\")\n",
    "            print(\"raw response:\")\n",
    "            print(response.content)\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "\n",
    "    print(\"Final raw LLM Response:\")\n",
    "    print(response.content)\n",
    "    raise Exception(\"Failed to parse LLM response as JSON after multiple attempts.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Back slash correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_latex_backslashes(text: str) -> str:\n",
    "    # This regex finds any single backslash `\\` that is not already part of an escaped backslash `\\\\`\n",
    "    # or a newline `\\n`, and replaces it with a double backslash `\\\\`.\n",
    "    return re.sub(r'\\\\\\\\|\\\\(?!n)', r'\\\\\\\\', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# Extract question parts and solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the tutorial output.\n",
    "class Set_Question(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the question (only the text, no numbering)\")\n",
    "    content: str = Field(..., description=\"Content of the question (no exercise title, no subquestions)\")\n",
    "    parts: list[str] = Field(..., description=\"List of parts within the question (only the text, no numbering)\")\n",
    "    images: list[str] = Field(..., description=\"List of image URLs associated with the question (no alt text, only URLs)\")\n",
    "\n",
    "class Set_Solution_Part(BaseModel):\n",
    "    part_solution: str = Field(..., description=\"The worked solution for the part (no numbering or counting)\")\n",
    "\n",
    "class Set_Solution(BaseModel):\n",
    "    parts_solutions: list[str] = Field(..., description=\"List of worked solutions for the question (no numbering or counting)\")\n",
    "\n",
    "    def __init__(self, parts_solutions: list[Set_Solution_Part]):\n",
    "        \"\"\"\n",
    "        Initialize the Set_Solution with a list of solutions for each part.\n",
    "        \n",
    "        Args:\n",
    "            parts_solutions (list[Set_Solution_Part]): The worked solutions for the parts.\n",
    "        \"\"\"\n",
    "        super().__init__(parts_solutions=[part.part_solution for part in parts_solutions])\n",
    "\n",
    "class Set_Question_With_Solution(Set_Question):\n",
    "    parts_solutions: list[str] = Field(..., description=\"The worked solution for the parts.\")\n",
    "\n",
    "    def __init__(self, question: Set_Question, solution: Set_Solution):\n",
    "        \"\"\"\n",
    "        Initialize the Set_Question_With_Solution with a question and its solution.\n",
    "        \n",
    "        Args:\n",
    "            question (Set_Question): The question object.\n",
    "            solution (Set_Solution): The solution object.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            **question.model_dump(),\n",
    "            parts_solutions=solution.parts_solutions\n",
    "        )\n",
    "\n",
    "\n",
    "class Set(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[Set_Question_With_Solution] = Field(..., description=\"List of questions in the set\")\n",
    "\n",
    "llm_task_seperate_parts_question = r\"\"\"\n",
    "    Your task is to separate a question's content into a main stem and distinct parts, then format it as a JSON object.\n",
    "    Follow these rules precisely:\n",
    "\n",
    "    1.  **Content Splitting:**\n",
    "        -   From the input `question_content`, identify the main introductory text (the stem) and place it in the `content` field.\n",
    "        -   Identify all sub-questions (e.g., \"(a)\", \"(b)\", \"i.\", \"ii.\") and place their text into the `parts` list.\n",
    "        -   Parts may also be implied.\n",
    "        -   All Question Must have at least one part.\n",
    "        -   Ensure that images references are correctly placed with their respective parts.\n",
    "        -   Preserve all content perfectly, including text, LaTeX, and image tags like `![pictureTag](filename.jpg)`.\n",
    "        -   Ensure no solution content is included in the `content` or `parts` fields.\n",
    "        -   The `title` should be a concise summary of the question.\n",
    "        -   The `images` list should be copied exactly from the input.\n",
    "\n",
    "    2.  **Output Format (Crucial):**\n",
    "        -   You MUST output ONLY a single, raw, valid JSON string.\n",
    "        -   Do NOT include any explanations, comments, or markdown code blocks (like ```json).\n",
    "\n",
    "    3.  **JSON Formatting Rules:**\n",
    "        -   **Escape Backslashes:** All LaTeX backslashes (`\\`) MUST be escaped as double backslashes (`\\\\`). For example, `\\cup` must become `\\\\cup`. This is the most important rule.\n",
    "        -   **Newlines:** Use `\\n` for newlines within the JSON string values.\n",
    "        -   **Content Integrity:** Preserve all text, LaTeX (`$...$`, `$$...$$`), and image tags (`![pictureTag](...)`) perfectly. Do not alter or summarize content.\n",
    "    \"\"\"\n",
    "\n",
    "llm_task_seperate_parts_solution = r\"\"\"\n",
    "    Your task is to extract the solution for a specific question part from the full solution provided.\n",
    "    Please follow these rules carefully:\n",
    "\n",
    "    1.  **Content Extraction:**\n",
    "        -   From the `full solution`, find the worked solution that corresponds to the given `question part`.\n",
    "        -   Make sure the solutions for all parts together include the entire full solution text, with no missing content.\n",
    "        -   Place this exact text into the `part_solution` field.\n",
    "        -   Ensure that images references are correctly placed with their respective parts.\n",
    "        -   Preserve all content perfectly, including text, LaTeX, and image tags like `![pictureTag](filename.jpg)`.\n",
    "        -   If no specific solution is found, use an empty string `\"\"`.\n",
    "\n",
    "    2.  **Output Format (Crucial):**\n",
    "        -   You MUST output ONLY a single, raw, valid JSON string.\n",
    "        -   Do NOT include any explanations, comments, or markdown code blocks (like ```json).\n",
    "\n",
    "    3.  **JSON Formatting Rules:**\n",
    "        -   **Escape Backslashes:** All LaTeX backslashes (`\\`) MUST be escaped as double backslashes (`\\\\`). For example, `\\cup` must become `\\\\cup` in the JSON string. This is the most important rule.\n",
    "        -   **Newlines:** Use `\\n` for newlines within the JSON string values.\n",
    "        -   **Math Delimiters:** Ensure all math delimiters (`$...$` and `$$...$$`) are correctly balanced and preserved.\n",
    "    \"\"\"\n",
    "\n",
    "def process_single_question(question_data):\n",
    "    \"\"\"Process a single question and its parts in parallel\"\"\"\n",
    "    question_idx, question = question_data\n",
    "    \n",
    "    # Initialize the output parser with the Set_Question schema.\n",
    "    question_parser = PydanticOutputParser(pydantic_object=Set_Question)\n",
    "    \n",
    "    # Process the question part\n",
    "    for attempt_idx in range(3):\n",
    "        prompt = f\"\"\"\n",
    "            Your task is to extract a JSON with the following structure exactly, ready to be parsed by a pydantic model:\n",
    "            {question_parser.get_format_instructions()}\n",
    "\n",
    "            {llm_task_seperate_parts_question}\n",
    "\n",
    "            Input Dictionary:\n",
    "            ```JSON\n",
    "            {json.dumps(question)}\n",
    "            ```\n",
    "\n",
    "            Return the JSON now.\n",
    "            \"\"\"\n",
    "        \n",
    "        response = llm_mini.invoke(prompt)\n",
    "        \n",
    "        try:\n",
    "            parsed_output_parts = question_parser.parse(response.content)\n",
    "            print(f\"LLM response successfully parsed question {question_idx + 1}.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing LLM response as JSON for question {question_idx + 1}:\")\n",
    "            print(f\"Retrying... Attempt No.{attempt_idx + 1}\")\n",
    "            time.sleep(2)\n",
    "    else:\n",
    "        print(\"Final LLM Response:\")\n",
    "        print(response.content)\n",
    "        raise Exception(f\"Failed to parse LLM response as JSON after multiple attempts for question {question_idx + 1}.\")\n",
    "\n",
    "    # Process solution parts in parallel\n",
    "    def process_solution_part(part_data):\n",
    "        part_idx, part = part_data\n",
    "        solution_parser = PydanticOutputParser(pydantic_object=Set_Solution_Part)\n",
    "        \n",
    "        for attempt_idx in range(3):\n",
    "            prompt = f\"\"\"\n",
    "                Your task is to extract a JSON with the following structure exactly, ready to be parsed by a pydantic model:\n",
    "                {solution_parser.get_format_instructions()}\n",
    "\n",
    "                {llm_task_seperate_parts_solution}\n",
    "\n",
    "                full solution:\n",
    "                {question[\"solution_content\"]}\n",
    "\n",
    "                question part:\n",
    "                {part}\n",
    "                \"\"\"\n",
    "            \n",
    "            response = llm_mini.invoke(prompt)\n",
    "            \n",
    "            try:\n",
    "                cleaned_response = escape_latex_backslashes(response.content.strip())\n",
    "                parsed_output_solution_part = solution_parser.parse(cleaned_response)\n",
    "                print(f\"LLM response successfully parsed solution for part {part_idx + 1} of question {question_idx + 1}.\")\n",
    "                return parsed_output_solution_part\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing LLM response as JSON for part {part_idx + 1} of question {question_idx + 1}:\")\n",
    "                print(f\"Retrying... Attempt No.{attempt_idx + 1}\")\n",
    "                time.sleep(2)\n",
    "        \n",
    "        raise Exception(f\"Failed to parse LLM response as JSON after multiple attempts for part {part_idx + 1} of question {question_idx + 1}.\")\n",
    "\n",
    "    # Process all parts in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        part_data_list = [(i, part) for i, part in enumerate(parsed_output_parts.parts)]\n",
    "        solutions_parts = list(executor.map(process_solution_part, part_data_list))\n",
    "\n",
    "    set_solution = Set_Solution(parts_solutions=solutions_parts)\n",
    "    return Set_Question_With_Solution(\n",
    "        question=parsed_output_parts,\n",
    "        solution=set_solution\n",
    "    )\n",
    "\n",
    "def extract_parts_question(questions_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "    Now processes questions in parallel while maintaining order.\n",
    "    \"\"\"\n",
    "\n",
    "    # Process all questions in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        question_data_list = [(i, q) for i, q in enumerate(questions_dict[\"questions\"])]\n",
    "        questions_in_parts = list(executor.map(process_single_question, question_data_list))\n",
    "\n",
    "    return Set(\n",
    "        name=questions_dict[\"name\"],\n",
    "        year=questions_dict[\"year\"],\n",
    "        questions=questions_in_parts\n",
    "    ).model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# LLM evaluation of the content of JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models for validation\n",
    "class PartTextModel(BaseModel):\n",
    "    part_text: str = Field(..., description=\"The text of the part\")\n",
    "\n",
    "class PartSolutionModel(BaseModel):\n",
    "    part_solution: str = Field(..., description=\"The solution for the part\")\n",
    "\n",
    "class QuestionContentModel(BaseModel):\n",
    "    title: str = Field(..., description=\"The title of the question\")\n",
    "    content: str = Field(..., description=\"The main content of the question\")\n",
    "\n",
    "llm_task_text_check = r\"\"\"\n",
    "    Your task is to validate and correct the content within the `part_text` field of the provided JSON input.\n",
    "    You MUST return ONLY a single, raw, valid JSON string that strictly follows the original schema. Do NOT add any explanations, comments, or markdown code blocks.\n",
    "\n",
    "    Apply these correction rules to the content inside the JSON fields:\n",
    "    1.  **JSON Escaping:** All LaTeX backslashes (`\\`) MUST be escaped as double backslashes (`\\\\`). For example, `\\cup` must be written as `\\\\cup`. Never escape backslashes for newlines (`\\n`), as they should remain as is.\n",
    "    2.  **Math Delimiters:** All mathematical content must be enclosed in `$...$` for inline math or `$$...$$` for display math. Ensure all delimiters are correctly balanced and closed. '$' and '$$' should not be used for any other purpose. Move all `\\n` outside the math delimiters.\n",
    "    3.  **Display Math:** `$$` delimiters must be on their own separate lines.\n",
    "    4.  **Image Tags:** Preserve image tags like `![pictureTag](filename.jpg)` exactly as they are.\n",
    "    5.  **Content Integrity:** Do not change, paraphrase, or summarize any text, formulas, or image links. Only fix formatting errors according to these rules.\n",
    "    6.  **Newlines:** Use `\\n` for newlines within the JSON string values.\n",
    "    \"\"\"\n",
    "\n",
    "def validate_part_text(part_text_data):\n",
    "    \"\"\"Validate a single part text with retry logic\"\"\"\n",
    "    question_idx, part_idx, part_text = part_text_data\n",
    "    part_text_parser = PydanticOutputParser(pydantic_object=PartTextModel)\n",
    "    \n",
    "    for attempt_idx in range(3):\n",
    "        part_text_validation_data = {\n",
    "            \"part_text\": part_text\n",
    "        }\n",
    "        \n",
    "        validation_prompt = f\"\"\"\n",
    "            Your task is to extract a JSON with the following structure exactly:\n",
    "            {part_text_parser.get_format_instructions()}\n",
    "\n",
    "            Your task is to validate and correct the content within the `part_text` field of the provided JSON input.\n",
    "            {llm_task_text_check}\n",
    "\n",
    "            Input Part Text:\n",
    "            ```json\n",
    "            {json.dumps(part_text_validation_data, indent=2)}\n",
    "            ```\n",
    "            return the JSON with the content fixed if needed.\n",
    "            \"\"\"\n",
    "\n",
    "        response = llm_mini.invoke(validation_prompt)\n",
    "\n",
    "        try:\n",
    "            parsed_output = part_text_parser.parse(response.content)\n",
    "            print(f\"LLM response successfully parsed part text validation for question {question_idx + 1}, part {part_idx + 1}\")\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(f\"Part text validation error for question {question_idx + 1}, part {part_idx + 1}: {ve}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing part text validation LLM response for question {question_idx + 1}, part {part_idx + 1}: {e}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "    \n",
    "    raise Exception(f\"Failed to parse part text validation LLM response after multiple attempts for question {question_idx + 1}, part {part_idx + 1}.\")\n",
    "\n",
    "def validate_part_solution(part_solution_data):\n",
    "    \"\"\"Validate a single part solution with retry logic\"\"\"\n",
    "    question_idx, part_idx, part_solution = part_solution_data\n",
    "    part_solution_parser = PydanticOutputParser(pydantic_object=PartSolutionModel)\n",
    "    \n",
    "    for attempt_idx in range(3):\n",
    "        part_solution_validation_data = {\n",
    "            \"part_solution\": part_solution\n",
    "        }\n",
    "        \n",
    "        validation_prompt = f\"\"\"\n",
    "            Your task is to extract a JSON with the following structure exactly:\n",
    "            {part_solution_parser.get_format_instructions()}\n",
    "\n",
    "            Your task is to validate and correct the content within the `part_solution` field of the provided JSON input.\n",
    "            {llm_task_text_check}\n",
    "\n",
    "            Input Part Solution:\n",
    "            ```json\n",
    "            {json.dumps(part_solution_validation_data, indent=2)}\n",
    "            ```\n",
    "            return the JSON with the content fixed if needed.\n",
    "            \"\"\"\n",
    "\n",
    "        response = llm_mini.invoke(validation_prompt)\n",
    "\n",
    "        try:\n",
    "            parsed_output = part_solution_parser.parse(response.content)\n",
    "            print(f\"LLM response successfully parsed part solution validation for question {question_idx + 1}, part {part_idx + 1}\")\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(f\"Part solution validation error for question {question_idx + 1}, part {part_idx + 1}: {ve}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing part solution validation LLM response for question {question_idx + 1}, part {part_idx + 1}: {e}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "    \n",
    "    raise Exception(f\"Failed to parse part solution validation LLM response after multiple attempts for question {question_idx + 1}, part {part_idx + 1}.\")\n",
    "\n",
    "def validate_question_content(question_data):\n",
    "    \"\"\"Validate question title and content with retry logic\"\"\"\n",
    "    question_idx, title, content = question_data\n",
    "    content_parser = PydanticOutputParser(pydantic_object=QuestionContentModel)\n",
    "    \n",
    "    for attempt_idx in range(3):\n",
    "        content_validation_data = {\n",
    "            \"title\": title,\n",
    "            \"content\": content\n",
    "        }\n",
    "        \n",
    "        validation_prompt = f\"\"\"\n",
    "            Your task is to extract a JSON with the following structure exactly:\n",
    "            {content_parser.get_format_instructions()}\n",
    "\n",
    "            Your task is to validate and correct the content within the `title` and `content` fields of the provided JSON input.\n",
    "            {llm_task_text_check}\n",
    "\n",
    "            Input Question Content:\n",
    "            ```json\n",
    "            {json.dumps(content_validation_data, indent=2)}\n",
    "            ```\n",
    "            return the JSON with the content fixed if needed.\n",
    "            \"\"\"\n",
    "\n",
    "        response = llm_mini.invoke(validation_prompt)\n",
    "\n",
    "        try:\n",
    "            parsed_output = content_parser.parse(response.content)\n",
    "            print(f\"LLM response successfully parsed content validation for question {question_idx + 1}\")\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(f\"Content validation error for question {question_idx + 1}: {ve}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing content validation LLM response for question {question_idx + 1}: {e}\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "    \n",
    "    raise Exception(f\"Failed to parse content validation LLM response after multiple attempts for question {question_idx + 1}.\")\n",
    "\n",
    "def process_single_question_validation(question_data):\n",
    "    \"\"\"Process validation for a single question's content, parts, and solutions in parallel\"\"\"\n",
    "    question_idx, question = question_data\n",
    "    \n",
    "    # Validate question content (title and content) separately\n",
    "    content_data = (question_idx, question.get(\"title\", \"\"), question.get(\"content\", \"\"))\n",
    "    validated_content = validate_question_content(content_data)\n",
    "    \n",
    "    # Prepare part text data for parallel processing\n",
    "    part_text_data_list = [\n",
    "        (question_idx, part_idx, part_text)\n",
    "        for part_idx, part_text in enumerate(question.get(\"parts\", []))\n",
    "    ]\n",
    "    \n",
    "    # Prepare part solution data for parallel processing\n",
    "    part_solution_data_list = [\n",
    "        (question_idx, part_idx, part_solution)\n",
    "        for part_idx, part_solution in enumerate(question.get(\"parts_solutions\", []))\n",
    "    ]\n",
    "    \n",
    "    # Process part texts and solutions in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit all validation tasks\n",
    "        part_text_futures = [executor.submit(validate_part_text, data) for data in part_text_data_list]\n",
    "        part_solution_futures = [executor.submit(validate_part_solution, data) for data in part_solution_data_list]\n",
    "        \n",
    "        # Collect results maintaining order\n",
    "        validated_part_texts = [future.result() for future in part_text_futures]\n",
    "        validated_part_solutions = [future.result() for future in part_solution_futures]\n",
    "\n",
    "    validated_parts = [p[\"part_text\"] for p in validated_part_texts]\n",
    "    validated_parts_solutions = [p[\"part_solution\"] for p in validated_part_solutions]\n",
    "    \n",
    "    return {\n",
    "        \"title\": validated_content[\"title\"],\n",
    "        \"content\": validated_content[\"content\"],\n",
    "        \"parts\": validated_parts,\n",
    "        \"parts_solutions\": validated_parts_solutions,\n",
    "        \"images\": question.get(\"images\", [])\n",
    "    }\n",
    "\n",
    "def content_texdown_check(validated_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Checks if the content of the JSON is in Texdown format by processing each question's content, parts, and solutions separately.\n",
    "    Now processes questions in parallel while maintaining order.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process all questions in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        question_data_list = [(i, q) for i, q in enumerate(validated_dict[\"questions\"])]\n",
    "        questions_in_parts = list(executor.map(process_single_question_validation, question_data_list))\n",
    "    \n",
    "    return {\n",
    "        \"name\": validated_dict[\"name\"],\n",
    "        \"year\": validated_dict[\"year\"],\n",
    "        \"questions\": questions_in_parts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_to_json(md_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "    \n",
    "    Args:\n",
    "        md_content (str): The content of a set.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    # corrected_md_content = correct_mistakes_in_markdown(md_content)\n",
    "    # print(\"Markdown content corrected for spelling, grammar, and structure.\")\n",
    "\n",
    "    questions_dict = extract_questions(md_content)\n",
    "    print(\"successfully extracted the questions from the markdown. Now extracting the parts...\")\n",
    "\n",
    "    extracted_dict = extract_parts_question(questions_dict)\n",
    "    print(\"succesfully extracted the parts from the questions.\")\n",
    "    print(json.dumps(extracted_dict, indent=2))\n",
    "    print(\"Now validating the content...\")\n",
    "\n",
    "    # content_validated_dict = content_texdown_check(extracted_dict)\n",
    "    # print(\"successfully validated the content.\")\n",
    "    # print(json.dumps(content_validated_dict, indent=2))\n",
    "    # print(\"successfully converted markdown to JSON.\")\n",
    "    \n",
    "    return extracted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_tutorial = md_to_json(md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Displaying questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title\n",
    "title = imported_tutorial[\"name\"] + \" \" + imported_tutorial[\"year\"]\n",
    "\n",
    "# Print the title\n",
    "print(f\"Title: {title}\\n\")\n",
    "\n",
    "# Extract questions\n",
    "questions = imported_tutorial[\"questions\"]\n",
    "\n",
    "print(questions)\n",
    "\n",
    "# Loop over and print each question\n",
    "for idx1, question in enumerate(questions, start=1):\n",
    "    print(f\"**Question {idx1}**:\\n{question.get('title')}\\n\")\n",
    "    print(f\"Content: {question.get('content')}\\n\")\n",
    "    for idx2, (part, part_answer) in enumerate(zip(question.get(\"parts\", []), question.get(\"parts_solutions\", [])), start=1):\n",
    "        print(f\"Question {idx1}:\")\n",
    "        print(f\"- Subquestion {idx2}: {part}\")\n",
    "        print(f\"- Worked Solution {idx2}: {part_answer}\")\n",
    "        print(\"\\n\")\n",
    "    print(\"-\" * 40)  # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# in2lambda to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = imported_tutorial[\"questions\"]\n",
    "\n",
    "in2lambda_questions = []\n",
    "\n",
    "# Loop over all questions and question_answers and use in2lambda API to create a JSON.\n",
    "for idx, question_dict in enumerate(questions, start=1):\n",
    "    parts = []\n",
    "    for part_question, part_solution in zip(question_dict.get(\"parts\", []), question_dict.get(\"parts_solutions\", [])):\n",
    "        part_obj = Part(\n",
    "            text=part_question,\n",
    "            worked_solution=part_solution\n",
    "        )\n",
    "        parts.append(part_obj)\n",
    "\n",
    "    # Handle image paths - ensure they exist\n",
    "    image_paths = []\n",
    "    for img in question_dict.get(\"images\", []):\n",
    "        if img.startswith(\"http\"):\n",
    "            # Skip URLs that weren't processed\n",
    "            continue\n",
    "        full_path = f\"{media_path}/{img}\"\n",
    "        if Path(full_path).exists():\n",
    "            image_paths.append(full_path)\n",
    "        else:\n",
    "            print(f\"Warning: Image file not found: {full_path}\")\n",
    "\n",
    "    question = Question(\n",
    "        title=question_dict.get(\"title\", f\"Question {idx}\"),\n",
    "        main_text=question_dict.get(\"content\", \"\"),\n",
    "        parts=parts,\n",
    "        images=image_paths\n",
    "    )\n",
    "    in2lambda_questions.append(question)\n",
    "\n",
    "try:\n",
    "    Module(in2lambda_questions).to_json(f\"{output_path}/out\")\n",
    "    print(\"JSON output successfully created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating JSON output: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
