{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# process description  \n",
    "\n",
    "the program takes in a pdf  \n",
    "mathpix is used to scan the pdf and turning it into markdown  \n",
    "markdown then processed to get the images  \n",
    "\n",
    "llm is used to extract the questions and solutions in **ONE** go.  \n",
    "the final JSON is made using the in2lambda api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from in2lambda.api.module import Module\n",
    "from in2lambda.api.question import Question\n",
    "from in2lambda.api.part import Part\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load environment variables from .env file.\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# scanning/processing the initial pdf into markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATHPIX_API_KEY = os.getenv(\"MATHPIX_API_KEY\")\n",
    "MATHPIX_APP_ID = os.getenv(\"MATHPIX_APP_ID\")\n",
    "\n",
    "def pdf_to_markdown(source_path: str, result_path: str):\n",
    "    ''' \n",
    "    converts the pdf at `source_path` to a markdown file at `result_path` using Mathpix API.\n",
    "    '''\n",
    "    # Upload PDF to Mathpix and returns a Markdown file with the content.\n",
    "    with open(source_path, \"rb\") as file:\n",
    "        r = requests.post(\n",
    "            \"https://api.mathpix.com/v3/pdf\",   \n",
    "            headers={\n",
    "                \"app_id\": MATHPIX_APP_ID,\n",
    "                \"app_key\": MATHPIX_API_KEY,\n",
    "            },\n",
    "            files={\"file\": file},\n",
    "        )\n",
    "        pdf_id = r.json()[\"pdf_id\"]\n",
    "        print(\"PDF ID:\", pdf_id)\n",
    "        print(\"Response:\", r.json())\n",
    "\n",
    "        url = f\"https://api.mathpix.com/v3/pdf/{pdf_id}.md\"\n",
    "        headers = {\n",
    "            \"app_id\": MATHPIX_APP_ID,\n",
    "            \"app_key\": MATHPIX_API_KEY,\n",
    "        }\n",
    "\n",
    "        max_retries = 10\n",
    "        retry_delay = 5  # seconds\n",
    "        for attempt in range(max_retries):\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                # Save the result if the request is successful\n",
    "                with open(result_path, \"w\") as f:\n",
    "                    f.write(response.text)\n",
    "                print(\"Downloaded MD successfully.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}/{max_retries}: Processing not complete. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "        else:\n",
    "            print(\"Failed to retrieve processed PDF after multiple attempts:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# setting up the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"conversion_content\"\n",
    "output_path = f\"{folder_path}/mathpix_to_llm_to_in2lambda_to_JSON_out\"\n",
    "media_path = f\"{output_path}/media\"\n",
    "\n",
    "Path(media_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "source_path = f\"{folder_path}/example.pdf\"\n",
    "result_path = f\"{output_path}/example.md\"\n",
    "\n",
    "# Only activate mathpix if the markdown has not been created yet.\n",
    "# This avoids unnecessary reprocessing of the same PDF.\n",
    "if not Path(f\"{output_path}/example.md\").exists():\n",
    "    pdf_to_markdown(source_path, result_path)\n",
    "\n",
    "with open(result_path, \"r\") as f:\n",
    "    md_content = f.read()\n",
    "\n",
    "# Print out a summary.\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {len(md_content)} characters\")\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {md_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# downlaoding extracted images from Mathpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the figures from the paper and answers.\n",
    "def extract_figures_from_text(text): #, ans=False):\n",
    "    \"\"\"\n",
    "    Extracts figures from the text using regex.\n",
    "    Finds figure references and their descriptions.\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    # Regex to match figure references and their descriptions\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    print(f\"Matches found: {matches}\")\n",
    "    \n",
    "    for match in matches:\n",
    "        url = match\n",
    "        url = url.strip()\n",
    "        figure_caption_pattern = rf'\\({re.escape(url)}\\)\\s*-?\\s*Figure\\s+(Q\\d+)\\s*-\\s*(.+?)\\n'\n",
    "        caption_match = re.search(figure_caption_pattern, text)\n",
    "\n",
    "        if caption_match:\n",
    "            title, description = caption_match.groups()\n",
    "            print(\"Caption match found\")\n",
    "        else:\n",
    "            title, description = \"\", \"\"\n",
    "\n",
    "        if url.startswith(\"http\"):\n",
    "            # Download the image and save it to a file\n",
    "            image = Image.open(requests.get(url, stream=True).raw)\n",
    "            # Create a figure name based on the URL\n",
    "            fig_name = os.path.basename(url)\n",
    "            figures[fig_name] = {\n",
    "                \"image\": image,\n",
    "                \"title\": title.strip(),\n",
    "                \"label\": description.strip(),\n",
    "                \"url\": url,\n",
    "                \"local_path\": \"\",\n",
    "                # \"answerFile\": ans\n",
    "            }\n",
    "    return figures\n",
    "\n",
    "# a dictionary storing information on the figures\n",
    "figures = extract_figures_from_text(md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# saving the images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figures_to_path(figures):\n",
    "    for idx, (fig_name, fig_info) in enumerate(figures.items()):\n",
    "        print(f\"FIGURE Title='{fig_info['title']}', Label='{fig_info['label']}', URL='{fig_info['url']}'\")\n",
    "        # image_name = f\"figure_{fig_info['title']}.png\" #{\"_ans\" if fig_info[\"answerFile\"] else \"\"}.png\"\n",
    "        # if image_name in os.listdir(f\"{set_path}media/\"):\n",
    "        #     image_name = f\"figure_{fig_info['title']}_{idx}.png\" #{\"_ans\" if fig_info[\"answerFile\"] else \"\"}.png\"\n",
    "        end_location = fig_name.index(\"?\")\n",
    "        image_name = f\"{idx}_{fig_name[:end_location]}\"\n",
    "        fig_info[\"local_path\"] = image_name\n",
    "        fig_info[\"image\"].save(f\"{media_path}{fig_info['local_path']}\")\n",
    "\n",
    "save_figures_to_path(figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# replacing url for images with local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_figures_in_markdown(md_content, figures):\n",
    "    #replace the image URLs in the markdown content with local paths\n",
    "    for fig_name, fig_info in figures.items():\n",
    "        md_content = md_content.replace(fig_info[\"url\"], fig_info[\"local_path\"])\n",
    "        print(f\"Replaced {fig_info['url']} with {fig_info['local_path']} in markdown content.\")\n",
    "    # Save the modified markdown content to a file\n",
    "    with open(f\"{output_path}/example.md\", \"w\") as f:\n",
    "        f.write(md_content)\n",
    "\n",
    "replace_figures_in_markdown(md_content, figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Initialising llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM via LangChain.\n",
    "llm = ChatOpenAI(\n",
    "            model=os.environ['OPENAI_MODEL'],\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Extract Questions and Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the tutorial output.\n",
    "class Set_Question(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the question (only the text, no numbering)\")\n",
    "    content: str = Field(..., description=\"Content of the question (no exercise title, no subquestions)\")\n",
    "    parts: list[str] = Field(..., description=\"List of parts within the question (only the text, no numbering)\")\n",
    "    parts_solutions: list[str] = Field(..., description=\"List of worked solutions for the question (no numbering or counting)\")\n",
    "\n",
    "class Set(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[Set_Question] = Field(..., description=\"List of questions in the set\")\n",
    "\n",
    "llm_task = \"\"\"\n",
    "Please follow these steps carefully:\n",
    "    1. You can decide what to call the Set.\n",
    "    2. Identify the year of the tutorial, if mentioned. Otherwise, use \"0\".\n",
    "    3. Every character should match the original source exactly unless you're instructed to split content into fields.\n",
    "    4. Identify the questions in the Input markdown and add them to the \"questions\" list.\n",
    "    5. for each question:\n",
    "        - Title is the only field where you are allowed to name it whatever you seem fit for the question.\n",
    "        - Identify the content of the question, which will be always visible above the individual parts. This field uses the Milkdown editor.\n",
    "        - Identify the parts of the question (subquestions) and their worked solutions. The parts could be obvious to find, like \"a)...\", \"b)...\", etc., or they could be implied by the question itself.\n",
    "        - If the worked solution is not given, leave the worked solution empty.\n",
    "        - Add the parts of the question (subquestions) and their worked solutions to the \"parts\" and \"parts_solutions\" lists, respectively.\n",
    "    6. Output only a valid, plain, raw JSON string matching the schema above, ready to parse immediately, with no code fence or extra text. Use plain newlines (not escaped as `\\n`).\n",
    "    7. The Text inside the JSON should be in Lexdown:\n",
    "        1. preserving all LaTeX math delimiters (`$...$` and `$$...$$`) and all formatting exactly as in the input, without paraphrasing, summarizing, or simplifying any mathematical expressions or formulas.\n",
    "        2. do not remove or collapse blank lines.\n",
    "        3. Do not escape characters like `\\n` or `\\\\`.\n",
    "\"\"\"\n",
    "\n",
    "def extract_questions(doc_page_content: str, extra_instruction: str = \"\") -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "\n",
    "    This function takes the content of a tutorial sheet (doc.page_content), constructs a prompt\n",
    "    instructing the LLM to infer the tutorial title and to split the text into separate questions.\n",
    "    The output must be a valid JSON string with the following structure:\n",
    "    \n",
    "    {\n",
    "        \"name\": \"<title of tutorial>\",\n",
    "        \"year\": \"<year of tutorial>\",\n",
    "        \"questions\": [\n",
    "            { title: \"exercise text 1\", content: \"content text exercise 1\", parts: [\"subquestion text 1\", \"subquestion text 2\", ...], parts_solutions: [\"solution text 1\", \"solution text 2\", ...] },\n",
    "            { title: \"exercise text 2\", content: \"content text exercise 2\", parts: [\"subquestion text 1\", \"subquestion text 2\", ...], parts_solutions: [\"solution text 1\", \"solution text 2\", ...] },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    the original text of the exercises. The function returns a dictionary parsed from the JSON output.\n",
    "    if any of the text mentions a figure/diagram, then also find the figure and add it to the content of the exercise.\n",
    "    \n",
    "    Args:\n",
    "        doc_page_content (str): The content of a set.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    # Initialize the output parser with the Tutorial schema.\n",
    "    parser = PydanticOutputParser(pydantic_object=Set)\n",
    "\n",
    "    # Construct the prompt, appending the parser's format instructions.\n",
    "    prompt = f\"\"\"\n",
    "        Input markdown:\n",
    "        ```markdown\n",
    "        {doc_page_content}\n",
    "        ```\n",
    "\n",
    "        Your task is to extract a JSON with the following structure exactly:\n",
    "        {parser.get_format_instructions()}\n",
    "\n",
    "        {llm_task}\n",
    "\n",
    "        {extra_instruction}\n",
    "\n",
    "        Return the JSON now.\n",
    "        \"\"\"\n",
    "    \n",
    "    # tries to call the LLM multiple times to ensure robustness.\n",
    "    for i in range(3):\n",
    "        \n",
    "        # Call the LLM\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        # Debug: print the raw LLM response\n",
    "        # print(\"Raw LLM Response:\")\n",
    "        # print(response)\n",
    "\n",
    "        try:\n",
    "            # Parse the response using the output parser.\n",
    "            parsed_output = parser.parse(response.content)\n",
    "            print(\"LLM response successfully parsed as JSON.\")\n",
    "            # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(\"❌ Pydantic Validation Error:\")\n",
    "            for error in ve.errors():\n",
    "                print(f\" - {error['loc']}: {error['msg']}\")\n",
    "            print(\"Raw LLM output:\")\n",
    "            print(response.content)\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing LLM response as JSON:\")\n",
    "            print(\"Retrying...\")\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# LLM evaluation of the content of JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_rules_obeyed_check(extracted_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "    \n",
    "    Args:\n",
    "        md_content (str): The content of a set.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    json_string = json.dumps(extracted_dict, indent=2)\n",
    "    \n",
    "    # prompt to let llm validate the JSON.\n",
    "    validation_prompt = f\"\"\"\n",
    "    You were given the following rules to follow:\n",
    "    {llm_task}\n",
    "\n",
    "    please make sure that the content of the JSON followed the rules above and return the new JSON.\n",
    "    {json_string}\n",
    "    \"\"\"\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=Set)\n",
    "    # loop 3 times to ensure robustness.\n",
    "    for i in range(3):\n",
    "        \n",
    "        # Call the LLM\n",
    "        response = llm.invoke(validation_prompt)\n",
    "\n",
    "        # Debug: print the raw LLM response\n",
    "        # print(\"Raw LLM Response:\")\n",
    "        # print(response)\n",
    "\n",
    "        try:\n",
    "            # Parse the response using the output parser.\n",
    "            parsed_output = parser.parse(response.content)\n",
    "            print(\"LLM response successfully parsed as validatedJSON.\")\n",
    "            # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(\"❌ Pydantic Validation Error:\")\n",
    "            for error in ve.errors():\n",
    "                print(f\" - {error['loc']}: {error['msg']}\")\n",
    "            print(\"Raw LLM output:\")\n",
    "            print(response.content)\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing validation LLM response as JSON:\")\n",
    "            print(\"Retrying...\")\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_texdown_check(validated_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Checks if the content of the JSON is in Texdown format.\n",
    "    \n",
    "    Args:\n",
    "        validated_dict (dict): The validated dictionary from the LLM.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    json_string = json.dumps(validated_dict, indent=2)\n",
    "    \n",
    "    # prompt to let llm validate the JSON.\n",
    "    validation_prompt = f\"\"\"\n",
    "    Here is a JSON:\n",
    "    ```json\n",
    "    {json_string}\n",
    "    ```\n",
    "\n",
    "    look inside questions, content, parts and parts_solutions, ensure that the content of the JSON follows these rules:\n",
    "        1. Ensure the JSON string contains no literal \"\\n\" or \"\\\\\" characters unless explicitly part of the input text.\n",
    "        2. All mathematical expressions and formulas must be fully enclosed within matching math delimiters: either inline math `$...$` or display math `$$...$$`.\n",
    "        3. Verify all `$$` delimiters are properly opened and closed; no unbalanced or partial math blocks allowed.\n",
    "        4. Verify all `$` delimiters are properly opened and closed; inline math should not span multiple lines.\n",
    "        5. Preserve all LaTeX formatting, including backslashes and braces, exactly as in the input without adding extra escaping or modifying math commands.\n",
    "        6. Blank lines inside math blocks must be preserved as-is.\n",
    "        7. Output only a valid JSON string without any additional escaping or characters.\n",
    "\n",
    "\n",
    "    return the JSON with the content fixed if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=Set)\n",
    "    \n",
    "    # loop 3 times to ensure robustness.\n",
    "    for i in range(3):\n",
    "        \n",
    "        # Call the LLM\n",
    "        response = llm.invoke(validation_prompt)\n",
    "\n",
    "        # Debug: print the raw LLM response\n",
    "        # print(\"Raw LLM Response:\")\n",
    "        # print(response)\n",
    "\n",
    "        try:\n",
    "            # Parse the response using the output parser.\n",
    "            parsed_output = parser.parse(response.content)\n",
    "            print(\"LLM response successfully parsed as Texdown JSON.\")\n",
    "            # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(\"❌ Pydantic Validation Error:\")\n",
    "            for error in ve.errors():\n",
    "                print(f\" - {error['loc']}: {error['msg']}\")\n",
    "            print(\"Raw LLM output:\")\n",
    "            print(response.content)\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing textdown LLM response as JSON:\")\n",
    "            print(\"Retrying...\")\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_to_json(md_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "    \n",
    "    Args:\n",
    "        md_content (str): The content of a set.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    extracted_dict = extract_questions(md_content)\n",
    "    # validated_dict = task_rules_obeyed_check(extracted_dict)\n",
    "    content_validated_dict = content_texdown_check(extracted_dict)\n",
    "    return content_validated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_tutorial = md_to_json(md_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title\n",
    "title = imported_tutorial[\"name\"] + \" \" + imported_tutorial[\"year\"]\n",
    "\n",
    "# Print the title\n",
    "print(f\"Title: {title}\\n\")\n",
    "\n",
    "# Extract questions\n",
    "questions = imported_tutorial[\"questions\"]\n",
    "\n",
    "print(questions)\n",
    "\n",
    "# Loop over and print each question\n",
    "for idx1, question in enumerate(questions, start=1):\n",
    "    print(f\"**Question {idx1}**:\\n{question.get(\"title\")}\\n\")\n",
    "    print(f\"Content: {question.get(\"content\")}\\n\")\n",
    "    for idx2, (part, part_answer) in enumerate(zip(question.get(\"parts\", []), question.get(\"parts_solutions\", [])), start=1):\n",
    "        print(f\"Question {idx1}:\")\n",
    "        print(f\"- Subquestion {idx2}: {part}\")\n",
    "        print(f\"- Worked Solution {idx2}: {part_answer}\")\n",
    "        print(\"\\n\")\n",
    "    print(\"-\" * 40)  # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Form JSON Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = imported_tutorial[\"questions\"]\n",
    "\n",
    "in2lambda_questions = []\n",
    "\n",
    "# Loop over all questions and question_answers and use in2lambda to create a JSON.\n",
    "for idx, question_dict in enumerate(questions, start=1):\n",
    "    parts = []\n",
    "    for part_question, part_solution in zip(question_dict.get(\"parts\", []), question_dict.get(\"parts_solutions\", [])):\n",
    "        part_obj = Part(\n",
    "            text=part_question,\n",
    "            worked_solution=part_solution\n",
    "        )\n",
    "        parts.append(part_obj)\n",
    "\n",
    "    question = Question(\n",
    "        title=question_dict.get(\"title\", f\"Question {idx}\"),\n",
    "        main_text=question_dict.get(\"content\", \"\"),\n",
    "        parts=parts\n",
    "    )\n",
    "    in2lambda_questions.append(question)\n",
    "\n",
    "Module(in2lambda_questions).to_json(f\"{output_path}/out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
