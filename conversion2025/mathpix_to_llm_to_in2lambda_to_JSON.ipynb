{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# process description  \n",
    "\n",
    "the program takes in a pdf  \n",
    "mathpix is used to scan the pdf and turning it into markdown  \n",
    "markdown then processed to get the images  \n",
    "\n",
    "llm is used to extract the questions and solutions in **ONE** go.  \n",
    "the final JSON is made using the in2lambda api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from in2lambda.api.module import Module\n",
    "from in2lambda.api.question import Question\n",
    "from in2lambda.api.part import Part\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load environment variables from .env file.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# scanning/processing the initial pdf into markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATHPIX_API_KEY = os.getenv(\"MATHPIX_API_KEY\")\n",
    "MATHPIX_APP_ID = os.getenv(\"MATHPIX_APP_ID\")\n",
    "\n",
    "def pdf_to_markdown(source_path: str, result_path: str):\n",
    "    ''' \n",
    "    converts the pdf at `source_path` to a markdown file at `result_path` using Mathpix API.\n",
    "    '''\n",
    "    # Upload PDF to Mathpix and returns a Markdown file with the content.\n",
    "    with open(source_path, \"rb\") as file:\n",
    "        r = requests.post(\n",
    "            \"https://api.mathpix.com/v3/pdf\",   \n",
    "            headers={\n",
    "                \"app_id\": MATHPIX_APP_ID,\n",
    "                \"app_key\": MATHPIX_API_KEY,\n",
    "            },\n",
    "            files={\"file\": file},\n",
    "        )\n",
    "        pdf_id = r.json()[\"pdf_id\"]\n",
    "        print(\"PDF ID:\", pdf_id)\n",
    "        print(\"Response:\", r.json())\n",
    "\n",
    "        url = f\"https://api.mathpix.com/v3/pdf/{pdf_id}.md\"\n",
    "        headers = {\n",
    "            \"app_id\": MATHPIX_APP_ID,\n",
    "            \"app_key\": MATHPIX_API_KEY,\n",
    "        }\n",
    "\n",
    "        max_retries = 10\n",
    "        retry_delay = 5  # seconds\n",
    "        for attempt in range(max_retries):\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                # Save the result if the request is successful\n",
    "                with open(result_path, \"w\") as f:\n",
    "                    f.write(response.text)\n",
    "                print(\"Downloaded MD successfully.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}/{max_retries}: Processing not complete. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "        else:\n",
    "            print(\"Failed to retrieve processed PDF after multiple attempts:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# setting up the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"conversion_content\"\n",
    "output_path = f\"{folder_path}/mathpix_to_llm_to_in2lambda_to_JSON_out\"\n",
    "media_path = f\"{output_path}/media\"\n",
    "\n",
    "Path(media_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "source_path = f\"{folder_path}/example.pdf\"\n",
    "result_path = f\"{output_path}/example.md\"\n",
    "\n",
    "# Only activate mathpix if the markdown has not been created yet.\n",
    "# This avoids unnecessary reprocessing of the same PDF.\n",
    "if not Path(result_path).exists():\n",
    "    if Path(source_path).exists():\n",
    "        pdf_to_markdown(source_path, result_path)\n",
    "    else:\n",
    "        print(f\"Error: Source PDF file not found at {source_path}\")\n",
    "        exit(1)\n",
    "\n",
    "try:\n",
    "    with open(result_path, \"r\") as f:\n",
    "        md_content = f.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Markdown file not found at {result_path}\")\n",
    "    exit(1)\n",
    "\n",
    "# Print out a summary.\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {len(md_content)} characters\")\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {md_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# downlaoding extracted images from Mathpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the figures from the paper and answers.\n",
    "def extract_figures_from_text(text): #, ans=False):\n",
    "    \"\"\"\n",
    "    Extracts figures from the text using regex.\n",
    "    Finds figure references and their descriptions.\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    # Regex to match figure references and their descriptions\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    print(f\"Matches found: {matches}\")\n",
    "    \n",
    "    for match in matches:\n",
    "        url = match\n",
    "        url = url.strip()\n",
    "        figure_caption_pattern = rf'\\({re.escape(url)}\\)\\s*-?\\s*Figure\\s+(Q\\d+)\\s*-\\s*(.+?)\\n'\n",
    "        caption_match = re.search(figure_caption_pattern, text)\n",
    "\n",
    "        if caption_match:\n",
    "            title, description = caption_match.groups()\n",
    "            print(\"Caption match found\")\n",
    "        else:\n",
    "            title, description = \"\", \"\"\n",
    "\n",
    "        if url.startswith(\"http\"):\n",
    "            # Download the image and save it to a file\n",
    "            image = Image.open(requests.get(url, stream=True).raw)\n",
    "            # Create a figure name based on the URL\n",
    "            fig_name = os.path.basename(url)\n",
    "            figures[fig_name] = {\n",
    "                \"image\": image,\n",
    "                \"title\": title.strip(),\n",
    "                \"label\": description.strip(),\n",
    "                \"url\": url,\n",
    "                \"local_path\": \"\",\n",
    "                # \"answerFile\": ans\n",
    "            }\n",
    "    return figures\n",
    "\n",
    "# a dictionary storing information on the figures\n",
    "figures = extract_figures_from_text(md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# saving the images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figures_to_path(figures):\n",
    "    for idx, (fig_name, fig_info) in enumerate(figures.items()):\n",
    "        print(f\"FIGURE Title='{fig_info['title']}', Label='{fig_info['label']}', URL='{fig_info['url']}'\")\n",
    "        # Extract file extension and create a clean filename\n",
    "        if \"?\" in fig_name:\n",
    "            end_location = fig_name.index(\"?\")\n",
    "            image_name = f\"{idx}_{fig_name[:end_location]}\"\n",
    "        else:\n",
    "            image_name = f\"{idx}_{fig_name}\"\n",
    "        \n",
    "        fig_info[\"local_path\"] = image_name\n",
    "        try:\n",
    "            fig_info[\"image\"].save(f\"{media_path}/{fig_info['local_path']}\")\n",
    "            print(f\"Saved image: {fig_info['local_path']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving image {image_name}: {e}\")\n",
    "\n",
    "save_figures_to_path(figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# replacing url for images with local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_figures_in_markdown(md_content, figures):\n",
    "    #replace the image URLs in the markdown content with local paths\n",
    "    # add pictureTag for Lambda Feedback to recognise it as a picture\n",
    "    md_content = md_content.replace(\"![]\", \"![pictureTag]\")\n",
    "    for fig_name, fig_info in figures.items():\n",
    "        md_content = md_content.replace(fig_info[\"url\"], fig_info[\"local_path\"])\n",
    "        print(f\"Replaced {fig_info['url']} with {fig_info['local_path']} in markdown content.\")\n",
    "    # Save the modified markdown content to a file\n",
    "    try:\n",
    "        with open(f\"{output_path}/example.md\", \"w\") as f:\n",
    "            f.write(md_content)\n",
    "        print(\"Modified markdown saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving modified markdown: {e}\")\n",
    "\n",
    "replace_figures_in_markdown(md_content, figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Initialising llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM via LangChain.\n",
    "\n",
    "# Uses gpt-4.1-nano:\n",
    "#    - a faster model\n",
    "#    - less intelligent\n",
    "\n",
    "llm_nano = ChatOpenAI(\n",
    "            model=\"gpt-4.1-nano\",\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )\n",
    "\n",
    "# Uses gpt-4o-mini:\n",
    "#    - more intelligent\n",
    "llm_mini = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Spelling and structure check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_task_correct_mistakes = \"\"\"\n",
    "The input is a markdown file that is converted from a pdf using Mathpix API.\n",
    "The pdf contains questions and may contain the solutions too.\n",
    "As the original pdf may contain hand written text, the markdown file may contain mistakes in spelling, grammar and structure.\n",
    "Your task is to:\n",
    "    1. Correct any spelling mistakes in the markdown file.\n",
    "    2. Correct any grammar mistakes in the markdown file.\n",
    "    3. Correct any structure mistakes in the markdown file, such that it follows the styles of the entire markdown file.\n",
    "    4. Do not change the content of the markdown file, only correct the mistakes.\n",
    "Output only a valid markdown file with the corrections applied, if any. Do not add any additional text or comments.\n",
    "\"\"\"\n",
    "\n",
    "def correct_mistakes_in_markdown(md_content: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "        {llm_task_correct_mistakes}\n",
    "\n",
    "        ```input\n",
    "        {md_content}\n",
    "        ```\n",
    "\n",
    "        Return the markdown now.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_nano.invoke(prompt)\n",
    "\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "# Extract Questions and Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define initial question model\n",
    "class QuestionModel(BaseModel):\n",
    "    # full question and full solution\n",
    "    question_content: str = Field(..., description=\"The content of the question.\")\n",
    "    solution_content: str = Field(..., description=\"The content of the solution.\")\n",
    "    images: list[str] = Field(..., description=\"A list of image URLs associated with the question.\")\n",
    "\n",
    "class AllQuestionsModel(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[QuestionModel] = Field(..., description=\"A list of questions.\")\n",
    "\n",
    "llm_task_seperate_questions = \"\"\"\n",
    "    Your task is to extract all the individual questions and their worked solutions from the markdown content.\n",
    "    please follow these steps carefully:\n",
    "        1. you can choose the name of \"AllQuestionModel\".\n",
    "        2. Identify the year of the tutorial, if mentioned. Otherwise, use \"0\".\n",
    "        3. Every character should match the original source exactly unless you're instructed to split content into fields, without adding escapes or modifications.\n",
    "        4. Look through the entire markdown:\n",
    "            - Do not neglect any images, figures, or other media mentioned in the question, do not alter or neglect the alt text and the image URL.\n",
    "            - Leave the Image links and alt text within the question/solution, but also make a copy and place it into the `images` field.\n",
    "            - Identify full Questions, place it into question_content, becareful to not Include the solution in the question.\n",
    "            - Identify the full Worked Solution for each full Question.\n",
    "            - If the Worked Solution is not found, try to find the Answers associated with it instead.\n",
    "            - If Worked Solution or Answers are found, place it into the solution_content. Otherwise leave as empty string, \"\".\n",
    "            - For each question and corresponding solution, extract all image references (like ![pictureTag](filename.jpg)) and place them into the `images` field. If no images, use empty array [].\n",
    "        5. Output only a valid, plain, raw JSON string matching the schema above, ready to parse immediately\n",
    "            - NO markdown code blocks, NO extra text, NO explanations.\n",
    "            - Use plain newlines (not escaped as `\\n`).\n",
    "            - In JSON strings, backslashes must be escaped. Use \\\\\\\\ for LaTeX backslashes.\n",
    "            - Always have each field in the JSON, even if it is empty.\n",
    "            - Becareful that the last element of a list is not followed by a comma.\n",
    "        6. The Text inside the JSON should be in Lexdown:\n",
    "            1. preserving all LaTeX math delimiters (`$...$` and `$$...$$`) and all formatting exactly as in the input, without paraphrasing, summarizing, or simplifying any mathematical expressions or formulas.\n",
    "            2. Do not remove or collapse blank lines.\n",
    "            3. Do not escape characters like `\\n` or `\\\\` except for JSON requirements.\n",
    "    \"\"\"\n",
    "\n",
    "def extract_questions(doc_page_content: str) -> dict:\n",
    "    # Initialise the parser for the output.\n",
    "    parser = PydanticOutputParser(pydantic_object=AllQuestionsModel)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Your task is to extract a JSON with the following structure exactly:\n",
    "        {parser.get_format_instructions()}\n",
    "\n",
    "        {llm_task_seperate_questions}\n",
    "\n",
    "        Input markdown:\n",
    "        ```\n",
    "        {doc_page_content}\n",
    "        ```\n",
    "        Return the JSON now.\n",
    "    \"\"\"\n",
    "\n",
    "    # tries to call the LLM multiple times to ensure robustness.\n",
    "    for attempt_idx in range(3):\n",
    "        \n",
    "        # Call the LLM\n",
    "        response = llm_mini.invoke(prompt)\n",
    "\n",
    "        # Debug: print the raw LLM response\n",
    "        # print(\"Raw LLM Response:\")\n",
    "        # print(response)\n",
    "\n",
    "        try:\n",
    "            # Parse the response using the output parser.\n",
    "            parsed_output = parser.parse(response.content.strip())\n",
    "            print(\"LLM response successfully parsed as JSON with questions:\")\n",
    "            print(response.content)\n",
    "            # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "            return parsed_output.model_dump()\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing LLM response as JSON:\")\n",
    "            print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "            time.sleep(2)\n",
    "\n",
    "    print(\"Final raw LLM Response:\")\n",
    "    print(repr(response.content))\n",
    "    raise Exception(\"Failed to parse LLM response as JSON after multiple attempts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the tutorial output.\n",
    "class Set_Question(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the question (only the text, no numbering)\")\n",
    "    content: str = Field(..., description=\"Content of the question (no exercise title, no subquestions)\")\n",
    "    parts: list[str] = Field(..., description=\"List of parts within the question (only the text, no numbering)\")\n",
    "    images: list[str] = Field(..., description=\"List of image URLs associated with the question (no alt text, only URLs)\")\n",
    "\n",
    "class Set_Solution_Part(BaseModel):\n",
    "    part_solution: str = Field(..., description=\"The worked solution for the part (no numbering or counting)\")\n",
    "\n",
    "class Set_Solution(BaseModel):\n",
    "    parts_solutions: list[str] = Field(..., description=\"List of worked solutions for the question (no numbering or counting)\")\n",
    "\n",
    "    def __init__(self, parts_solutions: list[Set_Solution_Part]):\n",
    "        \"\"\"\n",
    "        Initialize the Set_Solution with a list of solutions for each part.\n",
    "        \n",
    "        Args:\n",
    "            parts_solutions (list[Set_Solution_Part]): The worked solutions for the parts.\n",
    "        \"\"\"\n",
    "        super().__init__(parts_solutions=[part.part_solution for part in parts_solutions])\n",
    "\n",
    "class Set_Question_With_Solution(Set_Question):\n",
    "    parts_solutions: list[str] = Field(..., description=\"The worked solution for the parts.\")\n",
    "\n",
    "    def __init__(self, question: Set_Question, solution: Set_Solution):\n",
    "        \"\"\"\n",
    "        Initialize the Set_Question_With_Solution with a question and its solution.\n",
    "        \n",
    "        Args:\n",
    "            question (Set_Question): The question object.\n",
    "            solution (Set_Solution): The solution object.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            **question.model_dump(),\n",
    "            parts_solutions=solution.parts_solutions\n",
    "        )\n",
    "\n",
    "\n",
    "class Set(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[Set_Question_With_Solution] = Field(..., description=\"List of questions in the set\")\n",
    "\n",
    "\n",
    "# TODO: make parts completely seperate from stem to question\n",
    "# ensure no answer in question itself, only in parts_solutions.\n",
    "llm_task_seperate_parts_question = \"\"\"\n",
    "    Your task is to seperate the questions into individual parts.\n",
    "    Please follow these steps carefully:\n",
    "        1. Every character should match the original source exactly unless you're instructed to split content into fields, without adding escapes or modifications.\n",
    "        2. Use the same name and year.\n",
    "        3. Use the same list of images as in the input for each question.\n",
    "        4. For each question in questions:\n",
    "            - Title is the only field where you are allowed to name it whatever you seem fit for the question.\n",
    "            - Do not neglect any images, figures, or other media mentioned in the question, do not alter or neglect the alt text and the image URL.\n",
    "            - You may use the question_content and solution_content from the input, to help with knowing how to split the question into parts. Making sure that neither content nor parts contain any solution/answers.\n",
    "            - Try to get as many parts as possible, but do not split the question into too many parts.\n",
    "            - The parts may be obvious to find, like \"a)...\", \"b)...\", or, \"i)...\", \"ii)...\", etc, or they could be implied by the question itself. All question must have at least one part, if there is only one part.\n",
    "                1. The stem should be placed into the \"content\" field. Text in this field should be valid in the Milkdown editor. \n",
    "                2. the parts of the question (subquestions) should be placed into the \"parts\" field. Text in this field should be valid under Lexdown.\n",
    "        5. Output only a valid, plain, raw JSON string matching the schema above, ready to parse immediately\n",
    "            - NO markdown code blocks, NO extra text, NO explanations.\n",
    "            - Use plain newlines (not escaped as `\\n`).\n",
    "            - In JSON strings, backslashes must be escaped. Use \\\\\\\\ for LaTeX backslashes.\n",
    "            - Always have each field in the JSON, even if it is empty.\n",
    "            - Becareful that the last element of a list is not followed by a comma.\n",
    "        6. The Text inside the JSON should be in Lexdown:\n",
    "            1. preserving all LaTeX math delimiters (`$...$` and `$$...$$`) and all formatting exactly as in the input, without paraphrasing, summarizing, or simplifying any mathematical expressions or formulas.\n",
    "            2. Do not remove or collapse blank lines.\n",
    "            3. Do not escape characters like `\\n` or `\\\\` except for JSON requirements.\n",
    "    \"\"\"\n",
    "\n",
    "llm_task_seperate_parts_solution = \"\"\"\n",
    "    Your task is to extract the solution for each part of a question, there should be an equal number of solutions as there are parts in the question.\n",
    "    Please follow these steps carefully:\n",
    "        1. Every character should match the original source exactly unless you're instructed to split content into fields, without adding escapes or modifications.\n",
    "        2. Use the same list of images as in the input for each question.\n",
    "        3. For each parts of the question (subquestions):\n",
    "            - Carefully try to find the solution for each part, and place it into the \"part_solution\" field. Otherwise, leave as empty string. Text in this field should be valid under Lexdown.\n",
    "            - Make sure that the solution is only for the particular part.\n",
    "        4. Output only a valid, plain, raw JSON string matching the schema above, ready to parse immediately\n",
    "            - NO markdown code blocks, NO extra text, NO explanations.\n",
    "            - Use plain newlines (not escaped as `\\n`).\n",
    "            - In JSON strings, backslashes must be escaped. Use \\\\\\\\ for LaTeX backslashes.\n",
    "            - Always have each field in the JSON, even if it is empty.\n",
    "            - Becareful that the last element of a list is not followed by a comma.\n",
    "        5. The Text inside the JSON should be in Lexdown:\n",
    "            1. preserving all LaTeX math delimiters (`$...$` and `$$...$$`) and all formatting exactly as in the input, without paraphrasing, summarizing, or simplifying any mathematical expressions or formulas.\n",
    "            2. Do not remove or collapse blank lines.\n",
    "            3. Do not escape characters like `\\n` or `\\\\` except for JSON requirements.\n",
    "    \"\"\"\n",
    "\n",
    "def extract_parts_question(questions_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "\n",
    "    This function takes the content of a tutorial sheet (doc.page_content), constructs a prompt\n",
    "    instructing the LLM to infer the tutorial title and to split the text into separate questions.\n",
    "    The output must be a valid JSON string with the following structure:\n",
    "    \n",
    "    {\n",
    "        \"name\": \"<title of tutorial>\",\n",
    "        \"year\": \"<year of tutorial>\",\n",
    "        \"questions\": [\n",
    "            { title: \"exercise text 1\", content: \"content text exercise 1\", parts: [\"subquestion text 1\", \"subquestion text 2\", ...],\n",
    "            { title: \"exercise text 2\", content: \"content text exercise 2\", parts: [\"subquestion text 1\", \"subquestion text 2\", ...],\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    the original text of the exercises. The function returns a dictionary parsed from the JSON output.\n",
    "    if any of the text mentions a figure/diagram, then also find the figure and add it to the content of the exercise.\n",
    "    \n",
    "    Args:\n",
    "        doc_page_content (str): The content of a set.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    \n",
    "    questions_in_parts = []\n",
    "    for question_idx, question in enumerate(questions_dict[\"questions\"]):\n",
    "        question_parse_success = False\n",
    "\n",
    "        # Initialize the output parser with the Set_Question schema.\n",
    "        question_parser = PydanticOutputParser(pydantic_object=Set_Question)\n",
    "\n",
    "        for attempt_idx in range(3):\n",
    "\n",
    "            # Construct the prompt, appending the parser's format instructions.\n",
    "            prompt = f\"\"\"\n",
    "                Your task is to extract a JSON with the following structure exactly:\n",
    "                {question_parser.get_format_instructions()}\n",
    "\n",
    "                {llm_task_seperate_parts_question}\n",
    "\n",
    "                Input Dictionary:\n",
    "                ```JSON\n",
    "                {question}\n",
    "                ```\n",
    "\n",
    "                Return the JSON now.\n",
    "                \"\"\"\n",
    "            \n",
    "            # Call the LLM\n",
    "            response = llm_mini.invoke(prompt)\n",
    "\n",
    "            # Debug: print the raw LLM response\n",
    "            # print(\"Raw LLM Response:\")\n",
    "            # print(response)\n",
    "\n",
    "            try:\n",
    "                # Parse the response using the output parser.\n",
    "                parsed_output_parts = question_parser.parse(response.content)\n",
    "                print(f\"LLM response successfully parsed question {question_idx + 1}.\")\n",
    "                print(parsed_output_parts.content)\n",
    "                # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "                question_parse_success = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing LLM response as JSON for question {question_idx + 1}:\")\n",
    "                print(f\"Retrying... Attempt No.{attempt_idx + 1}\")\n",
    "                time.sleep(2)\n",
    "\n",
    "        if not question_parse_success:\n",
    "            print(\"Final LLM Response:\")\n",
    "            print(response.content)\n",
    "            raise Exception(f\"Failed to parse LLM response as JSON after multiple attempts for question {question_idx + 1}.\")\n",
    "        \n",
    "\n",
    "        # Initialize the output parser with the Set_Solution schema.\n",
    "        solution_parser = PydanticOutputParser(pydantic_object=Set_Solution_Part)\n",
    "        solutions_parts = []\n",
    "        for part_idx, part in enumerate(parsed_output_parts.parts):\n",
    "\n",
    "            solution_parse_success = False\n",
    "            for attempt_idx in range(3):\n",
    "                # Construct the prompt, appending the parser's format instructions.\n",
    "                prompt = f\"\"\"\n",
    "                    Your task is to extract a JSON with the following structure exactly:\n",
    "                    {solution_parser.get_format_instructions()}\n",
    "\n",
    "                    {llm_task_seperate_parts_solution}\n",
    "\n",
    "                    full solution:\n",
    "                    {question[\"solution_content\"]}\n",
    "\n",
    "                    question part:\n",
    "                    {part}\n",
    "                    \"\"\"\n",
    "                \n",
    "                # Call the LLM\n",
    "                response = llm_mini.invoke(prompt)\n",
    "\n",
    "                try:\n",
    "                    # Parse the response using the output parser.\n",
    "                    parsed_output_solution_part = solution_parser.parse(response.content)\n",
    "                    print(f\"LLM response successfully parsed solution for part {part_idx + 1} of question {question_idx + 1}.\")\n",
    "                    print(response.content)\n",
    "                    # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "                    solution_parse_success = True\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing LLM response as JSON for part {part_idx + 1} of question {question_idx + 1}:\")\n",
    "                    print(f\"Outputted response:\\n{response.content}\")\n",
    "                    print(f\"Retrying... Attempt No.{attempt_idx + 1}\")\n",
    "                    time.sleep(2)\n",
    "\n",
    "            if not solution_parse_success:\n",
    "                print(\"Final LLM Response:\")\n",
    "                print(response.content)\n",
    "                raise Exception(f\"Failed to parse LLM response as JSON after multiple attempts for part {part_idx + 1} of question {question_idx + 1}.\")\n",
    "\n",
    "            solutions_parts.append(parsed_output_solution_part)\n",
    "\n",
    "        set_solution = Set_Solution(parts_solutions=solutions_parts)\n",
    "        set_question_with_solution = Set_Question_With_Solution(\n",
    "            question=parsed_output_parts,\n",
    "            solution=set_solution\n",
    "        )\n",
    "        questions_in_parts.append(set_question_with_solution)\n",
    "\n",
    "    return Set(\n",
    "        name=questions_dict[\"name\"],\n",
    "        year=questions_dict[\"year\"],\n",
    "        questions=questions_in_parts\n",
    "    ).model_dump()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# LLM evaluation of the content of JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PartModel for validation\n",
    "class PartModel(BaseModel):\n",
    "    part_text: str = Field(..., description=\"The text of the part\")\n",
    "    part_solution: str = Field(..., description=\"The solution for the part\")\n",
    "\n",
    "llm_task_expression_check = r\"\"\"\n",
    "    Look inside the structure, specifically the `part_text` and `part_solution` fields. Ensure that the JSON content follows these rules:\n",
    "        1. JSON escaping: In JSON strings, backslashes must be escaped. Use \\\\\\\\ for LaTeX backslashes (e.g., \"$A \\\\\\\\cup B$\" not \"$A \\\\cup B$\").\n",
    "        2. Math delimiters: All LaTeX math commands and math macros must be fully enclosed within math delimiters â€” use `$...$` for inline math, and `$$...$$` for display math.\n",
    "        3. Balanced delimiters:\n",
    "            - All `$$` and `$` must be properly opened and closed.\n",
    "            - No unbalanced or partial math blocks.\n",
    "        4. Display math formatting:\n",
    "            - The opening `$$` must appear on a new line.\n",
    "            - The closing `$$` must also be on its own new line.\n",
    "            - The math content must appear immediately between them, with no extra blank lines unless they are part of the input.\n",
    "        5. Inline math rules:\n",
    "            - `$...$` should not span multiple lines.\n",
    "            - Avoid using `$$` for short inline expressions.\n",
    "        6. Preserve LaTeX syntax:\n",
    "            - All LaTeX commands, braces (`{}`, `[]`), and special characters must be preserved exactly as in the original input.\n",
    "            - Remember: in JSON, use \\\\\\\\ for each LaTeX backslash.\n",
    "        7. Blank lines:\n",
    "            - Preserve all blank lines inside math blocks.\n",
    "            - Outside math, follow the structure of the original input.\n",
    "        8. Alt text and image URLs:\n",
    "            - Ensure that all image URLs and alt text are preserved as they appear in the original input.\n",
    "            - The alt text must be `pictureTag`.\n",
    "        9. Output format:\n",
    "            - Output a single valid JSON string.\n",
    "            - Do not include any extra characters, explanations, or escaped formatting outside the JSON structure.\n",
    "            - No literal \\\\n sequences - use actual newlines in JSON strings.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def content_texdown_check(validated_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Checks if the content of the JSON is in Texdown format by processing each part individually.\n",
    "    \n",
    "    Args:\n",
    "        validated_dict (dict): The validated dictionary from the LLM.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    part_parser = PydanticOutputParser(pydantic_object=PartModel)\n",
    "\n",
    "    questions_in_parts = []\n",
    "    for question_idx, question in enumerate(validated_dict[\"questions\"]):\n",
    "        \n",
    "        # Process each part individually\n",
    "        validated_parts = []\n",
    "        validated_parts_solutions = []\n",
    "        \n",
    "        for part_idx, (part_text, part_solution) in enumerate(zip(question.get(\"parts\", []), question.get(\"parts_solutions\", []))):\n",
    "            passed = False\n",
    "            \n",
    "            # loop 3 times to ensure robustness.\n",
    "            for attempt_idx in range(3):\n",
    "                # Create a part object for validation\n",
    "                part_data = {\n",
    "                    \"part_text\": part_text,\n",
    "                    \"part_solution\": part_solution\n",
    "                }\n",
    "                \n",
    "                # prompt to let llm validate the part.\n",
    "                validation_prompt = f\"\"\"\n",
    "                    Your task is to extract a JSON with the following structure exactly:\n",
    "                    {part_parser.get_format_instructions()}\n",
    "\n",
    "                    {llm_task_expression_check}\n",
    "\n",
    "                    Input Part:\n",
    "                    ```json\n",
    "                    {json.dumps(part_data, indent=2)}\n",
    "                    ```\n",
    "                    return the JSON with the content fixed if needed.\n",
    "                    \"\"\"\n",
    "\n",
    "                # Call the LLM\n",
    "                response = llm_nano.invoke(validation_prompt)\n",
    "\n",
    "                try:\n",
    "                    # Parse the response using the output parser.\n",
    "                    parsed_output = part_parser.parse(response.content)\n",
    "                    print(f\"LLM response successfully parsed as JSON with valid $$ for question {question_idx + 1}, part {part_idx + 1}.\")\n",
    "                    # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "                    validated_part = parsed_output.model_dump()\n",
    "                    validated_parts.append(validated_part[\"part_text\"])\n",
    "                    validated_parts_solutions.append(validated_part[\"part_solution\"])\n",
    "                    passed = True\n",
    "                    break\n",
    "                except ValidationError as ve:\n",
    "                    print(f\"Validation error for question {question_idx + 1}, part {part_idx + 1}: {ve}\")\n",
    "                    print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "                    print(ve.errors())\n",
    "                    time.sleep(2)\n",
    "                except Exception as e:\n",
    "                    print(\"LLM Response:\")\n",
    "                    print(response.content)\n",
    "                    print(f\"Error parsing textdown LLM response as JSON for question {question_idx + 1}, part {part_idx + 1}: {e}\")\n",
    "                    print(\"Retrying... Attempt No.\", attempt_idx + 1)\n",
    "                    time.sleep(2)\n",
    "            \n",
    "            if not passed:\n",
    "                print(\"Final LLM Response:\")\n",
    "                print(response.content)\n",
    "                raise Exception(f\"Failed to parse LLM response as JSON after multiple attempts for question {question_idx + 1}, part {part_idx + 1}.\")\n",
    "        \n",
    "        # Create the validated question with processed parts\n",
    "        validated_question = {\n",
    "            \"title\": question.get(\"title\", \"\"),\n",
    "            \"content\": question.get(\"content\", \"\"),\n",
    "            \"parts\": validated_parts,\n",
    "            \"parts_solutions\": validated_parts_solutions,\n",
    "            \"images\": question.get(\"images\", [])\n",
    "        }\n",
    "        questions_in_parts.append(validated_question)\n",
    "    \n",
    "    return {\n",
    "        \"name\": validated_dict[\"name\"],\n",
    "        \"year\": validated_dict[\"year\"],\n",
    "        \"questions\": questions_in_parts\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_to_json(md_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "    \n",
    "    Args:\n",
    "        md_content (str): The content of a set.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    corrected_md_content = correct_mistakes_in_markdown(md_content)\n",
    "    print(\"Markdown content corrected for spelling, grammar, and structure.\")\n",
    "\n",
    "    questions_dict = extract_questions(corrected_md_content)\n",
    "    print(\"successfully extracted the questions from the markdown. Now extracting the parts...\")\n",
    "\n",
    "    extracted_dict = extract_parts_question(questions_dict)\n",
    "    print(\"succesfully extracted the parts from the questions. Now validating the content...\")\n",
    "\n",
    "    content_validated_dict = content_texdown_check(extracted_dict)\n",
    "    print(\"successfully validated the content.\")\n",
    "    print(\"successfully converted markdown to JSON.\")\n",
    "    \n",
    "    return content_validated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_tutorial = md_to_json(md_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title\n",
    "title = imported_tutorial[\"name\"] + \" \" + imported_tutorial[\"year\"]\n",
    "\n",
    "# Print the title\n",
    "print(f\"Title: {title}\\n\")\n",
    "\n",
    "# Extract questions\n",
    "questions = imported_tutorial[\"questions\"]\n",
    "\n",
    "print(questions)\n",
    "\n",
    "# Loop over and print each question\n",
    "for idx1, question in enumerate(questions, start=1):\n",
    "    print(f\"**Question {idx1}**:\\n{question.get('title')}\\n\")\n",
    "    print(f\"Content: {question.get('content')}\\n\")\n",
    "    for idx2, (part, part_answer) in enumerate(zip(question.get(\"parts\", []), question.get(\"parts_solutions\", [])), start=1):\n",
    "        print(f\"Question {idx1}:\")\n",
    "        print(f\"- Subquestion {idx2}: {part}\")\n",
    "        print(f\"- Worked Solution {idx2}: {part_answer}\")\n",
    "        print(\"\\n\")\n",
    "    print(\"-\" * 40)  # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Form JSON Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = imported_tutorial[\"questions\"]\n",
    "\n",
    "in2lambda_questions = []\n",
    "\n",
    "# Loop over all questions and question_answers and use in2lambda API to create a JSON.\n",
    "for idx, question_dict in enumerate(questions, start=1):\n",
    "    parts = []\n",
    "    for part_question, part_solution in zip(question_dict.get(\"parts\", []), question_dict.get(\"parts_solutions\", [])):\n",
    "        part_obj = Part(\n",
    "            text=part_question,\n",
    "            worked_solution=part_solution\n",
    "        )\n",
    "        parts.append(part_obj)\n",
    "\n",
    "    # Handle image paths - ensure they exist\n",
    "    image_paths = []\n",
    "    for img in question_dict.get(\"images\", []):\n",
    "        if img.startswith(\"http\"):\n",
    "            # Skip URLs that weren't processed\n",
    "            continue\n",
    "        full_path = f\"{media_path}/{img}\"\n",
    "        if Path(full_path).exists():\n",
    "            image_paths.append(full_path)\n",
    "        else:\n",
    "            print(f\"Warning: Image file not found: {full_path}\")\n",
    "\n",
    "    question = Question(\n",
    "        title=question_dict.get(\"title\", f\"Question {idx}\"),\n",
    "        main_text=question_dict.get(\"content\", \"\"),\n",
    "        parts=parts,\n",
    "        images=image_paths\n",
    "    )\n",
    "    in2lambda_questions.append(question)\n",
    "\n",
    "try:\n",
    "    Module(in2lambda_questions).to_json(f\"{output_path}/out\")\n",
    "    print(\"JSON output successfully created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating JSON output: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
