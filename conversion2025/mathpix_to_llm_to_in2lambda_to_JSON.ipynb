{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# process description  \n",
    "\n",
    "the program takes in a pdf  \n",
    "mathpix is used to scan the pdf and turning it into markdown  \n",
    "markdown then processed to get the images  \n",
    "\n",
    "llm is used to extract the questions and solutions in **ONE** go.  \n",
    "the final JSON is made using the in2lambda api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from in2lambda.api.module import Module\n",
    "from in2lambda.api.question import Question\n",
    "from in2lambda.api.part import Part\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Load environment variables from .env file.\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# scanning/processing the initial pdf into markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MATHPIX_API_KEY = os.getenv(\"MATHPIX_API_KEY\")\n",
    "MATHPIX_APP_ID = os.getenv(\"MATHPIX_APP_ID\")\n",
    "\n",
    "def pdf_to_markdown(source_path: str, result_path: str):\n",
    "    ''' \n",
    "    converts the pdf at `source_path` to a markdown file at `result_path` using Mathpix API.\n",
    "    '''\n",
    "    # Upload PDF to Mathpix and returns a Markdown file with the content.\n",
    "    with open(source_path, \"rb\") as file:\n",
    "        r = requests.post(\n",
    "            \"https://api.mathpix.com/v3/pdf\",   \n",
    "            headers={\n",
    "                \"app_id\": MATHPIX_APP_ID,\n",
    "                \"app_key\": MATHPIX_API_KEY,\n",
    "            },\n",
    "            files={\"file\": file},\n",
    "        )\n",
    "        pdf_id = r.json()[\"pdf_id\"]\n",
    "        print(\"PDF ID:\", pdf_id)\n",
    "        print(\"Response:\", r.json())\n",
    "\n",
    "        url = f\"https://api.mathpix.com/v3/pdf/{pdf_id}.md\"\n",
    "        headers = {\n",
    "            \"app_id\": MATHPIX_APP_ID,\n",
    "            \"app_key\": MATHPIX_API_KEY,\n",
    "        }\n",
    "\n",
    "        max_retries = 10\n",
    "        retry_delay = 5  # seconds\n",
    "        for attempt in range(max_retries):\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                # Save the result if the request is successful\n",
    "                with open(result_path, \"w\") as f:\n",
    "                    f.write(response.text)\n",
    "                print(\"Downloaded MD successfully.\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}/{max_retries}: Processing not complete. Retrying in {retry_delay} seconds...\")\n",
    "                time.sleep(retry_delay)\n",
    "        else:\n",
    "            print(\"Failed to retrieve processed PDF after multiple attempts:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# setting up the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"conversion_content\"\n",
    "output_path = f\"{folder_path}/mathpix_to_llm_to_in2lambda_to_JSON_out\"\n",
    "media_path = f\"{output_path}/media\"\n",
    "\n",
    "Path(media_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "source_path = f\"{folder_path}/example.pdf\"\n",
    "result_path = f\"{output_path}/example.md\"\n",
    "\n",
    "# Only activate mathpix if the markdown has not been created yet.\n",
    "# This avoids unnecessary reprocessing of the same PDF.\n",
    "if not Path(f\"{output_path}/example.md\").exists():\n",
    "    pdf_to_markdown(source_path, result_path)\n",
    "\n",
    "with open(result_path, \"r\") as f:\n",
    "    md_content = f.read()\n",
    "\n",
    "# Print out a summary.\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {len(md_content)} characters\")\n",
    "print(\"Markdown text: \")\n",
    "print(f\"  {result_path}: {md_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# downlaoding extracted images from Mathpix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the figures from the paper and answers.\n",
    "def extract_figures_from_text(text): #, ans=False):\n",
    "    \"\"\"\n",
    "    Extracts figures from the text using regex.\n",
    "    Finds figure references and their descriptions.\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    # Regex to match figure references and their descriptions\n",
    "    pattern = r'!\\[.*?\\]\\((.*?)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    print(f\"Matches found: {matches}\")\n",
    "    \n",
    "    for match in matches:\n",
    "        url = match\n",
    "        url = url.strip()\n",
    "        figure_caption_pattern = rf'\\({re.escape(url)}\\)\\s*-?\\s*Figure\\s+(Q\\d+)\\s*-\\s*(.+?)\\n'\n",
    "        caption_match = re.search(figure_caption_pattern, text)\n",
    "\n",
    "        if caption_match:\n",
    "            title, description = caption_match.groups()\n",
    "            print(\"Caption match found\")\n",
    "        else:\n",
    "            title, description = \"\", \"\"\n",
    "\n",
    "        if url.startswith(\"http\"):\n",
    "            # Download the image and save it to a file\n",
    "            image = Image.open(requests.get(url, stream=True).raw)\n",
    "            # Create a figure name based on the URL\n",
    "            fig_name = os.path.basename(url)\n",
    "            figures[fig_name] = {\n",
    "                \"image\": image,\n",
    "                \"title\": title.strip(),\n",
    "                \"label\": description.strip(),\n",
    "                \"url\": url,\n",
    "                \"local_path\": \"\",\n",
    "                # \"answerFile\": ans\n",
    "            }\n",
    "    return figures\n",
    "\n",
    "# a dictionary storing information on the figures\n",
    "figures = extract_figures_from_text(md_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# saving the images locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figures_to_path(figures):\n",
    "    for idx, (fig_name, fig_info) in enumerate(figures.items()):\n",
    "        print(f\"FIGURE Title='{fig_info['title']}', Label='{fig_info['label']}', URL='{fig_info['url']}'\")\n",
    "        # image_name = f\"figure_{fig_info['title']}.png\" #{\"_ans\" if fig_info[\"answerFile\"] else \"\"}.png\"\n",
    "        # if image_name in os.listdir(f\"{set_path}media/\"):\n",
    "        #     image_name = f\"figure_{fig_info['title']}_{idx}.png\" #{\"_ans\" if fig_info[\"answerFile\"] else \"\"}.png\"\n",
    "        end_location = fig_name.index(\"?\")\n",
    "        image_name = f\"{idx}_{fig_name[:end_location]}\"\n",
    "        fig_info[\"local_path\"] = image_name\n",
    "        fig_info[\"image\"].save(f\"{media_path}{fig_info['local_path']}\")\n",
    "\n",
    "save_figures_to_path(figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# replacing url for images with local path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_figures_in_markdown(md_content, figures):\n",
    "    #replace the image URLs in the markdown content with local paths\n",
    "    for fig_name, fig_info in figures.items():\n",
    "        md_content = md_content.replace(fig_info[\"url\"], fig_info[\"local_path\"])\n",
    "        print(f\"Replaced {fig_info['url']} with {fig_info['local_path']} in markdown content.\")\n",
    "    # Save the modified markdown content to a file\n",
    "    with open(f\"{output_path}/example.md\", \"w\") as f:\n",
    "        f.write(md_content)\n",
    "\n",
    "replace_figures_in_markdown(md_content, figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Initialising llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the LLM via LangChain.\n",
    "llm = ChatOpenAI(\n",
    "            model=os.environ['OPENAI_MODEL'],\n",
    "            api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# Extract Questions and Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define initial question model\n",
    "class QuestionModel(BaseModel):\n",
    "    # full question and full solution\n",
    "    question_content: str = Field(..., description=\"The content of the question.\")\n",
    "    solution_content: str = Field(..., description=\"The content of the solution.\")\n",
    "\n",
    "class AllQuestionsModel(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[QuestionModel] = Field(..., description=\"A list of questions.\")\n",
    "\n",
    "llm_task_seperate_questions = \"\"\"\n",
    "    Your task is to extract all the individual questions and their worked solutions from the markdown content.\n",
    "    please follow these steps carefully:\n",
    "        1. you can choose the name of \"AllQuestionModel\".\n",
    "        2. Identify the year of the tutorial, if mentioned. Otherwise, use \"0\".\n",
    "        3. Every character should match the original source exactly unless you're instructed to split content into fields, without adding escapes or modifications.\n",
    "        4. Look through the entire markdown:\n",
    "            - Without ignoring any mentions of images, figures, or other media.\n",
    "            - Identify full Questions, place it into question_content\n",
    "            - Identify the full Worked Solution for each full Question.\n",
    "            - If the Worked Solution is not found, try to find the Answers associated with it instead.\n",
    "            - If Worked Solution or Answers are found, place it into the solution_content. Otherwise leave as empty string, \"\".\n",
    "        5. Output only a valid, plain, raw JSON string matching the schema above, ready to parse immediately, with no code fence or extra text. Use plain newlines (not escaped as `\\n`).\n",
    "        6. The Text inside the JSON should be in Lexdown:\n",
    "            1. preserving all LaTeX math delimiters (`$...$` and `$$...$$`) and all formatting exactly as in the input, without paraphrasing, summarizing, or simplifying any mathematical expressions or formulas.\n",
    "            2. do not remove or collapse blank lines.\n",
    "            3. Do not escape characters like `\\n` or `\\\\`.\n",
    "    \"\"\"\n",
    "\n",
    "def extract_questions(doc_page_content: str) -> dict:\n",
    "    # Initialise the parser for the output.\n",
    "    parser = PydanticOutputParser(pydantic_object=AllQuestionsModel)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "        Your task is to extract a JSON with the following structure exactly:\n",
    "        {parser.get_format_instructions()}\n",
    "\n",
    "        {llm_task_seperate_questions}\n",
    "\n",
    "        Input markdown:\n",
    "        ```\n",
    "        {doc_page_content}\n",
    "        ```\n",
    "        Return the JSON now.\n",
    "    \"\"\"\n",
    "\n",
    "    # tries to call the LLM multiple times to ensure robustness.\n",
    "    for i in range(3):\n",
    "        \n",
    "        # Call the LLM\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        # Debug: print the raw LLM response\n",
    "        # print(\"Raw LLM Response:\")\n",
    "        # print(response)\n",
    "\n",
    "        try:\n",
    "            # Parse the response using the output parser.\n",
    "            parsed_output = parser.parse(response.content)\n",
    "            print(\"LLM response successfully parsed as JSON with questions.\")\n",
    "            # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(\"❌ Pydantic Validation Error:\")\n",
    "            for error in ve.errors():\n",
    "                print(f\" - {error['loc']}: {error['msg']}\")\n",
    "            print(\"Raw LLM output:\")\n",
    "            print(response.content)\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing LLM response as JSON:\")\n",
    "            print(\"Retrying...\")\n",
    "            time.sleep(2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the tutorial output.\n",
    "class Set_Question(BaseModel):\n",
    "    title: str = Field(..., description=\"Title of the question (only the text, no numbering)\")\n",
    "    content: str = Field(..., description=\"Content of the question (no exercise title, no subquestions)\")\n",
    "    parts: list[str] = Field(..., description=\"List of parts within the question (only the text, no numbering)\")\n",
    "    parts_solutions: list[str] = Field(..., description=\"List of worked solutions for the question (no numbering or counting)\")\n",
    "\n",
    "class Set(BaseModel):\n",
    "    name: str = Field(..., description=\"Title of the set\")\n",
    "    year: str = Field(..., description=\"Year of the set\")\n",
    "    questions: list[Set_Question] = Field(..., description=\"List of questions in the set\")\n",
    "\n",
    "\n",
    "# TODO: make parts completely seperate from stem to question\n",
    "# ensure no answer in question itself, only in parts_solutions.\n",
    "llm_task_seperate_parts = \"\"\"\n",
    "    Your task is to seperate the questions into indicidual parts and their worked solutions.\n",
    "    Please follow these steps carefully:\n",
    "        1. Every character should match the original source exactly unless you're instructed to split content into fields, without adding escapes or modifications.\n",
    "        2. Use the same name and year.\n",
    "        3. For each question in questions:\n",
    "            - Title is the only field where you are allowed to name it whatever you seem fit for the question.\n",
    "            - Do not neglect any images, figures, or other media mentioned in the question.\n",
    "            - Identify the stem and parts of the question, the parts may be obvious to find, like \"a)...\", \"b)...\", etc., or they could be implied by the question itself. All question must have at least one part, if there is only one part. :\n",
    "                1. The stem should be placed into the \"content\" field. Text in this field should be valid in the Milkdown editor. \n",
    "                2. the parts of the question (subquestions) should be placed into the \"parts\" field. Text in this field should be valid under Lexdown.\n",
    "                3. for each part, identify the worked solution/answer and place it into the \"parts_solutions\" field, if not found, leave as empty string, \"\". Text in this field should be valid under Lexdown.\n",
    "        4. Output only a valid, plain, raw JSON string matching the schema above, ready to parse immediately, with no code fence or extra text. Use plain newlines (not escaped as `\\n`).\n",
    "        5. The Text inside the JSON should be in Lexdown:\n",
    "            1. preserving all LaTeX math delimiters (`$...$` and `$$...$$`) and all formatting exactly as in the input, without paraphrasing, summarizing, or simplifying any mathematical expressions or formulas.\n",
    "            2. Do not remove or collapse blank lines.\n",
    "            3. Do not escape characters like `\\n` or `\\\\`.\n",
    "    \"\"\"\n",
    "\n",
    "def extract_parts(questions_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "\n",
    "    This function takes the content of a tutorial sheet (doc.page_content), constructs a prompt\n",
    "    instructing the LLM to infer the tutorial title and to split the text into separate questions.\n",
    "    The output must be a valid JSON string with the following structure:\n",
    "    \n",
    "    {\n",
    "        \"name\": \"<title of tutorial>\",\n",
    "        \"year\": \"<year of tutorial>\",\n",
    "        \"questions\": [\n",
    "            { title: \"exercise text 1\", content: \"content text exercise 1\", parts: [\"subquestion text 1\", \"subquestion text 2\", ...], parts_solutions: [\"solution text 1\", \"solution text 2\", ...] },\n",
    "            { title: \"exercise text 2\", content: \"content text exercise 2\", parts: [\"subquestion text 1\", \"subquestion text 2\", ...], parts_solutions: [\"solution text 1\", \"solution text 2\", ...] },\n",
    "            ...\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    the original text of the exercises. The function returns a dictionary parsed from the JSON output.\n",
    "    if any of the text mentions a figure/diagram, then also find the figure and add it to the content of the exercise.\n",
    "    \n",
    "    Args:\n",
    "        doc_page_content (str): The content of a set.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    # Initialize the output parser with the Tutorial schema.\n",
    "    parser = PydanticOutputParser(pydantic_object=Set)\n",
    "\n",
    "    # Construct the prompt, appending the parser's format instructions.\n",
    "    prompt = f\"\"\"\n",
    "        Input markdown:\n",
    "        ```markdown\n",
    "        {questions_dict}\n",
    "        ```\n",
    "\n",
    "        Your task is to extract a JSON with the following structure exactly:\n",
    "        {parser.get_format_instructions()}\n",
    "\n",
    "        {llm_task_seperate_parts}\n",
    "\n",
    "        Return the JSON now.\n",
    "        \"\"\"\n",
    "    \n",
    "    # tries to call the LLM multiple times to ensure robustness.\n",
    "    for i in range(3):\n",
    "        \n",
    "        # Call the LLM\n",
    "        response = llm.invoke(prompt)\n",
    "\n",
    "        # Debug: print the raw LLM response\n",
    "        # print(\"Raw LLM Response:\")\n",
    "        # print(response)\n",
    "\n",
    "        try:\n",
    "            # Parse the response using the output parser.\n",
    "            parsed_output = parser.parse(response.content)\n",
    "            print(\"LLM response successfully parsed as JSON with parts.\")\n",
    "            # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(\"❌ Pydantic Validation Error:\")\n",
    "            for error in ve.errors():\n",
    "                print(f\" - {error['loc']}: {error['msg']}\")\n",
    "            print(\"Raw LLM output:\")\n",
    "            print(response.content)\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing LLM response as JSON:\")\n",
    "            print(\"Retrying...\")\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# LLM evaluation of the content of JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_task_expression_check = r\"\"\"\n",
    "    Look inside the JSON object's `questions`, specifically the `content`, `parts`, and `parts_solutions` fields. Ensure that the JSON content follows these rules:\n",
    "        1. No extra escaping: The JSON string must contain no literal `\\\\n`, `\\\\\\\\`, or unnecessary escape sequences unless they are explicitly present in the original input text.\n",
    "        2. Careful to make the distinction between inline and display math, i.e. do not mess up the use of `$` and `$$`.\n",
    "        3. Math delimiters: All mathematical expressions must be fully enclosed within math delimiters — use `$...$` for inline math, and `$$...$$` for display math.\n",
    "        4. Balanced delimiters:\n",
    "            - All `$$` and `$` must be properly opened and closed.\n",
    "            - No unbalanced or partial math blocks.\n",
    "        4. Display math formatting:\n",
    "            - The opening `$$` must appear on a new line.\n",
    "            - The closing `$$` must also be on its own new line.\n",
    "            - The math content must appear immediately between them, with no extra blank lines unless they are part of the input.\n",
    "        5. Inline math rules:\n",
    "            - `$...$` should not span multiple lines.\n",
    "            - Avoid using `$$` for short inline expressions.\n",
    "        6. Preserve LaTeX syntax:\n",
    "            - All LaTeX commands, backslashes (`\\`), braces (`{}`, `[]`), and special characters must be preserved exactly as in the original input.\n",
    "            - Do not add or remove escaping.\n",
    "        7. Blank lines:\n",
    "            - Preserve all blank lines inside math blocks.\n",
    "            - Outside math, follow the structure of the original input.\n",
    "        8. Output format:\n",
    "            - Output a single valid JSON string.\n",
    "            - Do not include any extra characters, explanations, or escaped formatting outside the JSON structure.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def content_texdown_check(validated_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Checks if the content of the JSON is in Texdown format.\n",
    "    \n",
    "    Args:\n",
    "        validated_dict (dict): The validated dictionary from the LLM.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    json_string = json.dumps(validated_dict, indent=2)\n",
    "    \n",
    "    # prompt to let llm validate the JSON.\n",
    "    validation_prompt = f\"\"\"\n",
    "    {llm_task_expression_check}\n",
    "\n",
    "    Input JSON:\n",
    "    ```json\n",
    "    {json_string}\n",
    "    ```\n",
    "    return the JSON with the content fixed if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    parser = PydanticOutputParser(pydantic_object=Set)\n",
    "    \n",
    "    # loop 3 times to ensure robustness.\n",
    "    for i in range(3):\n",
    "        \n",
    "        # Call the LLM\n",
    "        response = llm.invoke(validation_prompt)\n",
    "\n",
    "        # Debug: print the raw LLM response\n",
    "        # print(\"Raw LLM Response:\")\n",
    "        # print(response)\n",
    "\n",
    "        try:\n",
    "            # Parse the response using the output parser.\n",
    "            parsed_output = parser.parse(response.content)\n",
    "            print(\"LLM response successfully parsed as JSON with valid $$.\")\n",
    "            # For Pydantic v2, use model_dump() to convert the model to a dictionary.\n",
    "            return parsed_output.model_dump()\n",
    "        except ValidationError as ve:\n",
    "            print(\"❌ Pydantic Validation Error:\")\n",
    "            for error in ve.errors():\n",
    "                print(f\" - {error['loc']}: {error['msg']}\")\n",
    "            print(\"Raw LLM output:\")\n",
    "            print(response.content)\n",
    "        except Exception as e:\n",
    "            print(\"Error parsing textdown LLM response as JSON:\")\n",
    "            print(\"Retrying...\")\n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md_to_json(md_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Extracts the title and individual questions from a tutorial sheet.\n",
    "    \n",
    "    Args:\n",
    "        md_content (str): The content of a set.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the keys \"name\" and \"exercise\".\n",
    "              If parsing fails, returns None.\n",
    "    \"\"\"\n",
    "    questions_dict = extract_questions(md_content)\n",
    "    extracted_dict = extract_parts(questions_dict)\n",
    "    content_validated_dict = content_texdown_check(extracted_dict)\n",
    "    return content_validated_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "imported_tutorial = md_to_json(md_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title\n",
    "title = imported_tutorial[\"name\"] + \" \" + imported_tutorial[\"year\"]\n",
    "\n",
    "# Print the title\n",
    "print(f\"Title: {title}\\n\")\n",
    "\n",
    "# Extract questions\n",
    "questions = imported_tutorial[\"questions\"]\n",
    "\n",
    "print(questions)\n",
    "\n",
    "# Loop over and print each question\n",
    "for idx1, question in enumerate(questions, start=1):\n",
    "    print(f\"**Question {idx1}**:\\n{question.get(\"title\")}\\n\")\n",
    "    print(f\"Content: {question.get(\"content\")}\\n\")\n",
    "    for idx2, (part, part_answer) in enumerate(zip(question.get(\"parts\", []), question.get(\"parts_solutions\", [])), start=1):\n",
    "        print(f\"Question {idx1}:\")\n",
    "        print(f\"- Subquestion {idx2}: {part}\")\n",
    "        print(f\"- Worked Solution {idx2}: {part_answer}\")\n",
    "        print(\"\\n\")\n",
    "    print(\"-\" * 40)  # Separator for readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Form JSON Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = imported_tutorial[\"questions\"]\n",
    "\n",
    "in2lambda_questions = []\n",
    "\n",
    "# Loop over all questions and question_answers and use in2lambda to create a JSON.\n",
    "for idx, question_dict in enumerate(questions, start=1):\n",
    "    parts = []\n",
    "    for part_question, part_solution in zip(question_dict.get(\"parts\", []), question_dict.get(\"parts_solutions\", [])):\n",
    "        part_obj = Part(\n",
    "            text=part_question,\n",
    "            worked_solution=part_solution\n",
    "        )\n",
    "        parts.append(part_obj)\n",
    "\n",
    "    question = Question(\n",
    "        title=question_dict.get(\"title\", f\"Question {idx}\"),\n",
    "        main_text=question_dict.get(\"content\", \"\"),\n",
    "        parts=parts\n",
    "    )\n",
    "    in2lambda_questions.append(question)\n",
    "\n",
    "Module(in2lambda_questions).to_json(f\"{output_path}/out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
